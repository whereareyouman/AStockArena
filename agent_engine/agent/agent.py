"""
AgenticWorkflow class - Base class for trading agents
Encapsulates core functionality including local tool management, AI agentic workflow creation, and trading execution
"""

import copy
import os
import sys
import json
import asyncio
import threading
import random
import shutil
import subprocess
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from zoneinfo import ZoneInfo
import pandas as pd
import pandas_ta as ta
from dotenv import load_dotenv
import threading

from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.prebuilt import create_react_agent

# Import project tools
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

load_dotenv()

from data_manager import DataManager
from agent_engine.shared_prefetch import SharedPrefetchCoordinator
from utils.runtime_config import extract_llm_conversation, extract_llm_tool_messages, get_runtime_config_value, write_runtime_config_value
from utils.json_file_manager import safe_read_json
from utils.position_manager import (
    add_no_trade_record,
    calculate_previous_trading_date,
    get_current_position,
    normalize_decision_time,
    normalize_positions,
    normalize_symbol,
    strip_exchange_prefix,
    upsert_position_record,
    get_price_limits,
)
from utils.news_deduplicator import deduplicate_news_by_embedding
from utils.eastmoney_news import stock_news_em_safe
from prompt_templates.prompts import get_agent_system_prompt, STOP_SIGNAL

# å¹¶å‘å®‰å…¨ï¼šç”¨äºä¿æŠ¤ news.csv å†™å…¥
NEWS_FILE_LOCK = threading.Lock()


class AgenticWorkflow:
    """
    Main functionalities:
    1. Local tool management (DataManager and price tools)
    2. AI agentic workflow creation and configuration
    3. Trading execution and decision loops
    4. Logging and management
    5. Position and configuration management
    """
    
    # ç§‘åˆ›æ¿ä»£è¡¨æ€§è‚¡ç¥¨ï¼ˆSTAR Market Stocksï¼‰
    DEFAULT_STOCK_SYMBOLS = [
        "SH688008",  # æ¾œèµ·ç§‘æŠ€ *
        "SH688111",  # é‡‘å±±åŠå…¬ *
        "SH688009",  # ä¸­å›½é€šå· *
        "SH688981",  # ä¸­èŠ¯å›½é™… *
        "SH688256",  # å¯’æ­¦çºª *
        "SH688271",  # è”å½±åŒ»ç–— *
        "SH688047",  # é¾™èŠ¯ä¸­ç§‘ *
        "SH688617",  # æƒ æ³°åŒ»ç–— *
        "SH688303",  # å¤§å…¨èƒ½æº *
        "SH688180",  # å›å®ç”Ÿç‰© *
    ]
    MINI_CANDLE_COUNT = 6
    RECENT_CLOSE_COUNT = 6
    LIMIT_ORDER_SUCCESS_RATE = 0.10
    LIMIT_THRESHOLD_RATIO = 0.999
    def __init__(
        self,
        signature: str,
        basemodel: str,
        stock_symbols: Optional[List[str]] = None,
        stock_json_path: str = "./data_flow/ai_stock_data.json",
        news_csv_path: str = "./data_flow/news.csv",
        macro_csv_path: Optional[str] = None,
        log_path: Optional[str] = None,
        max_steps: int = 10,
        max_retries: int = 3,
        base_delay: float = 0.5,
        openai_base_url: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        google_api_key: Optional[str] = None,
        safety_settings: Optional[Dict[str, str]] = None,
        parameters: Optional[Dict[str, Any]] = None,
        initial_cash: float = 1000000.0,
        init_date: Optional[str] = None,
        trading_rules: Optional[Dict[str, float]] = None,
        risk_management: Optional[Dict[str, float]] = None,
        force_replay: bool = False,
    ):
        """
        Initialize AgenticWorkflow
        
        Args:
            signature: Agent signature/name
            basemodel: Base model name
            stock_symbols: List of stock symbols
            stock_json_path: Path to stock price JSON file (ai_stock_data.json)
            news_csv_path: Path to news CSV file
            macro_csv_path: Path to macro news CSV file
            log_path: Log path, defaults to ./data_flow/trading_summary_each_agent
            max_steps: Maximum reasoning steps
            max_retries: Maximum retry attempts
            base_delay: Base delay time for retries
            openai_base_url: OpenAI API base URL
            openai_api_key: OpenAI API key
            google_api_key: Google Gemini API key
            safety_settings: Google Gemini safety settings
            initial_cash: Initial cash amount
            init_date: Initialization date
            trading_rules: Dictionary with trading rule settings
            risk_management: Dictionary with risk management settings
        """
        self.signature = signature
        self.basemodel = basemodel
        self.stock_symbols = stock_symbols or self.DEFAULT_STOCK_SYMBOLS
        self.allowed_symbols = {
            normalize_symbol(sym) for sym in self.stock_symbols if normalize_symbol(sym)
        }

        def _resolve_path(input_path: Optional[str], default_subpath: Optional[str]) -> Optional[str]:
            target = input_path or default_subpath
            if not target:
                return None
            if os.path.isabs(target):
                return target
            cleaned = target.lstrip("./")
            return os.path.join(project_root, cleaned)

        self.stock_json_path = _resolve_path(stock_json_path, "data_flow/ai_stock_data.json")
        self.news_csv_path = _resolve_path(news_csv_path, "data_flow/news.csv")
        self.macro_csv_path = _resolve_path(macro_csv_path, None)
        self._prefetched_news: Dict[str, Dict[str, Any]] = {}
        self._prefetched_prices: Dict[str, Dict[str, Any]] = {}
        self._prefetched_indicators: Dict[str, Dict[str, Any]] = {}
        self._last_prefetch_bundle: Optional[Dict[str, Any]] = None
        self._current_snapshot_info: Dict[str, Any] = {}
        self.max_steps = max_steps
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.initial_cash = initial_cash
        if not init_date:
            raise ValueError("init_date must be provided for AgenticWorkflow")
        self.init_date = init_date
        self.force_replay = force_replay
        
        # Load trading rules and risk management from config
        self.trading_rules = trading_rules or {}
        self.risk_management = risk_management or {}
        
        # Set log path
        self.base_log_path = _resolve_path(log_path, "data_flow/trading_summary_each_agent")
        os.makedirs(self.base_log_path, exist_ok=True)
        shared_override = os.getenv("SHARED_PREFETCH_DIR")
        self.shared_prefetch_root = _resolve_path(shared_override, "data_flow/agent_data/shared")
        if self.shared_prefetch_root:
            os.makedirs(self.shared_prefetch_root, exist_ok=True)
        self.prefetch_coordinator = SharedPrefetchCoordinator(base_dir=self.shared_prefetch_root)
        
        # Set OpenAI configuration
        if openai_base_url is None:
            if self.basemodel.startswith("qwen"):
                self.openai_base_url = (
                    os.getenv("QWEN_API_BASE")
                    or os.getenv("DASHSCOPE_API_BASE")
                    or "https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
            else:
                self.openai_base_url = os.getenv("OPENAI_API_BASE")
        else:
            self.openai_base_url = openai_base_url

        if openai_api_key is None:
            if self.basemodel.startswith("qwen"):
                self.openai_api_key = (
                    os.getenv("QWEN_API_KEY")
                    or os.getenv("DASHSCOPE_API_KEY")
                    or os.getenv("OPENAI_API_KEY")
                )
            else:
                self.openai_api_key = os.getenv("OPENAI_API_KEY")
        else:
            self.openai_api_key = openai_api_key
        
        # Set Google Gemini configuration
        if google_api_key is None:
            self.google_api_key = os.getenv("GEMINI_API_KEY")
        else:
            self.google_api_key = google_api_key
        
        # Store parameters and safety_settings for model initialization
        self.parameters = parameters or {}
        self.safety_settings = safety_settings
        
        # é¢„å…ˆæ¸…ç†æ–°é—»ç¼“å­˜ï¼Œé¿å…åŠ è½½éç™½åå•è‚¡ç¥¨
        if self.news_csv_path:
            self._purge_news_csv(self.news_csv_path)
        
        # Initialize DataManagerï¼ˆç°åœ¨ä¸éœ€è¦CSVï¼Œå®Œå…¨ä¾èµ–TinySoftå®æ—¶æ•°æ®ï¼‰
        try:
            self.dm = DataManager(
                stock_csv_path=None,  # ä¸å†ä½¿ç”¨CSVï¼Œå®Œå…¨ä¾èµ–TinySoftå®æ—¶è·å–
                news_csv_path=self.news_csv_path,
                macro_csv_path=self.macro_csv_path
            )
            print(f"âœ… DataManager åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨TinySoftå®æ—¶æ•°æ®æºï¼‰")
            if self.dm.news_df is not None:
                self.dm.news_df = self._filter_allowed_news_df(self.dm.news_df)
        except Exception as e:
            print(f"âŒ DataManager åˆå§‹åŒ–å¤±è´¥: {e}")
            self.dm = None
        
        # Initialize components
        self.local_tools: List = []  # æœ¬åœ° DataManager å·¥å…·
        self.tools: List = []  # åˆå¹¶åçš„æ‰€æœ‰å·¥å…·
        self.model: Optional[Any] = None
        self.agent: Optional[Any] = None
        
        # Data paths
        self.data_path = os.path.join(self.base_log_path, self.signature)
        self.position_file = os.path.join(self.data_path, "position", "position.jsonl")
        
        # --- å¹¶å‘è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ˆæ›¿ä»£å¯¹å…¨å±€ runtime_env.json çš„è¯»å–ä¾èµ–ï¼‰ ---
        self.runtime_context: Dict[str, Any] = {
            "TODAY_DATE": None,
            "CURRENT_TIME": None,
            "DECISION_COUNT": 0
        }
    
    def _reset_agent_storage(self) -> None:
        """Remove existing agentic workflow data_pipeline directory (positions + logs) when replaying."""
        agent_path = Path(self.data_path)
        if agent_path.exists():
            shutil.rmtree(agent_path, ignore_errors=True)
    
    def _get_context_value(self, key: str):
        """ä¼˜å…ˆè¯»å–å®ä¾‹çº§ä¸Šä¸‹æ–‡ï¼Œå…¶æ¬¡å›é€€åˆ°å…¨å±€é…ç½®ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰"""
        if key in self.runtime_context and self.runtime_context[key] is not None:
            return self.runtime_context[key]
        return get_runtime_config_value(key)
    
    def _is_allowed_symbol(self, symbol: Optional[str], *, allow_sell_existing: bool = False) -> bool:
        """
        åˆ¤æ–­ç¬¦å·æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­ã€‚
        allow_sell_existing=True æ—¶ï¼Œå¦‚æœç¬¦å·å½“å‰æŒä»“ä¸­å­˜åœ¨ï¼Œä¹Ÿå…è®¸ã€‚
        """
        normalized = normalize_symbol(symbol) if symbol else None
        if normalized and normalized in self.allowed_symbols:
            return True
        if allow_sell_existing and normalized:
            latest_positions, _, _ = get_current_position(
                self._get_context_value("TODAY_DATE") or self.init_date,
                self.signature
            )
            return normalized in latest_positions
        return False

    def _allowed_symbol_list(self) -> List[str]:
        return sorted(sym for sym in self.allowed_symbols if sym)

    def _filter_allowed_news_df(self, df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:
        if df is None or df.empty or 'symbol' not in df.columns:
            return df
        filtered = df[df['symbol'].astype(str).apply(
            lambda sym: self._is_allowed_symbol(normalize_symbol(sym))
        )]
        return filtered

    def _filter_allowed_news_records(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        filtered: List[Dict[str, Any]] = []
        for item in records:
            sym = normalize_symbol(item.get("symbol"))
            if self._is_allowed_symbol(sym):
                filtered.append(item)
        return filtered

    def _prefetch_all_news(self, today_date: str, current_time: str, max_retries: int = 2) -> None:
        print("ğŸ“° Prefetching news for all whitelisted symbols...")
        # è§‚å¯Ÿçª—å£ï¼šå½“å¤© + è¿‡å» 2 å¤©ï¼ˆå…± 3 å¤©ï¼‰ã€‚å¹¶ä¸”åªå– <= current_time çš„æ–°é—»ï¼Œé¿å…â€œçœ‹è§æœªæ¥â€ã€‚
        cutoff = pd.to_datetime(today_date) - pd.Timedelta(days=2)
        now_dt = pd.to_datetime(current_time, errors="coerce")
        for sym in self.stock_symbols:
            normalized = normalize_symbol(sym)
            if not normalized:
                continue
            raw_query = f"{strip_exchange_prefix(normalized) or normalized} æœ€æ–°æ¶ˆæ¯"
            result_str = self.search_stock_news(raw_query, max_retries=max_retries)
            try:
                payload = json.loads(result_str)
            except Exception:
                continue
            if not payload.get("success"):
                continue

            entries = payload.get("historical_news", []) + payload.get("realtime_news", [])
            entries = self._filter_allowed_news_records(entries)
            recent_items: List[Dict[str, Any]] = []
            for item in entries:
                title = str(item.get("title") or item.get("æ–°é—»æ ‡é¢˜") or "").strip()
                if not title:
                    continue
                publish_raw = item.get("publish_time") or item.get("å‘å¸ƒæ—¶é—´")
                publish_dt = pd.to_datetime(publish_raw, errors="coerce")
                if publish_dt is not None and publish_dt.tzinfo is not None:
                    publish_dt = publish_dt.tz_convert("Asia/Shanghai").tz_localize(None)
                if publish_dt is not None:
                    if publish_dt < cutoff:
                        continue
                    if now_dt is not None and publish_dt > now_dt:
                        continue
                display_time = publish_dt.strftime("%Y-%m-%d %H:%M") if publish_dt is not None else str(publish_raw)
                recent_items.append({
                    "title": title[:120],
                    "publish_time": display_time
                })

            recent_items.sort(key=lambda item: item.get("publish_time", ""), reverse=True)
            # ç”¨ title ä½œä¸ºâ€œæ–°é—»æ‘˜è¦â€ï¼›é»˜è®¤ç»™æ›´å¤šæ¡ï¼Œä¾¿äºæ¨¡å‹è‡ªå·±å½’çº³
            truncated_items = recent_items[:20]
            self._prefetched_news[normalized] = {
                "news": truncated_items,
                "count": len(truncated_items)
            }

    def _prefetch_all_prices(self, today_date: str, current_time: str) -> None:
        """
        é¢„æŠ“å– Observation çª—å£ï¼ˆ3 å¤©ï¼‰ï¼š
        - ä»·æ ¼ï¼šè¿‡å» 3 å¤©ï¼ˆå«å½“å¤©ï¼‰åœ¨å†³ç­–æ—¶åˆ»çš„ close
        - æŠ€æœ¯æŒ‡æ ‡ï¼šè¿‡å» 3 å¤©ï¼ˆå«å½“å¤©ï¼‰åœ¨å†³ç­–æ—¶åˆ»çš„ RSI / MACD(12-26-9)
        æ³¨æ„ï¼šä¸è®¡ç®—/ä¸è¾“å‡º OBVï¼ˆä¸åŒè‚¡ç¥¨é‡çº²å·®å¼‚å¤ªå¤§ï¼‰ã€‚
        """
        print("ğŸ’¹ Prefetching 3-day window: price + RSI + MACD(12-26-9) ...")
        if not self.dm:
            print("âš ï¸ DataManager unavailable; skip prefetch prices/indicators.")
            return

        # è§‚å¯Ÿçª—å£ï¼šå½“å¤© + è¿‡å» 2 å¤©
        window_days = 3
        rsi_length = 3
        macd_params = {"fast": 12, "slow": 26, "signal": 9}

        # ä¸ºäº†è®¡ç®— MACD(12/26/9) ä¸ RSIï¼Œéœ€è¦æ›´é•¿çš„å†å²çª—å£ï¼›è¿™é‡ŒæŒ‰â€œè‡ªç„¶å°æ—¶â€å‘å‰å– 20 å¤©
        lookback_hours = 24 * 20

        # å†³ç­–æ—¶åˆ»å¯¹é½ï¼ˆ10:30/11:30/14:00ï¼‰
        anchor_time = (current_time.split(" ")[1] if " " in current_time else "15:00:00").strip()
        try:
            anchor_time_obj = datetime.strptime(anchor_time, "%H:%M:%S").time()
        except Exception:
            anchor_time_obj = None

        try:
            today_obj = datetime.strptime(today_date, "%Y-%m-%d").date()
        except Exception:
            today_obj = None

        for sym in self.stock_symbols:
            normalized = normalize_symbol(sym)
            if not normalized:
                continue

            try:
                plain_symbol = strip_exchange_prefix(normalized) or normalized
                df = self.dm.get_hourly_stock_data(
                    symbol=plain_symbol,
                    end_date=current_time,
                    lookback_hours=lookback_hours,
                )
                if df is None or df.empty:
                    continue

                df = df.copy()
                df = df.sort_index()

                # DataManager è¿”å› UTC æ—¶é—´æˆ³ï¼›ç»Ÿä¸€è½¬æ¢ä¸º Asia/Shanghai çš„ naive datetimeï¼Œä¾¿äºå’Œå†³ç­–æ—¶åˆ»å¯¹é½
                try:
                    if getattr(df.index, "tz", None) is not None:
                        df.index = df.index.tz_convert("Asia/Shanghai").tz_localize(None)
                except Exception:
                    # ä¿åº•ï¼šå½“ä½œæ™®é€šæ—¶é—´æˆ³å¤„ç†
                    df.index = pd.to_datetime(df.index, errors="coerce").tz_localize(None)

                if "close" not in df.columns:
                    continue

                # è®¡ç®—æŒ‡æ ‡ï¼šMACD(12/26/9) + RSI(3)
                try:
                    df.ta.macd(
                        fast=macd_params["fast"],
                        slow=macd_params["slow"],
                        signal=macd_params["signal"],
                        append=True,
                    )
                    df.ta.rsi(length=rsi_length, append=True)
                except Exception as e:
                    print(f"âš ï¸ é¢„è®¡ç®—æŒ‡æ ‡å¤±è´¥ï¼š{normalized} - {e}")
                    continue

                # é€‰å–â€œå½“å¤© + è¿‡å»2å¤©â€çš„å¯¹é½ç‚¹ï¼ˆåŒä¸€ HH:MM:SSï¼‰ï¼Œå¦‚æœæ²¡æœ‰ç²¾ç¡®ç‚¹åˆ™å–å½“æ—¥ <= target çš„æœ€è¿‘ä¸€æ¡
                points: List[Dict[str, Any]] = []
                unique_dates = sorted({ts.date() for ts in df.index if isinstance(ts, datetime)})
                unique_dates = [d for d in unique_dates if (today_obj is None or d <= today_obj)]
                for d in reversed(unique_dates):
                    if len(points) >= window_days:
                        break
                    day_df = df[df.index.date == d]
                    if day_df.empty:
                        continue

                    if anchor_time_obj is not None:
                        target_dt = datetime.combine(d, anchor_time_obj)
                    else:
                        target_dt = datetime.combine(d, datetime.strptime("15:00:00", "%H:%M:%S").time())

                    cand = day_df[day_df.index <= target_dt]
                    if cand.empty:
                        sel_row = day_df.iloc[0]
                        sel_ts = day_df.index[0]
                    else:
                        sel_row = cand.iloc[-1]
                        sel_ts = cand.index[-1]

                    close_val = sel_row.get("close")
                    rsi_val = sel_row.get(f"RSI_{rsi_length}")
                    macd_val = sel_row.get("MACD_12_26_9")

                    def _to_num(v):
                        try:
                            if v is None or (isinstance(v, float) and pd.isna(v)):
                                return None
                            if pd.isna(v):
                                return None
                        except Exception:
                            pass
                        try:
                            return float(v)
                        except Exception:
                            return None

                    points.append({
                        "date": d.strftime("%Y-%m-%d"),
                        "timestamp": sel_ts.strftime("%Y-%m-%d %H:%M:%S") if isinstance(sel_ts, datetime) else str(sel_ts),
                        "close": _to_num(close_val),
                        f"RSI_{rsi_length}": _to_num(rsi_val),
                        "MACD_12_26_9": _to_num(macd_val),
                    })

                points.sort(key=lambda item: item.get("date", ""))
                latest_point = points[-1] if points else None

                # --- prices payloadï¼ˆä¾› LLM & æ—¥å¿—ä½¿ç”¨ï¼‰ ---
                change_pct = None
                if len(points) >= 2:
                    prev_close = points[-2].get("close")
                    last_close = points[-1].get("close")
                    if prev_close not in (None, 0) and last_close is not None:
                        change_pct = round(((last_close / prev_close) - 1.0) * 100.0, 4)

                self._prefetched_prices[normalized] = {
                    "symbol": normalized,
                    "anchor_time": anchor_time,
                    "window_days": window_days,
                    "summary": {
                        "timestamp": latest_point.get("timestamp") if latest_point else None,
                        "close": latest_point.get("close") if latest_point else None,
                        "change_pct": change_pct,
                    },
                    # ä»·æ ¼çª—å£ï¼ˆ3å¤©ï¼‰
                    "prices_3d": [
                        {"date": p.get("date"), "timestamp": p.get("timestamp"), "close": p.get("close")}
                        for p in points
                    ],
                }

                # --- indicators payloadï¼ˆåªæä¾› RSI/MACDï¼Œä¸”çª—å£=3å¤©ï¼›ä¸å« OBVï¼‰ ---
                latest_rsi = latest_point.get(f"RSI_{rsi_length}") if latest_point else None
                latest_macd = latest_point.get("MACD_12_26_9") if latest_point else None
                self._prefetched_indicators[normalized] = {
                    "symbol": normalized,
                    "anchor_time": anchor_time,
                    "window_days": window_days,
                    "rsi_length": rsi_length,
                    "macd_params": macd_params,
                    "indicators": {
                        f"RSI_{rsi_length}": latest_rsi,
                        "MACD_12_26_9": latest_macd,
                    },
                    "indicators_3d": [
                        {
                            "date": p.get("date"),
                            "timestamp": p.get("timestamp"),
                            f"RSI_{rsi_length}": p.get(f"RSI_{rsi_length}"),
                            "MACD_12_26_9": p.get("MACD_12_26_9"),
                        }
                        for p in points
                    ],
                }
            except Exception as e:
                print(f"âš ï¸ é¢„æŠ“å–çª—å£å¤±è´¥ï¼š{normalized} - {e}")
                continue

    # ä¿ç•™æ—§æ¥å£åï¼ˆé˜²æ­¢æœªæ¥æœ‰äººä»åˆ«å¤„è°ƒç”¨ï¼‰ï¼Œä½†å½“å‰ä¸å†å•ç‹¬é¢„æŠ“å–â€œ10å¤©æŒ‡æ ‡â€
    def _prefetch_all_indicators(self, today_date: str, current_time: str) -> None:
        return

    def _build_observation_summary(self) -> str:
        if not self.stock_symbols:
            return "  â€¢ (no whitelisted symbols configured)"

        lines: List[str] = []
        for sym in self.stock_symbols:
            normalized = normalize_symbol(sym)
            if not normalized:
                continue

            price_payload = self._prefetched_prices.get(normalized, {})
            price_summary = price_payload.get("summary") if isinstance(price_payload, dict) else None
            price_text = "Px: -"
            if price_summary:
                close = price_summary.get("close")
                change_pct = price_summary.get("change_pct")
                if close is not None:
                    price_text = f"Px Â¥{close:,.2f}"
                if change_pct is not None:
                    sign = "+" if change_pct >= 0 else ""
                    price_text += f" ({sign}{change_pct:.2f}%)"

            indicator_payload = self._prefetched_indicators.get(normalized, {})
            indicators = indicator_payload.get("indicators") if isinstance(indicator_payload, dict) else None
            indicator_parts: List[str] = []
            if indicators:
                if indicators.get("SMA_10") is not None:
                    indicator_parts.append(f"SMA10 {indicators['SMA_10']:.2f}")
                if indicators.get("RSI_10") is not None:
                    indicator_parts.append(f"RSI10 {indicators['RSI_10']:.1f}")
                macd_val = indicators.get("MACD_12_26_9")
                if macd_val is not None:
                    indicator_parts.append(f"MACD {macd_val:.2f}")
            indicator_text = "; ".join(indicator_parts) if indicator_parts else "Indicators: -"

            news_payload = self._prefetched_news.get(normalized, {})
            news_titles = [item.get("title", "") for item in news_payload.get("news", [])]
            news_titles = [title for title in news_titles if title]
            if news_titles:
                news_text = " | ".join(news_titles[:2])
            else:
                news_text = "no recent news (â‰¤3d)"

            line = f"  â€¢ {normalized}: {price_text}; {indicator_text}; News: {news_text}"
            lines.append(line)

        if not lines:
            return "  â€¢ (prefetch unavailable)"
        return "\n".join(lines)

    def _symbols_signature(self) -> str:
        normalized = sorted(sym for sym in self.allowed_symbols if sym)
        return "|".join(normalized)

    def _collect_prefetch_bundle(
        self,
        today_date: str,
        current_time: str,
        decision_count: int,
    ) -> Dict[str, Any]:
        """
        Run all prefetch helpers and return a serializable snapshot bundle
        for shared caching/logging.
        """
        self._prefetched_news.clear()
        self._prefetched_prices.clear()
        self._prefetched_indicators.clear()
        self._prefetch_all_news(today_date, current_time, max_retries=2)
        self._prefetch_all_prices(today_date, current_time)
        
        # åŒæ­¥æ›´æ–° ai_stock_data.jsonï¼šç¡®ä¿å¿«ç…§ä¸­çš„æ•°æ®ä¹Ÿä¿å­˜åˆ°æŒä¹…åŒ–æ–‡ä»¶
        # è¿™æ ·å¯ä»¥é¿å…æ•°æ®åªåœ¨å¿«ç…§ä¸­å­˜åœ¨ï¼Œè€Œ ai_stock_data.json ä¸­æ²¡æœ‰çš„æƒ…å†µ
        # æ³¨æ„ï¼š_prefetch_all_prices é€šè¿‡ get_hourly_stock_data æŸ¥è¯¢äº†æ•°æ®ä½†æœªä¿å­˜ï¼Œ
        # è¿™é‡Œé€šè¿‡ save_ts_data æ‰¹é‡ä¿å­˜ï¼Œç¡®ä¿æ•°æ®æŒä¹…åŒ–
        if self.dm:
            try:
                ai_stock_data_path = self.stock_json_path
                # æ›´æ–°æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®ï¼ˆä½¿ç”¨è¾ƒé•¿çš„å›æº¯å¤©æ•°ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰æŸ¥è¯¢çš„æ—¶é—´èŒƒå›´ï¼‰
                # save_ts_data ä¼šåˆå¹¶å†å²æ•°æ®ï¼Œä¸ä¼šè¦†ç›–å·²æœ‰æ•°æ®
                self.dm.save_ts_data(
                    symbols=list(self.stock_symbols),
                    ndays=60,  # ä½¿ç”¨60å¤©ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰å†å²æ•°æ®
                    out_path=ai_stock_data_path
                )
                print(f"ğŸ’¾ å·²åŒæ­¥æ›´æ–° ai_stock_data.jsonï¼ˆå¿«ç…§ç”Ÿæˆæ—¶è‡ªåŠ¨åŒæ­¥ï¼‰")
            except Exception as e:
                print(f"âš ï¸ åŒæ­¥æ›´æ–° ai_stock_data.json å¤±è´¥ï¼ˆä¸å½±å“å¿«ç…§ç”Ÿæˆï¼‰: {e}")

        snapshot_id = f"{today_date}_{current_time.replace(':', '-').replace(' ', '_')}"
        summary = self._build_observation_summary()
        now_iso = datetime.now(timezone.utc).isoformat()

        bundle: Dict[str, Any] = {
            "snapshot_id": snapshot_id,
            "schema_version": 1,
            "today_date": today_date,
            "decision_time": current_time,
            "decision_count": decision_count,
            "generated_at": now_iso,
            "source_agent": self.signature,
            "symbols": list(self.stock_symbols),
            "normalized_symbols": sorted(sym for sym in self.allowed_symbols if sym),
            "symbols_signature": self._symbols_signature(),
            "news": copy.deepcopy(self._prefetched_news),
            "prices": copy.deepcopy(self._prefetched_prices),
            "indicators": copy.deepcopy(self._prefetched_indicators),
            "observation_summary": summary,
            "prefetch_config": {
                "news_window_days": 3,
                "metrics_window_days": 3,
                "news_uses_title_only": True,
                "price_window_days": 3,
                "rsi_length": 3,
                "macd_params": {"fast": 12, "slow": 26, "signal": 9},
                "obv_used": False,
            },
        }
        self._last_prefetch_bundle = bundle
        return bundle

    def _apply_prefetch_bundle(self, bundle: Dict[str, Any]) -> str:
        """
        Load prefetched structures from a snapshot bundle back into the agentic workflow.
        Returns the observation summary text.
        """
        self._last_prefetch_bundle = bundle
        news_payload = bundle.get("news") or {}
        prices_payload = bundle.get("prices") or {}
        indicators_payload = bundle.get("indicators") or {}
        self._prefetched_news = copy.deepcopy(news_payload)
        self._prefetched_prices = copy.deepcopy(prices_payload)
        self._prefetched_indicators = copy.deepcopy(indicators_payload)
        summary = bundle.get("observation_summary")
        if not summary:
            summary = self._build_observation_summary()
        return summary

    def _purge_news_csv(self, csv_path: Optional[str]) -> None:
        if not csv_path or not os.path.exists(csv_path):
            return
        with NEWS_FILE_LOCK:
            df = None
            for encoding in ['utf-8', 'utf-8-sig', 'gbk', 'gb18030', 'latin1']:
                try:
                    df = pd.read_csv(csv_path, encoding=encoding)
                    print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å– {csv_path} ä»¥è¿›è¡Œæ¸…ç†")
                    break
                except Exception:
                    continue
            if df is None or df.empty:
                return
            df = self._sanitize_news_dataframe(df)
            filtered_df = self._filter_allowed_news_df(df)
            if filtered_df is None:
                return
            if len(filtered_df) != len(df):
                filtered_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                print(f"ğŸ§¹ å·²æ¸…ç† {csv_path} ä¸­çš„éç™½åå•æ–°é—»è®°å½•")
    
    # --- æœ¬åœ°å·¥å…·å‡½æ•°å®šä¹‰ (DM Functions) ---
    
    def get_current_stock_prices(self, symbols: List[str], target_time: str) -> str:
        """
        è·å–å¤šä¸ªè‚¡ç¥¨åœ¨æŒ‡å®šæ—¶é—´ç‚¹æˆ–ä¹‹å‰æœ€æ–°çš„æ”¶ç›˜ä»·æ ¼ã€‚
        
        Args:
            symbols (List[str]): è‚¡ç¥¨ä»£ç åˆ—è¡¨(ä¾‹å¦‚ ["MSFT", "AAPL"])ã€‚
            target_time (str): ç›®æ ‡æ—¶é—´(YYYY-MM-DD HH:MM:SS)ã€‚
        
        Returns:
            str: JSON å­—ç¬¦ä¸²æ ¼å¼çš„å­—å…¸,æ˜ å°„è‚¡ç¥¨ä»£ç åˆ°ä»·æ ¼(å¦‚æœæœªæ‰¾åˆ°åˆ™ä¸º null)ã€‚
        """
        if not self.dm:
            return json.dumps({"error": "DataManager æœªåˆå§‹åŒ–"})
        try:
            prices_dict = self.dm.get_prices_at(symbols=symbols, target_time=target_time)
            return json.dumps(prices_dict)
        except Exception as e:
            return json.dumps({"error": f"è·å–å½“å‰ä»·æ ¼æ—¶å‡ºé”™: {str(e)}"})
    
    def get_hourly_stock_data(self, symbol: str, end_time: str, lookback_hours: Optional[int] = 24) -> str:
        """
        è·å–å•ä¸ªè‚¡ç¥¨çš„å°æ—¶çº¿æ•°æ®ï¼ˆ60åˆ†é’Ÿ K çº¿ï¼‰ä»¥åŠç®€è¦æ‘˜è¦ã€‚
        æ‘˜è¦åŒ…å«ï¼šæœ€æ–°ä¸€æ ¹ K çº¿ã€ä¸Šä¸€æ ¹æ”¶ç›˜ä»·ã€æœ€è¿‘è‹¥å¹²æ”¶ç›˜ä»·ï¼›åŒæ—¶è¿”å›ä¸€å°æ®µæœ€æ–°çš„å°æ—¶ K çº¿åˆ—è¡¨ã€‚
        
        ä¼˜å…ˆä½¿ç”¨å¿«ç…§æ•°æ®ï¼Œå¦‚æœå¿«ç…§ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œå†å›é€€åˆ° DataManagerã€‚
        """
        normalized_symbol = normalize_symbol(symbol)
        if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
            return json.dumps({
                "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„ç ”ç©¶åå•ä¸­",
                "allowed_symbols": self._allowed_symbol_list(),
                "symbol": normalized_symbol
            }, ensure_ascii=False)

        def _to_float(value: Any) -> Optional[float]:
            try:
                if value is None:
                    return None
                return float(value)
            except (TypeError, ValueError):
                return None

        try:
            window = lookback_hours or 24
            plain_symbol = strip_exchange_prefix(normalized_symbol) if normalized_symbol else symbol
            query_symbol = plain_symbol or symbol
            
            # ä¼˜å…ˆå°è¯•ä»å¿«ç…§ä¸­è·å–å½“å‰ä»·æ ¼ï¼ˆå¦‚æœåªéœ€è¦å½“å‰ä»·æ ¼æˆ–å°‘é‡å†å²æ•°æ®ï¼‰
            # å¿«ç…§ä¸­åŒ…å« prices_3dï¼ˆè¿‡å»3å¤©çš„ä»·æ ¼ç‚¹ï¼‰ï¼Œå¯ä»¥ç”¨äºæ„å»ºç®€å•çš„å†å²æ•°æ®
            df = None
            price_source = "unknown"
            
            # å¦‚æœåªéœ€è¦å½“å‰ä»·æ ¼æˆ–å°‘é‡å†å²æ•°æ®ï¼Œå°è¯•ä»å¿«ç…§æ„å»º
            if window <= 3 and self._prefetched_prices:
                price_payload = self._prefetched_prices.get(normalized_symbol)
                if isinstance(price_payload, dict):
                    prices_3d = price_payload.get("prices_3d", [])
                    summary = price_payload.get("summary", {})
                    
                    # å¦‚æœå¿«ç…§ä¸­æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œå°è¯•æ„å»º DataFrame
                    if prices_3d and summary:
                        try:
                            # ä»å¿«ç…§çš„ prices_3d æ„å»ºç®€å•çš„å†å²æ•°æ®
                            # æ³¨æ„ï¼šå¿«ç…§ä¸­çš„ prices_3d æ˜¯æ—¥çº¿æ•°æ®ï¼Œä¸æ˜¯å°æ—¶çº¿ï¼Œä½†å¯ä»¥ç”¨äºæ„å»ºæ‘˜è¦
                            # å¦‚æœéœ€è¦çœŸæ­£çš„å°æ—¶çº¿æ•°æ®ï¼Œä»ç„¶éœ€è¦è°ƒç”¨ DataManager
                            # è¿™é‡Œåªä¼˜åŒ–ï¼šå¦‚æœåªéœ€è¦å½“å‰ä»·æ ¼ï¼Œä½¿ç”¨å¿«ç…§é¿å…è°ƒç”¨ DataManager
                            if window <= 1:
                                # åªéœ€è¦å½“å‰ä»·æ ¼ï¼Œå¯ä»¥ä»å¿«ç…§è·å–
                                current_close = summary.get("close")
                                current_ts = summary.get("timestamp")
                                if current_close is not None:
                                    # æ„å»ºä¸€ä¸ªç®€å•çš„å•è¡Œ DataFrame
                                    import pandas as pd
                                    df = pd.DataFrame([{
                                        "timestamp": pd.to_datetime(current_ts) if current_ts else pd.Timestamp.now(),
                                        "close": float(current_close),
                                        "open": float(current_close),  # ä½¿ç”¨ close ä½œä¸ºè¿‘ä¼¼å€¼
                                        "high": float(current_close),
                                        "low": float(current_close),
                                        "volume": 0.0  # å¿«ç…§ä¸­æ²¡æœ‰ volume
                                    }])
                                    df.set_index("timestamp", inplace=True)
                                    price_source = "snapshot"
                        except Exception as e:
                            # å¦‚æœä»å¿«ç…§æ„å»ºå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨ DataManager
                            pass
            
            # å¦‚æœå¿«ç…§ä¸­æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæˆ–éœ€è¦æ›´å¤šå†å²æ•°æ®ï¼Œä½¿ç”¨ DataManager
            if df is None or df.empty:
                if not self.dm:
                    return json.dumps({"error": "DataManager æœªåˆå§‹åŒ–ä¸”å¿«ç…§ä¸­æ— æ•°æ®"})
                
                # è¯´æ˜ï¼šå¿«ç…§åªåŒ…å«å½“å‰ä»·æ ¼å’Œè¿‡å»3å¤©çš„æ—¥çº¿ä»·æ ¼ç‚¹ï¼Œä¸åŒ…å«å°æ—¶çº¿å†å²æ•°æ®
                # å¦‚æœ LLM éœ€è¦å†å²å°æ—¶çº¿æ•°æ®ï¼ˆlookback_hours > 1ï¼‰ï¼Œå¿…é¡»è°ƒç”¨ DataManager
                if window > 1:
                    print(f"ğŸ“Š å¿«ç…§ä¸­æ— å°æ—¶çº¿å†å²æ•°æ®ï¼Œä½¿ç”¨ DataManager è·å– {normalized_symbol} è¿‡å» {window} å°æ—¶çš„æ•°æ®")
                
                df = self.dm.get_hourly_stock_data(
                    symbol=query_symbol,
                    end_date=end_time, 
                    lookback_hours=window
                )
                price_source = "datamanager"
            else:
                print(f"âœ… ä½¿ç”¨å¿«ç…§æ•°æ®è·å– {normalized_symbol} çš„ä»·æ ¼ï¼ˆé¿å…è°ƒç”¨ DataManagerï¼‰")
            if df.empty:
                fallback_df = self._build_single_price_dataframe(normalized_symbol or symbol, end_time)
                if fallback_df is not None:
                    df = fallback_df
                else:
                    return json.dumps({"error": f"æœªæ‰¾åˆ° {normalized_symbol or symbol} çš„å°æ—¶çº¿æ•°æ®"})

            df = df.sort_index()
            latest = df.iloc[-1]
            latest_ts = latest.name
            latest_close = _to_float(latest.get("close"))
            prev_close = _to_float(df.iloc[-2]["close"]) if len(df) > 1 and "close" in df.columns else None

            recent_closes: List[Optional[float]] = []
            if "close" in df.columns:
                closes_series = df["close"].dropna().tail(min(len(df), self.RECENT_CLOSE_COUNT))
                recent_closes = [_to_float(val) for val in closes_series]

            summary: Dict[str, Any] = {
                "timestamp": str(latest_ts),
                "open": _to_float(latest.get("open", latest_close)),
                "high": _to_float(latest.get("high", latest_close)),
                "low": _to_float(latest.get("low", latest_close)),
                "close": latest_close,
                "volume": _to_float(latest.get("volume")),
                "previous_close": prev_close,
                "recent_closes": recent_closes,
            }
            if prev_close is not None and latest_close is not None and prev_close != 0:
                summary["change"] = round(latest_close - prev_close, 4)
                summary["change_pct"] = round(((latest_close - prev_close) / prev_close) * 100, 4)

            mini_count = min(len(df), self.MINI_CANDLE_COUNT)
            candles: List[Dict[str, Any]] = []
            candles_df = df.tail(mini_count).reset_index()
            for _, row in candles_df.iterrows():
                candles.append({
                    "timestamp": str(row.get("timestamp")),
                    "open": _to_float(row.get("open", row.get("close"))),
                    "high": _to_float(row.get("high", row.get("close"))),
                    "low": _to_float(row.get("low", row.get("close"))),
                    "close": _to_float(row.get("close")),
                    "volume": _to_float(row.get("volume"))
                })

            payload = {
                "symbol": symbol,
                "lookback_hours": window,
                "total_candles_available": int(len(df)),
                "summary": summary,
                "candles": candles
            }
            return json.dumps(payload, ensure_ascii=False)
        except Exception as e:
            return json.dumps({"error": f"è·å– {symbol} å°æ—¶çº¿æ•°æ®æ—¶å‡ºé”™: {str(e)}"})

    def _build_single_price_dataframe(self, symbol: str, timestamp: str) -> Optional[pd.DataFrame]:
        """
        æ„é€ ä»…å«ä¸€æ¡è®°å½•çš„å°æ—¶çº¿ DataFrameï¼Œä½¿ç”¨å‰ä¸€äº¤æ˜“æ—¥æ”¶ç›˜ä»·æˆ– DataManager å¯è·å¾—çš„ä»·æ ¼ã€‚
        """
        normalized_symbol = normalize_symbol(symbol) or symbol
        plain_symbol = strip_exchange_prefix(normalized_symbol) or normalized_symbol

        try:
            ts = pd.to_datetime(timestamp)
        except Exception:
            ts = pd.Timestamp(datetime.now())

        date_str = ts.strftime("%Y-%m-%d")
        fallback_price = self._get_previous_close(normalized_symbol, date_str)

        if (fallback_price is None) and self.dm:
            try:
                fallback_price = self.dm.get_price_at(plain_symbol, ts)
            except Exception:
                fallback_price = None

        if fallback_price is None:
            return None

        record = {
            "timestamp": ts,
            "symbol": plain_symbol,
            "open": float(fallback_price),
            "high": float(fallback_price),
            "low": float(fallback_price),
            "close": float(fallback_price),
            "volume": 0.0,
        }
        df = pd.DataFrame([record])
        df.set_index("timestamp", inplace=True)
        return df
    
    def get_technical_indicators(self, symbol: str, end_date: str, lookback_days: int = 10) -> str:
        """
        è·å–æŠ€æœ¯æŒ‡æ ‡ï¼šè¯»å–å†å²å°æ—¶çº¿æŒ‡æ ‡ + è®¡ç®—å®æ—¶å°æ—¶çº¿æŒ‡æ ‡ + ä¿å­˜æ›´æ–°åˆ° ai_stock_data.json
        
        Args:
            symbol (str): è‚¡ç¥¨ä»£ç 
            end_date (str): ç»“æŸæ—¥æœŸ (YYYY-MM-DD) æˆ–ç»“æŸæ—¶é—´ (YYYY-MM-DD HH:MM:SS)
            lookback_days (int): ç”¨äºè®¡ç®—æŒ‡æ ‡çš„å†å²æ•°æ®å¤©æ•°ï¼Œé»˜è®¤30å¤©ï¼ˆè½¬æ¢ä¸ºå°æ—¶æ•°ï¼‰
            
        Returns:
            str: JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å†å²å°æ—¶çº¿æŒ‡æ ‡å’Œå®æ—¶å°æ—¶çº¿æŒ‡æ ‡
        """
        if not self.dm:
            return json.dumps({"error": "DataManager æœªåˆå§‹åŒ–"})
        
        normalized_symbol = normalize_symbol(symbol)
        if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
            return json.dumps({
                "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„ç ”ç©¶åå•ä¸­",
                "allowed_symbols": self._allowed_symbol_list(),
                "symbol": normalized_symbol
            }, ensure_ascii=False)
        
        try:
            # 1. è¯»å–å†å²å°æ—¶çº¿æŒ‡æ ‡ä¸è¡Œæƒ…ï¼ˆä» ai_stock_data.jsonï¼Œä½¿ç”¨JsonFileManagerï¼‰
            historical_indicators: List[Dict[str, Any]] = []
            historical_price_data: List[Dict[str, Any]] = []
            ai_stock_data_path = self.stock_json_path or os.path.join(project_root, "data_flow", "ai_stock_data.json")
            from utils.json_file_manager import safe_read_json
            
            all_data = safe_read_json(ai_stock_data_path, default={})
            stock_entry: Optional[Dict[str, Any]] = None
            for key in [symbol, f"SH{symbol}", f"SZ{symbol}"]:
                entry = all_data.get(key)
                if entry:
                    stock_entry = entry
                    break
            
            def _reload_stock_entry() -> Optional[Dict[str, Any]]:
                refreshed = safe_read_json(ai_stock_data_path, default={})
                for candidate in [symbol, f"SH{symbol}", f"SZ{symbol}"]:
                    entry = refreshed.get(candidate)
                    if entry:
                        return entry
                return None

            if stock_entry:
                if "å°æ—¶çº¿æŒ‡æ ‡" in stock_entry:
                    historical_indicators = stock_entry["å°æ—¶çº¿æŒ‡æ ‡"] or []
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_indicators)} æ¡å†å²å°æ—¶çº¿æŠ€æœ¯æŒ‡æ ‡ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
                if "å°æ—¶çº¿è¡Œæƒ…" in stock_entry:
                    historical_price_data = stock_entry["å°æ—¶çº¿è¡Œæƒ…"] or []
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_price_data)} æ¡å†å²å°æ—¶çº¿è¡Œæƒ…ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
            else:
                print(f"âš ï¸ æœªåœ¨ {ai_stock_data_path} æ‰¾åˆ° {symbol} çš„å†å²è®°å½•ï¼Œå°è¯•ä» TinySoft å›å¡«ã€‚")

            max_expected_candles = lookback_days * 4
            if not historical_price_data or len(historical_price_data) < max_expected_candles:
                try:
                    ndays = max(lookback_days, 10)
                    self.dm.save_ts_data(symbols=[symbol], ndays=ndays, out_path=ai_stock_data_path)
                    stock_entry = _reload_stock_entry()
                    if stock_entry and "å°æ—¶çº¿è¡Œæƒ…" in stock_entry:
                        historical_price_data = stock_entry["å°æ—¶çº¿è¡Œæƒ…"] or []
                        print(f"ğŸ“¡ é€šè¿‡ TinySoft å›å¡« {len(historical_price_data)} æ¡å†å²å°æ—¶çº¿è¡Œæƒ…ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
                except Exception as e:
                    print(f"âš ï¸ å›å¡«å°æ—¶çº¿è¡Œæƒ…å¤±è´¥ï¼š{e}")

            if not historical_indicators and stock_entry and "å°æ—¶çº¿æŒ‡æ ‡" in stock_entry:
                historical_indicators = stock_entry["å°æ—¶çº¿æŒ‡æ ‡"] or []
                if historical_indicators:
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_indicators)} æ¡å†å²å°æ—¶çº¿æŠ€æœ¯æŒ‡æ ‡ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
            
            # 2. ç»„è£…æŒ‡æ ‡å­—æ®µç™½åå•
            indicator_keys = [
                'SMA_10',
                'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',
                'RSI_10',
                'BBL_10_2.0', 'BBM_10_2.0', 'BBU_10_2.0'
            ]

            def build_indicator_payload(source_label: str, indicator_dict: Dict[str, Any], timestamp_hint: Optional[Any] = None, include_saved_path: bool = False) -> str:
                summary = {key: indicator_dict.get(key) for key in indicator_keys}
                payload: Dict[str, Any] = {
                    "source": source_label,
                    "symbol": symbol,
                    "timestamp": str(timestamp_hint) if timestamp_hint else str(end_date),
                    "indicators": summary
                }
                if include_saved_path:
                    payload["saved_to"] = ai_stock_data_path
                return json.dumps(payload, ensure_ascii=False)
            
            # 3. è·å–å®æ—¶å°æ—¶çº¿æ•°æ®ï¼ˆè½¬æ¢ä¸ºå°æ—¶æ•°ï¼šæ¯å¤©4å°æ—¶äº¤æ˜“æ—¶é—´ï¼Œä¹˜ä»¥å¤©æ•°ï¼‰
            lookback_hours = lookback_days * 4  # æ¯å¤©4å°æ—¶äº¤æ˜“æ—¶é—´ï¼ˆ9:30-11:30, 13:00-15:00ï¼‰
            # å¦‚æœ end_date åªåŒ…å«æ—¥æœŸï¼Œæ·»åŠ å½“å‰æ—¶é—´
            if len(end_date) == 10:  # YYYY-MM-DD
                end_time = f"{end_date} 15:00:00"  # ä½¿ç”¨æ”¶ç›˜æ—¶é—´
            else:
                end_time = end_date
            
            df_realtime = self.dm.get_hourly_stock_data(
                symbol=symbol, 
                end_date=end_time, 
                lookback_hours=lookback_hours
            )
            
            # 4. åˆå¹¶å†å²+å®æ—¶æ•°æ®
            df = pd.DataFrame()  # æœ€ç»ˆåˆå¹¶åçš„DataFrame
            
            # å¦‚æœæœ‰å†å²ä»·æ ¼æ•°æ®ï¼Œå…ˆè½¬æ¢ä¸ºDataFrame
            if historical_price_data:
                if len(historical_price_data) > max_expected_candles:
                    historical_price_data = historical_price_data[-max_expected_candles:]
                try:
                    df_historical = pd.DataFrame(historical_price_data)
                    # è½¬æ¢æ—¥æœŸåˆ—ä¸ºdatetimeï¼ˆnaiveï¼Œæ— æ—¶åŒºï¼‰
                    df_historical['timestamp'] = pd.to_datetime(df_historical['date'], utc=False)
                    # å¦‚æœè½¬æ¢åæœ‰æ—¶åŒºï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯ï¼ˆè½¬ä¸ºnaiveï¼‰
                    if df_historical['timestamp'].dt.tz is not None:
                        df_historical['timestamp'] = df_historical['timestamp'].dt.tz_localize(None)
                    # é‡å‘½ååˆ—ä»¥åŒ¹é…DataFrameæ ¼å¼
                    df_historical = df_historical.rename(columns={
                        'close': 'close',
                        'vol': 'volume'
                    })
                    # åªä¿ç•™éœ€è¦çš„åˆ—
                    if 'close' in df_historical.columns:
                        df_historical = df_historical[['timestamp', 'close', 'volume']]
                        df_historical.set_index('timestamp', inplace=True)
                        df = df_historical.copy()
                        print(f"ğŸ“š å·²åŠ è½½ {len(df)} æ¡å†å²å°æ—¶çº¿ä»·æ ¼æ•°æ®ï¼ˆnaive datetimeï¼‰")
                except Exception as e:
                    print(f"âš ï¸ è½¬æ¢å†å²ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            
            # åˆå¹¶å®æ—¶æ•°æ®ï¼ˆç¡®ä¿æ—¶åŒºä¸€è‡´ï¼‰
            if not df_realtime.empty:
                # åªä½¿ç”¨closeå’Œvolumeåˆ—
                df_realtime_subset = df_realtime[['close', 'volume']].copy()
                # ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼ˆè½¬ä¸ºnaive datetimeï¼Œé¿å…æ—¶åŒºæ¯”è¾ƒé”™è¯¯ï¼‰
                if df_realtime_subset.index.tz is not None:
                    df_realtime_subset.index = df_realtime_subset.index.tz_localize(None)
                # åˆå¹¶ï¼ˆå»é‡ï¼Œä¿ç•™æœ€æ–°çš„ï¼‰
                if not df.empty:
                    # åˆå¹¶ï¼Œå®æ—¶æ•°æ®è¦†ç›–å†å²æ•°æ®ä¸­çš„ç›¸åŒæ—¶é—´æˆ³
                    df = pd.concat([df, df_realtime_subset])
                    df = df[~df.index.duplicated(keep='last')]  # ä¿ç•™æœ€æ–°çš„
                    df = df.sort_index()
                else:
                    df = df_realtime_subset
                print(f"ğŸ“¡ å·²åˆå¹¶ {len(df_realtime)} æ¡å®æ—¶å°æ—¶çº¿ä»·æ ¼æ•°æ®ï¼Œæ€»è®¡ {len(df)} æ¡ï¼ˆå·²ç»Ÿä¸€ä¸ºnaive datetimeï¼‰")
            
            # é™åˆ¶çª—å£å¤§å°
            if not df.empty and len(df) > max_expected_candles:
                df = df.tail(max_expected_candles)
            
            # æ£€æŸ¥åˆå¹¶åçš„æ•°æ®é‡ï¼ˆå°æ—¶çº¿è‡³å°‘éœ€è¦10ä¸ªäº¤æ˜“æ—¥ * 4å°æ—¶ = 40æ¡ï¼‰
            min_hours_needed = max_expected_candles
            if df.empty:
                # å¦‚æœåˆå¹¶åä»ä¸ºç©ºï¼Œè¿”å›å†å²æŒ‡æ ‡
                if historical_indicators:
                    print(f"âš ï¸ æ— ä»·æ ¼æ•°æ®ï¼Œè¿”å›å†å²æŒ‡æ ‡")
                    latest_hist = historical_indicators[-1] if historical_indicators else {}
                    ts_hint = latest_hist.get("timestamp") or latest_hist.get("date")
                    return build_indicator_payload("historical_only", latest_hist, ts_hint)
                else:
                    return json.dumps({"error": f"æœªæ‰¾åˆ° {symbol} çš„è‚¡ç¥¨æ•°æ®"})
            
            # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            if len(df) < min_hours_needed:
                if historical_indicators:
                    print(f"âš ï¸ åˆå¹¶åæ•°æ®ä¸è¶³ï¼ˆ{len(df)}æ¡ < {min_hours_needed}æ¡ï¼‰ï¼Œè¿”å›å†å²æŒ‡æ ‡")
                    latest_hist = historical_indicators[-1] if historical_indicators else {}
                    ts_hint = latest_hist.get("timestamp") or latest_hist.get("date")
                    return build_indicator_payload("historical_only", latest_hist, ts_hint)
                else:
                    return json.dumps({
                        "error": f"æ•°æ®é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{min_hours_needed}æ¡å°æ—¶çº¿æ•°æ®ï¼Œåˆå¹¶ååªæœ‰{len(df)}æ¡"
                    }, ensure_ascii=False)
            
            # 5. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆä½¿ç”¨åˆå¹¶åçš„å®Œæ•´å°æ—¶çº¿æ•°æ®ï¼‰
            try:
                # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
                df = df.sort_index()
                
                # ç¡®ä¿DataFrameæœ‰.taå±æ€§ï¼ˆpandas_taéœ€è¦æ­£ç¡®åˆå§‹åŒ–ï¼‰
                if not hasattr(df, 'ta'):
                    # å¦‚æœ.taå±æ€§ä¸å­˜åœ¨ï¼Œå°è¯•é‡æ–°å¯¼å…¥å¹¶æ³¨å†Œ
                    try:
                        import pandas_ta as ta
                        # pandas_taé€šè¿‡monkey patchçš„æ–¹å¼æ·»åŠ åˆ°DataFrameï¼Œç¡®ä¿å·²åŠ è½½
                        if not hasattr(pd.DataFrame, 'ta'):
                            import pandas_ta.core as pta
                    except Exception as e:
                        print(f"âš ï¸ pandas_taåˆå§‹åŒ–å¤±è´¥: {e}")
                        raise Exception(f"pandas_taä¸å¯ç”¨: {e}")
                
                # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—ç”¨äºè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                if 'close' not in df.columns:
                    raise Exception("DataFrameç¼ºå°‘'close'åˆ—")
                
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆåŸºäºå°æ—¶çº¿æ•°æ®ï¼‰
                # ä½¿ç”¨10å¤©å‚æ•°è®¡ç®—æŒ‡æ ‡
                df.ta.sma(length=10, append=True)
                df.ta.macd(append=True)
                df.ta.rsi(length=10, append=True)
                df.ta.bbands(length=10, append=True)
                print(f"ğŸ“Š åŸºäºåˆå¹¶å°æ—¶çº¿æ•°æ®ï¼ˆå†å²+å®æ—¶ï¼Œå…±{len(df)}æ¡ï¼‰è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ10å¤©ï¼‰")
            except Exception as e:
                print(f"âš ï¸ è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
                if historical_indicators:
                    latest_hist = historical_indicators[-1] if historical_indicators else {}
                    ts_hint = latest_hist.get("timestamp") or latest_hist.get("date")
                    return build_indicator_payload("historical_only", latest_hist, ts_hint)
                else:
                    return json.dumps({"error": f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {str(e)}"})
            
            # 6. æå–æœ€æ–°çš„æŒ‡æ ‡
            latest_row = df.iloc[-1]
            latest_indicators: Dict[str, Any] = {}
            for key in indicator_keys:
                value = latest_row.get(key) if hasattr(latest_row, "get") else latest_row[key] if key in latest_row else None
                latest_indicators[key] = value
            
            # æ¸…ç† NaN å€¼
            for key, value in latest_indicators.items():
                if pd.isna(value):
                    latest_indicators[key] = None

            print(f"ğŸ“Š å·²è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆåŸºäºåˆå¹¶å°æ—¶çº¿æ•°æ®ï¼šå†å²+å®æ—¶ï¼Œå…±{len(df)}æ¡ï¼‰")
            
            # 7. ä¿å­˜åˆ° ai_stock_data.jsonï¼ˆä½¿ç”¨ DataManager çš„ save_ts_dataï¼Œè‡ªåŠ¨åˆå¹¶å†å²ï¼‰
            try:
                self.dm.save_ts_data(symbols=[symbol], ndays=60, out_path=ai_stock_data_path)
                print(f"ğŸ’¾ å·²æ›´æ–°å°æ—¶çº¿æŠ€æœ¯æŒ‡æ ‡åˆ° {ai_stock_data_path}ï¼ˆåˆå¹¶å†å²æ•°æ®ï¼‰")
                
                # é‡æ–°è¯»å–åˆå¹¶åçš„å®Œæ•´æŒ‡æ ‡æ•°æ®ï¼ˆä½¿ç”¨JsonFileManagerï¼‰
                all_data_updated = safe_read_json(ai_stock_data_path, default={})
                for key in [symbol, f"SH{symbol}", f"SZ{symbol}"]:
                    if key in all_data_updated and "å°æ—¶çº¿æŒ‡æ ‡" in all_data_updated[key]:
                        # ä½¿ç”¨å®Œæ•´çš„åˆå¹¶åæ•°æ®
                        historical_indicators = all_data_updated[key]["å°æ—¶çº¿æŒ‡æ ‡"]
                        print(f"âœ… åˆå¹¶åæŒ‡æ ‡ï¼š{len(historical_indicators)} æ¡")
                        break
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            
            # 8. è¿”å›ç»„åˆç»“æœï¼ˆä»…æœ€æ–°æŒ‡æ ‡ï¼‰
            indicator_timestamp = None
            if not df.empty:
                indicator_timestamp = df.index[-1]

            return build_indicator_payload(
                "combined_merged",
                latest_indicators,
                indicator_timestamp,
                include_saved_path=True
            )
        except Exception as e:
            return json.dumps({"error": f"è·å–æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}"})

    def get_current_position_tool(self, today_date: str) -> str:
        """ä»æŒä»“æ–‡ä»¶ä¸­è·å–ä»£ç†çš„æœ€æ–°äº¤æ˜“æŒä»“ã€‚æ­¤å‡½æ•°è¯»å–ä»£ç†çš„æŒä»“æ–‡ä»¶ä»¥æ‰¾åˆ°å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æŒä»“ã€‚"""
        try:
            positions, _, _ = get_current_position(today_date, self.signature)
            return json.dumps(positions)
        except Exception as e:
            return json.dumps({"error": f"è·å–æœ€æ–°æŒä»“æ—¶å‡ºé”™: {str(e)}"})
    
    def add_no_trade_record_tool(self, today_date: str) -> str:
        """ä¸ºå½“å‰ä»£ç†åœ¨ç»™å®šæ—¥æœŸè®°å½•"æ— äº¤æ˜“"æ“ä½œã€‚æ­¤å‡½æ•°æ›´æ–°ä»£ç†çš„æŒä»“æ–‡ä»¶ä»¥å»¶ç»­å‰ä¸€å¤©çš„æŒä»“ã€‚"""
        try:
            # ä¼˜å…ˆä½¿ç”¨å®ä¾‹ä¸Šä¸‹æ–‡ï¼Œé¿å…å¹¶å‘çŠ¶æ€æ±¡æŸ“
            decision_time = self._get_context_value("CURRENT_TIME") or f"{today_date} 00:00:00"
            decision_count_raw = self._get_context_value("DECISION_COUNT")
            decision_count = int(decision_count_raw) if decision_count_raw is not None else 0
            add_no_trade_record(today_date, decision_time, decision_count, self.signature)
            return json.dumps({
                "success": True,
                "action": "no_trade",
                "date": today_date,
                "decision_time": decision_time,
                "agentic workflow": self.signature
            })
        except Exception as e:
            return json.dumps({"error": f"æ·»åŠ æ— äº¤æ˜“è®°å½•æ—¶å‡ºé”™: {str(e)}"})

    def _get_prefetched_trade_price(self, normalized_symbol: str) -> Tuple[Optional[float], Optional[str]]:
        """
        ä»å…±äº« prefetch snapshot ä¸­å–è¯¥è‚¡ç¥¨åœ¨â€œå½“å‰å†³ç­–é”šç‚¹â€å¯¹åº”çš„ä»·æ ¼ï¼Œç”¨äºäº¤æ˜“æ‰§è¡Œã€‚
        ç›®çš„ï¼šé¿å…å¤šè¿›ç¨‹/å¤šæ¨¡å‹ä¸‹å„è‡ª DataManager çŠ¶æ€ä¸åŒå¯¼è‡´ä»·æ ¼ç¼ºå¤±ï¼Œä»è€Œå‡ºç°â€œæœ‰çš„æŠ¥é”™æœ‰çš„ä¸æŠ¥é”™â€ã€‚
        """
        try:
            payload = (
                self._prefetched_prices.get(normalized_symbol)
                if isinstance(self._prefetched_prices, dict)
                else None
            )
            if not isinstance(payload, dict):
                return None, None

            # ä¼˜å…ˆå– summary.closeï¼ˆå·²å¯¹é½åˆ°å½“å‰å†³ç­–é”šç‚¹ï¼‰
            summary = payload.get("summary")
            if isinstance(summary, dict):
                close = summary.get("close")
                ts = summary.get("timestamp")
                try:
                    close_num = float(close) if close is not None else None
                except Exception:
                    close_num = None
                if close_num is not None:
                    try:
                        if pd.isna(close_num) or close_num <= 0:
                            close_num = None
                    except Exception:
                        if close_num <= 0:
                            close_num = None
                if close_num is not None:
                    return close_num, str(ts) if ts is not None else None

            # fallbackï¼šprices_3d çš„æœ€åä¸€ä¸ª close
            prices_3d = payload.get("prices_3d")
            if isinstance(prices_3d, list) and prices_3d:
                last = prices_3d[-1] if isinstance(prices_3d[-1], dict) else None
                if isinstance(last, dict):
                    close = last.get("close")
                    ts = last.get("timestamp")
                    try:
                        close_num = float(close) if close is not None else None
                    except Exception:
                        close_num = None
                    if close_num is not None:
                        try:
                            if pd.isna(close_num) or close_num <= 0:
                                close_num = None
                        except Exception:
                            if close_num <= 0:
                                close_num = None
                    if close_num is not None:
                        return close_num, str(ts) if ts is not None else None
        except Exception:
            return None, None
        return None, None
    
    def buy_stock(self, symbol: str, amount: int) -> str:
        """
        ä¹°å…¥è‚¡ç¥¨ï¼ˆä½¿ç”¨å½“å‰å°æ—¶çº§ä»·æ ¼ï¼‰ã€‚
        
        æ­¤å‡½æ•°æ¨¡æ‹Ÿè‚¡ç¥¨ä¹°å…¥æ“ä½œï¼ŒåŒ…æ‹¬ï¼š
        1. è·å–å½“å‰æŒä»“å’Œæ“ä½œID
        2. è·å–å½“å‰å°æ—¶çš„è‚¡ç¥¨ä»·æ ¼ï¼ˆä¼˜å…ˆå°æ—¶çº§ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·ï¼‰
        3. éªŒè¯ä¹°å…¥æ¡ä»¶ï¼ˆç°é‡‘æ˜¯å¦å……è¶³ï¼‰
        4. æ›´æ–°æŒä»“ï¼ˆå¢åŠ è‚¡ç¥¨æ•°é‡ï¼Œå‡å°‘ç°é‡‘ï¼‰
        5. è®°å½•äº¤æ˜“åˆ° position.jsonl æ–‡ä»¶
        
        Args:
            symbol (str): è‚¡ç¥¨ä»£ç ï¼Œå¦‚ "600519"
            amount (int): ä¹°å…¥æ•°é‡ï¼Œå¿…é¡»æ˜¯100çš„æ•´æ•°å€
        
        Returns:
            str: JSON å­—ç¬¦ä¸²ï¼ŒæˆåŠŸæ—¶è¿”å›æ–°æŒä»“ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
        """
        try:
            normalized_symbol = normalize_symbol(symbol)
            if not normalized_symbol:
                return json.dumps({"error": "æ— æ•ˆçš„è‚¡ç¥¨ä»£ç "})
            data_symbol = strip_exchange_prefix(normalized_symbol) or normalized_symbol

            if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
                return json.dumps({
                    "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„æŒä»“/äº¤æ˜“åå•ä¸­",
                    "allowed_symbols": self._allowed_symbol_list(),
                    "symbol": normalized_symbol
                }, ensure_ascii=False)

            if not self._is_allowed_symbol(normalized_symbol):
                return json.dumps({
                    "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„äº¤æ˜“åå•ä¸­",
                    "allowed_symbols": self._allowed_symbol_list(),
                    "symbol": normalized_symbol
                }, ensure_ascii=False)
            
            # ä½¿ç”¨å®ä¾‹ä¸Šä¸‹æ–‡ï¼Œé¿å…ä»å…±äº« runtime_env.json è¯»å–
            today_date = self._get_context_value("TODAY_DATE")
            current_time = self._get_context_value("CURRENT_TIME")
            decision_time = current_time or f"{today_date} 00:00:00"
            decision_count_raw = self._get_context_value("DECISION_COUNT")
            decision_count = int(decision_count_raw) if decision_count_raw is not None else 0
            if not today_date:
                return json.dumps({"error": "æœªè®¾ç½® TODAY_DATE"})
            decision_time = normalize_decision_time(today_date, decision_time)
            
            # äº¤æ˜“å•ä½æ£€æŸ¥ (100è‚¡çš„æ•´æ•°å€)
            if amount <= 0 or amount % 100 != 0:
                return json.dumps({
                    "error": "ä¹°å…¥æ•°é‡å¿…é¡»æ˜¯100çš„æ•´æ•°å€ä¸”å¤§äº0",
                    "symbol": normalized_symbol,
                    "amount": amount
                })

            # è·å–å½“å‰æŒä»“å’Œæ“ä½œID
            current_position, current_action_id, latest_record = get_current_position(today_date, self.signature)
            
            # è·å–å½“å‰æ—¶åˆ»çš„è‚¡ç¥¨ä»·æ ¼ï¼š
            # ä¼˜å…ˆä½¿ç”¨å…±äº« snapshot çš„ä»·æ ¼ï¼›ç¼ºå¤±æ—¶å†å›é€€åˆ° DataManagerï¼ˆå°æ—¶çº§â†’æ—¥çº¿ï¼‰ï¼Œé¿å…å¤šæ¨¡å‹ä»·æ ¼ä¸ä¸€è‡´
            price_source = None
            try:
                this_symbol_price = None

                snapshot_price, snapshot_ts = self._get_prefetched_trade_price(normalized_symbol)
                if snapshot_price is not None:
                    this_symbol_price = snapshot_price
                    price_source = "prefetch_snapshot"
                    ts_text = snapshot_ts or current_time or today_date
                    print(f"ğŸ’¹ ä½¿ç”¨å…±äº«å¿«ç…§ä»·æ ¼: {normalized_symbol} = Â¥{this_symbol_price} ({ts_text})")

                # è‹¥ snapshot ç¼ºå¤±ï¼Œåˆ™èµ° DataManagerï¼ˆå°æ—¶çº§ï¼‰
                if this_symbol_price is None or pd.isna(this_symbol_price):
                    if self.dm and current_time:
                        hourly_data = self.dm.get_hourly_stock_data(
                            symbol=data_symbol,
                            end_date=current_time,
                            lookback_hours=1,
                        )
                        if hourly_data is not None and not hourly_data.empty:
                            this_symbol_price = float(hourly_data["close"].iloc[-1])
                            price_source = "dm_hourly"
                        print(f"ğŸ’¹ ä½¿ç”¨å°æ—¶çº§ä»·æ ¼: {normalized_symbol} = Â¥{this_symbol_price} ({current_time})")
                
                # å¦‚æœæ²¡æœ‰å°æ—¶çº§æ•°æ®ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·
                if this_symbol_price is None or pd.isna(this_symbol_price):
                    if not self.dm:
                        return json.dumps(
                            {
                                "error": f"æœªæ‰¾åˆ°è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®",
                                "symbol": normalized_symbol,
                                "date": today_date,
                                "detail": "snapshot ç¼ºå¤±ä¸” DataManager ä¸å¯ç”¨",
                            },
                            ensure_ascii=False,
                        )
                    stock_data = self.dm.get_stock_data(symbol=data_symbol, end_date=today_date, lookback_days=1)
                    if stock_data is None or stock_data.empty:
                        return json.dumps(
                            {
                                "error": f"æœªæ‰¾åˆ°è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®",
                                "symbol": normalized_symbol,
                                "date": today_date,
                            },
                            ensure_ascii=False,
                        )
                    this_symbol_price = (
                        float(stock_data["open"].iloc[-1])
                        if "open" in stock_data.columns
                        else float(stock_data["close"].iloc[-1])
                    )
                    price_source = "dm_daily_open"
                    print(f"ğŸ’¹ ä½¿ç”¨å¼€ç›˜ä»·: {normalized_symbol} = Â¥{this_symbol_price}")
                
                if pd.isna(this_symbol_price) or this_symbol_price <= 0:
                    return json.dumps(
                        {
                            "error": f"è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®æ— æ•ˆ",
                            "symbol": normalized_symbol,
                            "date": today_date,
                            "price": this_symbol_price,
                            "price_source": price_source,
                        },
                        ensure_ascii=False,
                    )
            except Exception as e:
                return json.dumps(
                    {
                        "error": f"è·å–è‚¡ç¥¨ä»·æ ¼å¤±è´¥: {str(e)}",
                        "symbol": normalized_symbol,
                        "date": today_date,
                        "price_source": price_source,
                    },
                    ensure_ascii=False,
                )
            
            limit_info: Optional[Dict[str, float]] = None
            prev_close = self._get_previous_close(normalized_symbol, today_date)
            limit_info = get_price_limits(normalized_symbol, prev_close)
            allowed, reason = self._passes_price_limit_liquidity("sell", this_symbol_price, limit_info)
            if not allowed:
                return json.dumps({
                    "error": reason,
                    "symbol": normalized_symbol,
                    "price": this_symbol_price,
                    "limit_info": limit_info
                }, ensure_ascii=False)
            
            # --- é£é™©ç®¡ç†æ£€æŸ¥ ---
            single_stock_max_position = self.risk_management.get("single_stock_max_position", 0.50)
            total_assets = current_position.get("CASH", 0)
            for stock, data in current_position.items():
                if stock != "CASH":
                    # å‡è®¾æˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»·æ ¼æ¥ä¼°ç®—å½“å‰è‚¡ç¥¨ä»·å€¼ï¼Œè¿™é‡Œç”¨ä»Šå¤©çš„å¼€ç›˜ä»·
                    # åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„ä»·æ ¼è·å–é€»è¾‘
                    stock_value = data.get("shares", 0) * this_symbol_price # ä¼°ç®—
                    total_assets += stock_value

            required_cash = this_symbol_price * amount
            if (required_cash / total_assets) > single_stock_max_position:
                 return json.dumps({
                    "error": f"å•åªè‚¡ç¥¨æŒä»“è¶…è¿‡ä¸Šé™ ({single_stock_max_position * 100}%)",
                    "symbol": normalized_symbol,
                    "max_allowed_investment": total_assets * single_stock_max_position,
                    "requested_investment": required_cash
                })

            # --- äº¤æ˜“æˆæœ¬è®¡ç®— ---
            commission_rate = self.trading_rules.get("commission_rate", 0.0003)
            min_commission = self.trading_rules.get("min_commission", 5.0)
            commission = max(required_cash * commission_rate, min_commission)
            total_cost = required_cash + commission

            # éªŒè¯ä¹°å…¥æ¡ä»¶
            cash_left = current_position.get("CASH", 0) - total_cost
            
            if cash_left < 0:
                return json.dumps({
                    "error": "ç°é‡‘ä¸è¶³ï¼ˆå·²è€ƒè™‘äº¤æ˜“è´¹ç”¨ï¼‰ï¼äº¤æ˜“ä¸è¢«å…è®¸ã€‚",
                    "total_cost": total_cost,
                    "cash_available": current_position.get("CASH", 0),
                    "symbol": normalized_symbol,
                    "date": today_date
                })
            
            limit_info: Optional[Dict[str, float]] = None
            prev_close = self._get_previous_close(normalized_symbol, today_date)
            limit_info = get_price_limits(normalized_symbol, prev_close)
            allowed, reason = self._passes_price_limit_liquidity("buy", this_symbol_price, limit_info)
            if not allowed:
                return json.dumps({
                    "error": reason,
                    "symbol": normalized_symbol,
                    "price": this_symbol_price,
                    "limit_info": limit_info
                }, ensure_ascii=False)
            
            # æ‰§è¡Œä¹°å…¥æ“ä½œ
            new_position = copy.deepcopy(current_position)
            new_position["CASH"] = cash_left
            
            # æ›´æ–°æŒä»“ï¼ˆç»´æŠ¤åŠ æƒå¹³å‡æˆæœ¬ï¼‰
            if normalized_symbol in new_position:
                existing_entry = new_position[normalized_symbol]
                current_shares = existing_entry.get("shares", 0)
                existing_avg = existing_entry.get("avg_price")
                if existing_avg is None:
                    existing_avg = this_symbol_price
                total_shares = current_shares + amount
                if total_shares > 0:
                    weighted_avg = ((existing_avg * current_shares) + (this_symbol_price * amount)) / total_shares
                else:
                    weighted_avg = this_symbol_price
                existing_entry["shares"] = total_shares
                existing_entry["avg_price"] = weighted_avg
            else:
                new_position[normalized_symbol] = {
                    "shares": amount,
                    "purchase_date": today_date,
                    "avg_price": this_symbol_price
                }

            new_position = normalize_positions(new_position)

            # è®°å½•äº¤æ˜“
            record: Dict[str, Any] = {
                "date": today_date,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "this_action": {"action": "buy", "symbol": normalized_symbol, "amount": amount},
                "positions": new_position
            }
            if latest_record and latest_record.get("decision_time") == decision_time:
                record["id"] = latest_record.get("id")
            else:
                record["id"] = current_action_id + 1
            upsert_position_record(self.signature, record)
            
            write_runtime_config_value("IF_TRADE", True)
            return json.dumps({
                "success": True,
                "action": "buy",
                "symbol": normalized_symbol,
                "amount": amount,
                "price": this_symbol_price,
                "cost": required_cash,
                "commission": commission,
                "price_limit": limit_info,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "new_position": new_position
            })
        
        except Exception as e:
            return json.dumps({"error": f"ä¹°å…¥è‚¡ç¥¨æ—¶å‡ºé”™: {str(e)}"})
    
    def search_stock_news(self, query: str, max_retries: int = 3) -> str:
        """
        æœç´¢è‚¡ç¥¨ç›¸å…³çš„å®æ—¶æ–°é—» + è¯»å–å†å²æ–°é—»ï¼Œä½¿ç”¨ AKShareï¼Œå¤±è´¥é‡è¯•ã€‚
        åŒæ—¶ä¼šä» DataManager è¯»å–å†å²æ–°é—»ï¼Œå¹¶å°†æ–°é—»ä¿å­˜åˆ° news.csv
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "600519 æœ€æ–°æ¶ˆæ¯"ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
        
        Returns:
            str: JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å†å²æ–°é—»å’Œå®æ—¶æ–°é—»
        """
        # æå– 6 ä½ä»£ç ï¼ˆå…¼å®¹ï¼š600519 / 688008 / SH688008 / "SH688008 æœ€æ–°æ¶ˆæ¯" / "688008 æœ€æ–°æ¶ˆæ¯"ï¼‰
        symbol: Optional[str] = None
        raw_query = str(query or "").strip()
        upper_query = raw_query.upper()
        if len(upper_query) == 8 and upper_query[:2] in ("SH", "SZ") and upper_query[2:].isdigit():
            symbol = upper_query[2:]
        elif raw_query.isdigit() and len(raw_query) == 6:
            symbol = raw_query
        else:
            import re
            # æ‰¾åˆ°ä»»æ„éæ•°å­—è¾¹ç•Œä¸Šçš„ 6 ä½æ•°å­—ï¼Œå…è®¸å‰é¢æœ‰ SH/SZ å‰ç¼€
            m = re.search(r"(?i)(?:SH|SZ)?(?<!\d)(\d{6})(?!\d)", upper_query)
            if m:
                symbol = m.group(1)

        if not symbol:
            return json.dumps({"success": False, "message": "è¯·æä¾›6ä½Aè‚¡ä»£ç ï¼Œä¾‹å¦‚ 600519"}, ensure_ascii=False)

        normalized_symbol = normalize_symbol(symbol)
        symbol_for_query = strip_exchange_prefix(normalized_symbol) or symbol

        if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
            return json.dumps({
                "success": False,
                "message": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„ç ”ç©¶åå•ä¸­ï¼Œè¯·èšç„¦é¢„è®¾çš„ç§‘åˆ›æ¿åˆ—è¡¨ã€‚",
                "allowed_symbols": self._allowed_symbol_list()
            }, ensure_ascii=False)

        cached_news = self._prefetched_news.get(normalized_symbol)
        if cached_news:
            return json.dumps(cached_news, ensure_ascii=False)

        today_date = get_runtime_config_value("TODAY_DATE")
        current_time = get_runtime_config_value("CURRENT_TIME")
        if current_time:
            search_time = current_time
        elif today_date:
            search_time = f"{today_date} 00:00:00"
        else:
            search_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # å¢é‡è¿‡æ»¤é€»è¾‘å·²ç§»é™¤ï¼Œæ”¹å›è¿”å›å®Œæ•´å†å²+å®æ—¶æ–°é—»

        # 1. è¯»å–å†å²æ–°é—»ï¼ˆä» DataManagerï¼‰
        historical_news = []
        if self.dm and self.dm.news_df is not None:
            try:
                news_df = self.dm.get_news(end_date=today_date, symbols=[symbol_for_query], limit=20)
                news_df = self._filter_allowed_news_df(news_df)
                if news_df is not None and not news_df.empty:
                    # å°†æ‰€æœ‰åˆ—è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… Timestamp åºåˆ—åŒ–é—®é¢˜
                    cleaned_news_df = self._sanitize_news_dataframe(news_df.astype(str))
                    historical_news = self._filter_allowed_news_records(cleaned_news_df.to_dict('records'))
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_news)} æ¡å†å²æ–°é—»ï¼ˆè‚¡ç¥¨ï¼š{normalized_symbol or symbol_for_query}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ è¯»å–å†å²æ–°é—»å¤±è´¥: {e}")

        # 2. è·å–å®æ—¶æ–°é—»ï¼ˆä¸œæ–¹è´¢å¯Œæœç´¢æ¥å£ï¼Œå¸¦é‡è¯•ï¼‰
        realtime_results = []
        csv_path = os.path.join('data_flow', 'news.csv')
        
        # åˆ›å»ºè°ƒè¯•æ–‡ä»¶å¤¹ç”¨äºä¿å­˜é”™è¯¯ä¿¡æ¯
        debug_dir = os.path.join('data_flow', 'debug', 'akshare_errors')
        os.makedirs(debug_dir, exist_ok=True)
        
        for attempt in range(1, max_retries + 1):
            # åˆå§‹åŒ–è¯·æ±‚ä¿¡æ¯ï¼ˆåœ¨æ‰€æœ‰ä»£ç è·¯å¾„ä¸­éƒ½éœ€è¦ï¼‰
            request_info = {
                'symbol': symbol_for_query,
                'function': 'stock_news_em_safe',
                'parameters': {'symbol': symbol_for_query}
            }
            
            try:
                import time
                import traceback
                
                # é‡è¯•å‰æ·»åŠ å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
                # ç¬¬ä¸€æ¬¡è¯·æ±‚ä¹Ÿå»¶è¿Ÿ1ç§’ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if attempt == 1:
                    wait_time_before = 1  # ç¬¬ä¸€æ¬¡å»¶è¿Ÿ1ç§’
                    print(f"â³ è¯·æ±‚å‰ç­‰å¾… {wait_time_before} ç§’ï¼ˆé¿å…é¢‘ç‡é™åˆ¶ï¼‰...")
                    time.sleep(wait_time_before)
                else:
                    wait_time_before = (attempt - 1) * 5  # é€’å¢å»¶è¿Ÿï¼š5ç§’ã€10ç§’
                    print(f"â³ è¯·æ±‚å‰ç­‰å¾… {wait_time_before} ç§’ï¼ˆé¿å…é¢‘ç‡é™åˆ¶ï¼‰...")
                    time.sleep(wait_time_before)
                
                print(f"ğŸ“¡ å°è¯•è·å–å®æ—¶æ–°é—»ï¼ˆå°è¯• {attempt}/{max_retries}ï¼Œè‚¡ç¥¨ä»£ç ï¼š{symbol_for_query}ï¼‰...")
                
                # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆåº”è¯¥æ˜¯6ä½æ•°å­—ï¼‰
                if not symbol_for_query.isdigit() or len(symbol_for_query) != 6:
                    print(f"âš ï¸ è‚¡ç¥¨ä»£ç æ ¼å¼é”™è¯¯: {symbol_for_query}ï¼Œåº”è¯¥æ˜¯6ä½æ•°å­—")
                    break
                
                # å°è¯•æ‹¦æˆª HTTP å“åº”
                capture_context, request_info_captured = self._capture_akshare_response(symbol_for_query)
                
                # è°ƒç”¨ AKShare APIï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ•è·å“åº”ï¼‰
                # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½ä¼šåœ¨ä¸Šä¸‹æ–‡ä¸­æ›´æ–° request_info_captured
                try:
                    with capture_context:
                        news_df = stock_news_em_safe(symbol=symbol_for_query, page_size=10, timeout=60)
                    # åˆå¹¶æ•è·çš„è¯·æ±‚ä¿¡æ¯ï¼ˆrequest_info_captured æ˜¯å¯å˜å¯¹è±¡ï¼Œå·²åœ¨ä¸Šä¸‹æ–‡ä¸­æ›´æ–°ï¼‰
                    if request_info_captured:
                        request_info.update(request_info_captured)
                except Exception:
                    # å¼‚å¸¸å‘ç”Ÿæ—¶ï¼Œè¯·æ±‚ä¿¡æ¯å·²ç»åœ¨ä¸Šä¸‹æ–‡ä¸­è¢«æ•è·ï¼Œåˆå¹¶å®ƒ
                    if request_info_captured:
                        request_info.update(request_info_captured)
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚ç»Ÿä¸€å¤„ç†
                
                if news_df is not None and not news_df.empty:
                    news_df = news_df.head(5)
                    for _, row in news_df.iterrows():
                        realtime_results.append({
                            'title': str(row.get('æ–°é—»æ ‡é¢˜', '')),
                            'content': str(row.get('æ–°é—»å†…å®¹', '')),
                            'publish_time': str(row.get('å‘å¸ƒæ—¶é—´', '')),
                            'source': 'AKShare-ä¸œæ–¹è´¢å¯Œ',
                            'url': str(row.get('æ–°é—»é“¾æ¥', '')),
                            'symbol': normalized_symbol or symbol_for_query,
                            'query': query,
                            'search_time': search_time
                        })
                    
                    realtime_results = self._filter_allowed_news_records(realtime_results)
                    
                    # å¯¹å®æ—¶æ–°é—»è¿›è¡ŒåµŒå…¥å»é‡ï¼ˆé’ˆå¯¹æ ‡é¢˜ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼0.85ï¼‰
                    if realtime_results:
                        print(f"ğŸ” å¯¹ {len(realtime_results)} æ¡å®æ—¶æ–°é—»è¿›è¡ŒåµŒå…¥å»é‡...")
                        try:
                            realtime_results = deduplicate_news_by_embedding(
                                realtime_results,
                                similarity_threshold=0.85,
                                field_to_compare='title'
                            )
                        except Exception as e:
                            print(f"âš ï¸ åµŒå…¥å»é‡å¤±è´¥: {e}ï¼Œè·³è¿‡å»é‡æ­¥éª¤")
                    
                    print(f"âœ… æˆåŠŸè·å– {len(realtime_results)} æ¡å®æ—¶æ–°é—»ï¼ˆè‚¡ç¥¨ï¼š{normalized_symbol or symbol_for_query}ï¼‰")
                    
                    # 3. ä¿å­˜å®æ—¶æ–°é—»åˆ° data_flow/news.csvï¼ˆè¿½åŠ æ¨¡å¼ï¼Œå»é‡ï¼‰â€”â€”å¹¶å‘å®‰å…¨å†™å…¥
                    try:
                        self._purge_news_csv(csv_path)
                        with NEWS_FILE_LOCK:
                            os.makedirs('data_flow', exist_ok=True)
                            df_new = pd.DataFrame(realtime_results)
                            df_new = self._sanitize_news_dataframe(df_new)
                            df_new = self._filter_allowed_news_df(df_new)
                            if df_new is None or df_new.empty:
                                continue
                            if 'search_time' not in df_new.columns:
                                df_new['search_time'] = str(search_time)
                            else:
                                df_new['search_time'] = df_new['search_time'].astype(str)
                            dedupe_subset = ['symbol', 'title', 'search_time', 'query']
                            df_new = df_new.drop_duplicates(subset=dedupe_subset, keep='last')
                            
                            if os.path.exists(csv_path):
                                # å°è¯•å¤šç§ç¼–ç è¯»å–ç°æœ‰æ–‡ä»¶
                                old = None
                                for encoding in ['utf-8', 'utf-8-sig', 'gbk', 'gb18030', 'latin1']:
                                    try:
                                        old = pd.read_csv(csv_path, encoding=encoding)
                                        print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å– news.csv")
                                        break
                                    except Exception:
                                        continue
                                
                                if old is not None:
                                    old = self._sanitize_news_dataframe(old)
                                    old = self._filter_allowed_news_df(old)
                                    if 'search_time' not in old.columns:
                                        old['search_time'] = pd.NA
                                    mask = ~(
                                        (old['symbol'].astype(str) == str(normalized_symbol or symbol_for_query)) &
                                        (old['query'].astype(str) == str(query)) &
                                        (old['search_time'].astype(str) == str(search_time))
                                    )
                                    old = old[mask]
                                    # åˆå¹¶å¹¶å»é‡
                                    combined = pd.concat([old, df_new], axis=0, ignore_index=True)
                                    combined = self._sanitize_news_dataframe(combined)
                                    combined = combined.drop_duplicates(subset=dedupe_subset, keep='last')
                                    
                                    # å¯¹åˆå¹¶åçš„æ•°æ®æŒ‰ symbol åˆ†ç»„è¿›è¡ŒåµŒå…¥å»é‡ï¼ˆé’ˆå¯¹ç§‘åˆ›æ¿æ–°é—»ï¼‰
                                    try:
                                        if 'symbol' in combined.columns and 'title' in combined.columns:
                                            # æŒ‰ symbol åˆ†ç»„å»é‡
                                            deduplicated_groups = []
                                            for symbol_code, group in combined.groupby('symbol'):
                                                if symbol_code and str(symbol_code).startswith('SH688'):
                                                    print(f"ğŸ” å¯¹è‚¡ç¥¨ {symbol_code} çš„ {len(group)} æ¡æ–°é—»è¿›è¡ŒåµŒå…¥å»é‡...")
                                                    group_list = group.to_dict('records')
                                                    deduplicated_list = deduplicate_news_by_embedding(
                                                        group_list,
                                                        similarity_threshold=0.85,
                                                        field_to_compare='title'
                                                    )
                                                    deduplicated_groups.extend(deduplicated_list)
                                                else:
                                                    # éç§‘åˆ›æ¿æ–°é—»ä¸å»é‡
                                                    deduplicated_groups.extend(group.to_dict('records'))
                                            combined = pd.DataFrame(deduplicated_groups)
                                    except Exception as e:
                                        print(f"âš ï¸ åˆå¹¶åçš„åµŒå…¥å»é‡å¤±è´¥: {e}ï¼Œè·³è¿‡å»é‡æ­¥éª¤")
                                    
                                    combined.to_csv(csv_path, index=False, encoding='utf-8-sig')
                                    print(f"ğŸ’¾ å·²å°†æ–°é—»è¿½åŠ åˆ° {csv_path}ï¼ˆå»é‡åï¼‰")
                                else:
                                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œç›´æ¥è¦†ç›–
                                    df_new.to_csv(csv_path, index=False, encoding='utf-8-sig')
                                    print(f"âš ï¸ æ— æ³•è¯»å–æ—§æ–‡ä»¶ï¼Œå·²åˆ›å»ºæ–°æ–‡ä»¶ {csv_path}")
                            else:
                                df_new.to_csv(csv_path, index=False, encoding='utf-8-sig')
                                print(f"ğŸ’¾ å·²åˆ›å»ºæ–°é—»æ–‡ä»¶ {csv_path}")
                    except Exception as e:
                        print(f"âš ï¸ ä¿å­˜æ–°é—»åˆ°CSVå¤±è´¥: {e}")
                    
                    # è¿”å›ç»„åˆç»“æœï¼šå†å² + å®æ—¶
                    return json.dumps({
                        'success': True,
                        'source': 'akshare',
                        'historical_count': len(historical_news),
                        'realtime_count': len(realtime_results),
                        'total_count': len(historical_news) + len(realtime_results),
                        'historical_news': historical_news[:10],
                        'realtime_news': realtime_results,
                        'saved_to': csv_path
                    }, ensure_ascii=False)
                else:
                    print(f"âš ï¸ AKShare è¿”å›ç©ºæ•°æ®ï¼ˆå°è¯• {attempt}/{max_retries}ï¼Œè‚¡ç¥¨ä»£ç ï¼š{symbol_for_query}ï¼‰")
                    print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼š1) è¯¥è‚¡ç¥¨æš‚æ— æ–°é—» 2) APIè¿”å›ç©ºæ•°æ® 3) ç½‘ç»œé—®é¢˜")
                    
            except json.JSONDecodeError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare JSONè§£æå¤±è´¥ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šAPIè¿”å›äº†éJSONå†…å®¹ï¼ˆå¦‚HTMLé”™è¯¯é¡µé¢ï¼‰ï¼Œé€šå¸¸æ˜¯ç½‘ç»œé—®é¢˜æˆ–åçˆ¬è™«é™åˆ¶")
                if attempt < max_retries:
                    print(f"ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except UnicodeDecodeError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare ç¼–ç é”™è¯¯ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šè¿”å›å†…å®¹çš„ç¼–ç æ ¼å¼ä¸åŒ¹é…")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except AttributeError as e:
                error_msg = str(e)
                if "'NoneType' object has no attribute" in error_msg:
                    print(f"âš ï¸ AKShare è¿”å›Noneï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                    print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šAPIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›äº†Noneï¼Œé€šå¸¸æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé™åˆ¶")
                else:
                    print(f"âš ï¸ AKShare å±æ€§é”™è¯¯ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except ConnectionError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare è¿æ¥é”™è¯¯ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é˜²ç«å¢™è®¾ç½®")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except TimeoutError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare è¯·æ±‚è¶…æ—¶ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šAPIå“åº”æ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½æ˜¯ç½‘ç»œæ…¢æˆ–æœåŠ¡å™¨è´Ÿè½½é«˜")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except Exception as e:
                error_type = type(e).__name__
                error_msg = str(e)
                print(f"âš ï¸ AKShare å¤±è´¥ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: [{error_type}] {error_msg}")
                
                # ç‰¹æ®Šå¤„ç† JSON è§£æé”™è¯¯ï¼ˆå¯èƒ½è¢«åŒ…è£…åœ¨å…¶ä»–å¼‚å¸¸ä¸­ï¼‰
                if "Expecting value" in error_msg or "JSON" in error_msg.upper():
                    print(f"ğŸ’¡ æ£€æµ‹åˆ°JSONè§£æé—®é¢˜ï¼Œå¯èƒ½æ˜¯APIè¿”å›äº†ç©ºå“åº”æˆ–é”™è¯¯é¡µé¢")
                    print(f"ğŸ’¡ å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) éªŒè¯è‚¡ç¥¨ä»£ç  {symbol_for_query} æ˜¯å¦æœ‰æ•ˆ")
                    print(f"ğŸ’¡ å¯ä»¥å°è¯•ï¼šæ‰‹åŠ¨è®¿é—®ä¸œæ–¹è´¢å¯Œç½‘ç«™æŸ¥çœ‹è¯¥è‚¡ç¥¨æ˜¯å¦æœ‰æ–°é—»")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    error_type, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
                
                # åœ¨æœ€åä¸€æ¬¡å°è¯•æ—¶æ‰“å°å®Œæ•´å †æ ˆ
                if attempt == max_retries:
                    print(f"ğŸ“‹ å®Œæ•´é”™è¯¯ä¿¡æ¯:")
                    traceback.print_exc()
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œé¿å…é¢‘ç‡é™åˆ¶
            if attempt < max_retries:
                wait_time = attempt * 5  # é€’å¢å»¶è¿Ÿï¼š5ç§’ã€10ç§’
                print(f"â³ ç­‰å¾… {wait_time} ç§’åè¿›è¡Œç¬¬ {attempt + 1} æ¬¡å°è¯•...")
                import time
                time.sleep(wait_time)
            else:
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                print(f"âŒ AKShare æ‰€æœ‰ {max_retries} æ¬¡å°è¯•å‡å¤±è´¥ï¼Œå°†è¿”å›å†å²æ–°é—»")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›å†å²æ–°é—»ï¼ˆå¦‚æœæœ‰ï¼‰
        # æ›´æ–°æœ€æ–°æ—¶é—´æˆ³
        if realtime_results or historical_news:
            return json.dumps({
                'success': True,
                'source': 'akshare',
                'historical_count': len(historical_news),
                'realtime_count': len(realtime_results),
                'total_count': len(historical_news) + len(realtime_results),
                'historical_news': historical_news[:10],
                'realtime_news': realtime_results
            }, ensure_ascii=False)

        if historical_news:
            print(f"âš ï¸ AKShare é‡è¯•{max_retries}æ¬¡å‡å¤±è´¥ï¼Œè¿”å›å†å²æ–°é—»")
            return json.dumps({
                'success': True,
                'source': 'historical_only',
                'historical_count': len(historical_news),
                'realtime_count': 0,
                'total_count': len(historical_news),
                'historical_news': historical_news[:10],
                'realtime_news': [],
                'message': self._provider_downtime_message("AKShare")
            }, ensure_ascii=False)
        else:
            return json.dumps({
                'success': False, 
                'message': self._provider_downtime_message("AKShare")
            }, ensure_ascii=False)
    
    def sell_stock(self, symbol: str, amount: int) -> str:
        """
        å–å‡ºè‚¡ç¥¨ï¼ˆä½¿ç”¨å½“å‰å°æ—¶çº§ä»·æ ¼ï¼‰ã€‚
        
        æ­¤å‡½æ•°æ¨¡æ‹Ÿè‚¡ç¥¨å–å‡ºæ“ä½œï¼ŒåŒ…æ‹¬ï¼š
        1. è·å–å½“å‰æŒä»“å’Œæ“ä½œID
        2. è·å–å½“å‰å°æ—¶çš„è‚¡ç¥¨ä»·æ ¼ï¼ˆä¼˜å…ˆå°æ—¶çº§ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·ï¼‰
        3. éªŒè¯å–å‡ºæ¡ä»¶ï¼ˆæ˜¯å¦æŒæœ‰è¯¥è‚¡ç¥¨ï¼Œæ•°é‡æ˜¯å¦å……è¶³ï¼‰
        4. æ›´æ–°æŒä»“ï¼ˆå‡å°‘è‚¡ç¥¨æ•°é‡ï¼Œå¢åŠ ç°é‡‘ï¼‰
        5. è®°å½•äº¤æ˜“åˆ° position.jsonl æ–‡ä»¶
        
        Args:
            symbol (str): è‚¡ç¥¨ä»£ç ï¼Œå¦‚ "600519"
            amount (int): å–å‡ºæ•°é‡ï¼Œå¿…é¡»æ˜¯æ­£æ•´æ•°
        
        Returns:
            str: JSON å­—ç¬¦ä¸²ï¼ŒæˆåŠŸæ—¶è¿”å›æ–°æŒä»“ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
        """
        try:
            normalized_symbol = normalize_symbol(symbol)
            if not normalized_symbol:
                return json.dumps({"error": "æ— æ•ˆçš„è‚¡ç¥¨ä»£ç "})
            data_symbol = strip_exchange_prefix(normalized_symbol) or normalized_symbol
            
            # ä½¿ç”¨å®ä¾‹ä¸Šä¸‹æ–‡ï¼Œé¿å…ä»å…±äº« runtime_env.json è¯»å–
            today_date = self._get_context_value("TODAY_DATE")
            current_time = self._get_context_value("CURRENT_TIME")
            decision_time = current_time or f"{today_date} 00:00:00"
            decision_count_raw = self._get_context_value("DECISION_COUNT")
            decision_count = int(decision_count_raw) if decision_count_raw is not None else 0
            if not today_date:
                return json.dumps({"error": "æœªè®¾ç½® TODAY_DATE"})
            decision_time = normalize_decision_time(today_date, decision_time)
            
            # è·å–å½“å‰æŒä»“å’Œæ“ä½œID
            current_position, current_action_id, latest_record = get_current_position(today_date, self.signature)
            
            # --- T+1 è§„åˆ™æ£€æŸ¥ ---
            if normalized_symbol in current_position and isinstance(current_position[normalized_symbol], dict):
                purchase_date = current_position[normalized_symbol].get("purchase_date")
                if purchase_date == today_date:
                    return json.dumps({
                        "error": "T+1è§„åˆ™é™åˆ¶ï¼šä»Šæ—¥ä¹°å…¥çš„è‚¡ç¥¨ä¸èƒ½åœ¨å½“æ—¥å–å‡º",
                        "symbol": normalized_symbol,
                        "purchase_date": purchase_date
                    })

            # è·å–å½“å‰æ—¶åˆ»çš„è‚¡ç¥¨ä»·æ ¼ï¼š
            # ä¼˜å…ˆä½¿ç”¨å…±äº« snapshot çš„ä»·æ ¼ï¼›ç¼ºå¤±æ—¶å†å›é€€åˆ° DataManagerï¼ˆå°æ—¶çº§â†’æ—¥çº¿ï¼‰ï¼Œé¿å…å¤šæ¨¡å‹ä»·æ ¼ä¸ä¸€è‡´
            price_source = None
            try:
                this_symbol_price = None

                snapshot_price, snapshot_ts = self._get_prefetched_trade_price(normalized_symbol)
                if snapshot_price is not None:
                    this_symbol_price = snapshot_price
                    price_source = "prefetch_snapshot"
                    ts_text = snapshot_ts or current_time or today_date
                    print(f"ğŸ’¹ ä½¿ç”¨å…±äº«å¿«ç…§ä»·æ ¼: {normalized_symbol} = Â¥{this_symbol_price} ({ts_text})")

                # è‹¥ snapshot ç¼ºå¤±ï¼Œåˆ™èµ° DataManagerï¼ˆå°æ—¶çº§ï¼‰
                if this_symbol_price is None or pd.isna(this_symbol_price):
                    if self.dm and current_time:
                        hourly_data = self.dm.get_hourly_stock_data(
                            symbol=data_symbol,
                            end_date=current_time,
                            lookback_hours=1,
                        )
                        if hourly_data is not None and not hourly_data.empty:
                            this_symbol_price = float(hourly_data["close"].iloc[-1])
                            price_source = "dm_hourly"
                        print(f"ğŸ’¹ ä½¿ç”¨å°æ—¶çº§ä»·æ ¼: {normalized_symbol} = Â¥{this_symbol_price} ({current_time})")
                
                # å¦‚æœæ²¡æœ‰å°æ—¶çº§æ•°æ®ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·
                if this_symbol_price is None or pd.isna(this_symbol_price):
                    if not self.dm:
                        return json.dumps(
                            {
                                "error": f"æœªæ‰¾åˆ°è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®",
                                "symbol": normalized_symbol,
                                "date": today_date,
                                "detail": "snapshot ç¼ºå¤±ä¸” DataManager ä¸å¯ç”¨",
                            },
                            ensure_ascii=False,
                        )
                    stock_data = self.dm.get_stock_data(symbol=data_symbol, end_date=today_date, lookback_days=1)
                    if stock_data is None or stock_data.empty:
                        return json.dumps(
                            {
                                "error": f"æœªæ‰¾åˆ°è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®",
                                "symbol": normalized_symbol,
                                "date": today_date,
                            },
                            ensure_ascii=False,
                        )
                    this_symbol_price = (
                        float(stock_data["open"].iloc[-1])
                        if "open" in stock_data.columns
                        else float(stock_data["close"].iloc[-1])
                    )
                    price_source = "dm_daily_open"
                    print(f"ğŸ’¹ ä½¿ç”¨å¼€ç›˜ä»·: {normalized_symbol} = Â¥{this_symbol_price}")
                
                if pd.isna(this_symbol_price) or this_symbol_price <= 0:
                    return json.dumps(
                        {
                            "error": f"è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®æ— æ•ˆ",
                            "symbol": normalized_symbol,
                            "date": today_date,
                            "price": this_symbol_price,
                            "price_source": price_source,
                        },
                        ensure_ascii=False,
                    )
            except Exception as e:
                return json.dumps(
                    {
                        "error": f"è·å–è‚¡ç¥¨ä»·æ ¼å¤±è´¥: {str(e)}",
                        "symbol": normalized_symbol,
                        "date": today_date,
                        "price_source": price_source,
                    },
                    ensure_ascii=False,
                )
            
            limit_info: Optional[Dict[str, float]] = None
            prev_close = self._get_previous_close(normalized_symbol, today_date)
            limit_info = get_price_limits(normalized_symbol, prev_close)
            allowed, reason = self._passes_price_limit_liquidity("sell", this_symbol_price, limit_info)
            if not allowed:
                return json.dumps({
                    "error": reason,
                    "symbol": normalized_symbol,
                    "price": this_symbol_price,
                    "limit_info": limit_info
                }, ensure_ascii=False)
            
            # éªŒè¯å–å‡ºæ¡ä»¶
            if normalized_symbol not in current_position or not isinstance(current_position[normalized_symbol], dict):
                return json.dumps({"error": f"æœªæŒæœ‰è‚¡ç¥¨ {normalized_symbol}ï¼äº¤æ˜“ä¸è¢«å…è®¸ã€‚", "symbol": normalized_symbol, "date": today_date})
            
            current_shares = current_position.get(normalized_symbol, {}).get("shares", 0)
            if current_shares < amount:
                return json.dumps({
                    "error": "æŒè‚¡æ•°é‡ä¸è¶³ï¼äº¤æ˜“ä¸è¢«å…è®¸ã€‚",
                    "have": current_shares,
                    "want_to_sell": amount,
                    "symbol": normalized_symbol,
                    "date": today_date
                })
            
            # æ‰§è¡Œå–å‡ºæ“ä½œ
            new_position = copy.deepcopy(current_position)
            new_position[normalized_symbol]["shares"] = current_shares - amount
            
            # å¦‚æœè‚¡ç¥¨æ•°é‡ä¸º0ï¼Œåˆ™ä»æŒä»“ä¸­ç§»é™¤
            if new_position[normalized_symbol]["shares"] == 0:
                del new_position[normalized_symbol]

            revenue = this_symbol_price * amount
            
            # --- äº¤æ˜“æˆæœ¬è®¡ç®— ---
            commission_rate = self.trading_rules.get("commission_rate", 0.0003)
            min_commission = self.trading_rules.get("min_commission", 5.0)
            stamp_duty_rate = self.trading_rules.get("stamp_duty_rate", 0.0005)
            
            commission = max(revenue * commission_rate, min_commission)
            stamp_duty = revenue * stamp_duty_rate
            total_deduction = commission + stamp_duty
            
            net_revenue = revenue - total_deduction
            new_position["CASH"] = new_position.get("CASH", 0) + net_revenue
            new_position = normalize_positions(new_position)
            
            # è®°å½•äº¤æ˜“
            record: Dict[str, Any] = {
                "date": today_date,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "this_action": {"action": "sell", "symbol": normalized_symbol, "amount": amount},
                "positions": new_position
            }
            if latest_record and latest_record.get("decision_time") == decision_time:
                record["id"] = latest_record.get("id")
            else:
                record["id"] = current_action_id + 1
            upsert_position_record(self.signature, record)
            
            write_runtime_config_value("IF_TRADE", True)
            return json.dumps({
                "success": True,
                "action": "sell",
                "symbol": normalized_symbol,
                "amount": amount,
                "price": this_symbol_price,
                "revenue": revenue,
                "commission": commission,
                "stamp_duty": stamp_duty,
                "net_revenue": net_revenue,
                "price_limit": limit_info,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "new_position": new_position
            })
        
        except Exception as e:
            return json.dumps({"error": f"å–å‡ºè‚¡ç¥¨æ—¶å‡ºé”™: {str(e)}"})
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ– AI æ¨¡å‹å’Œå·¥å…·ï¼ˆçº¯æœ¬åœ°æ¨¡å¼ï¼‰"""
        print(f"ğŸš€ åˆå§‹åŒ–ä»£ç†: {self.signature}")
        print("ğŸ’» ä½¿ç”¨çº¯æœ¬åœ°æ¨¡å¼...")
        if self.dm:
            self.tools = [
                tool(self.search_stock_news),            # ğŸ”„ å·²æ•´åˆï¼šå†å²+å®æ—¶æ–°é—»ï¼Œè‡ªåŠ¨ä¿å­˜
                tool(self.get_technical_indicators),     # ğŸ”„ å·²æ•´åˆï¼šå†å²+å®æ—¶æŒ‡æ ‡ï¼Œè‡ªåŠ¨ä¿å­˜
                tool(self.get_hourly_stock_data),
                tool(self.get_current_stock_prices),
                tool(self.get_current_position_tool),
                tool(self.add_no_trade_record_tool),
                tool(self.buy_stock),
                tool(self.sell_stock)
            ]
            print(f"âœ… å·²åŠ è½½ {len(self.tools)} ä¸ªæœ¬åœ°å·¥å…·ï¼ˆæ‰€æœ‰å·¥å…·å·²æ•´åˆå†å²+å®æ—¶æ•°æ®ï¼‰")
        else:
            print(f"âŒ DataManager æœªåˆå§‹åŒ–,æœªåŠ è½½ä»»ä½•å·¥å…·")
            self.tools = []
        
        if self.basemodel.startswith("gemini"):
            print(f"ğŸ¤– Initializing Google Gemini model: {self.basemodel}")
            gemini_safety_settings = None
            if self.safety_settings:
                try:
                    from google.genai.types import HarmCategory, HarmBlockThreshold
                    gemini_safety_settings = {}
                    harm_category_map = {
                        "HARM_CATEGORY_HARASSMENT": HarmCategory.HARM_CATEGORY_HARASSMENT,
                        "HARM_CATEGORY_HATE_SPEECH": HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                        "HARM_CATEGORY_SEXUALLY_EXPLICIT": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                        "HARM_CATEGORY_DANGEROUS_CONTENT": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                    }
                    harm_threshold_map = {
                        "BLOCK_NONE": HarmBlockThreshold.BLOCK_NONE,
                        "BLOCK_LOW_AND_ABOVE": HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                        "BLOCK_MEDIUM_AND_ABOVE": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                        "BLOCK_ONLY_HIGH": HarmBlockThreshold.BLOCK_ONLY_HIGH,
                    }
                    for category, threshold in self.safety_settings.items():
                        if category in harm_category_map and threshold in harm_threshold_map:
                            # LangChain çš„ ChatGoogleGenerativeAI éœ€è¦æ•´æ•°æšä¸¾å€¼ï¼Œè€Œä¸æ˜¯æšä¸¾å¯¹è±¡
                            # å°†æšä¸¾å¯¹è±¡è½¬æ¢ä¸ºæ•´æ•°å€¼
                            category_enum = harm_category_map[category]
                            threshold_enum = harm_threshold_map[threshold]
                            # ä½¿ç”¨æšä¸¾å€¼ï¼ˆæ•´æ•°ï¼‰ä½œä¸ºé”®å’Œå€¼
                            gemini_safety_settings[int(category_enum)] = int(threshold_enum)
                except Exception as e:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•è§£æ safety_settingsï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®: {e}")
                    gemini_safety_settings = None
            # Process parameters for Gemini
            # Gemini 3 Pro uses thinking_level (high/low), not thinking_budget
            # Gemini 2.5 uses thinking_budget (token count)
            model_kwargs = {}
            if self.parameters:
                # Gemini 3 Pro: thinking_level
                if "thinking_level" in self.parameters:
                    model_kwargs["thinking_level"] = self.parameters["thinking_level"]
                # Gemini 2.5: thinking_budget (backward compatibility)
                elif "thinking_budget" in self.parameters:
                    model_kwargs["thinking_budget"] = self.parameters["thinking_budget"]
                # Gemini 3 Pro: include_thoughts
                if "include_thoughts" in self.parameters:
                    model_kwargs["include_thoughts"] = self.parameters["include_thoughts"]
                if "max_output_tokens" in self.parameters:
                    model_kwargs["max_output_tokens"] = self.parameters["max_output_tokens"]
                if "temperature" in self.parameters:
                    model_kwargs["temperature"] = self.parameters["temperature"]
            
            # Note: For Gemini 3 models, function calls must include thought_signature.
            # LangChain's ChatGoogleGenerativeAI should automatically handle this when using
            # standard chat history management. If you encounter "thought_signature" errors,
            # ensure langchain-google-genai >= 3.2.0 is installed and that the conversation
            # history is properly managed by LangGraph/LangChain (not manually reconstructed).
            self.model = ChatGoogleGenerativeAI(
                model=self.basemodel,
                google_api_key=self.google_api_key,
                safety_settings=gemini_safety_settings,
                max_retries=5,
                timeout=60,
                **model_kwargs
            )
            print(f"âœ… Google Gemini model initialized with API key from {'environment' if self.google_api_key == os.getenv('GEMINI_API_KEY') else 'config'}")
            # Check if this is Gemini 3 and warn about thought_signature requirements
            if "gemini-3" in self.basemodel.lower() or "3-pro" in self.basemodel.lower() or "3-flash" in self.basemodel.lower():
                print(f"â„¹ï¸  Using Gemini 3 model: thought_signature is required for function calls and should be automatically handled by LangChain.")
        elif self.basemodel.startswith("qwen"):
            print(f"ğŸ¤– Initializing Qwen model: {self.basemodel}")
            dashscope_url = self.openai_base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            # Process parameters for Qwen (enable_thinking, temperature, max_tokens, etc.)
            extra_body = {}
            model_kwargs = {}
            if self.parameters:
                if "enable_thinking" in self.parameters:
                    extra_body["enable_thinking"] = self.parameters["enable_thinking"]
                if "temperature" in self.parameters:
                    model_kwargs["temperature"] = self.parameters["temperature"]
                if "max_tokens" in self.parameters:
                    model_kwargs["max_tokens"] = self.parameters["max_tokens"]
            
            self.model = ChatOpenAI(
                model=self.basemodel,
                base_url=dashscope_url,
                api_key=self.openai_api_key,
                max_retries=5,
                timeout=180,
                extra_body=extra_body if extra_body else None,
                **model_kwargs
            )
            print(f"âœ… Qwen model initialized via {dashscope_url}" + (f" (thinking: {extra_body.get('enable_thinking')})" if extra_body.get('enable_thinking') else ""))
        elif "reasoner" in self.basemodel.lower():
            print(f"ğŸ¤– Initializing Reasoner model: {self.basemodel} (with extended timeout)")
            # Process parameters for Reasoner models
            extra_body = {}
            model_kwargs = {}
            if self.parameters:
                if "max_tokens" in self.parameters:
                    model_kwargs["max_tokens"] = self.parameters["max_tokens"]
                if "temperature" in self.parameters:
                    model_kwargs["temperature"] = self.parameters["temperature"]
            
            self.model = ChatOpenAI(
                model=self.basemodel,
                base_url=self.openai_base_url,
                api_key=self.openai_api_key,
                max_retries=5,
                timeout=1200,  # 20åˆ†é’Ÿè¶…æ—¶ï¼Œä¸ºæ¨ç†æ¨¡å‹é¢„ç•™å……è¶³æ—¶é—´
                extra_body=extra_body if extra_body else None,
                **model_kwargs
            )
            print(f"âœ… Reasoner model initialized with 1200s timeout (20 minutes, extended for reasoning)")
        else:
            print(f"ğŸ¤– Initializing OpenAI-compatible model: {self.basemodel}")
            # Process parameters for OpenAI-compatible models (Claude, GPT, etc.)
            # For Claude with /anthropic endpoint, thinking parameters go to extra_body
            # For GPT, reasoning parameters go to extra_body
            extra_body = {}
            model_kwargs = {}
            if self.parameters:
                # Handle Claude thinking parameters (for /anthropic endpoint)
                if "thinking" in self.parameters:
                    extra_body["thinking"] = self.parameters["thinking"]
                # Handle GPT reasoning parameters
                # GPT-5.2 / o-series ä½¿ç”¨ reasoning_effort (ä¸æ˜¯ reasoning)
                if "reasoning_effort" in self.parameters:
                    extra_body["reasoning_effort"] = self.parameters["reasoning_effort"]
                # Handle reasoning (æ—§æ ¼å¼ï¼Œå‘åå…¼å®¹)
                elif "reasoning" in self.parameters:
                    extra_body["reasoning"] = self.parameters["reasoning"]
                # Handle max_completion_tokens (GPT reasoning æ¨¡å‹ä½¿ç”¨è¿™ä¸ª)
                if "max_completion_tokens" in self.parameters:
                    extra_body["max_completion_tokens"] = self.parameters["max_completion_tokens"]
                # Handle max_output_tokens (æŸäº›æƒ…å†µä¸‹ä½¿ç”¨ï¼Œä½† reasoning æ¨¡å‹ä¼˜å…ˆç”¨ max_completion_tokens)
                elif "max_output_tokens" in self.parameters:
                    extra_body["max_output_tokens"] = self.parameters["max_output_tokens"]
                # Standard parameters
                if "temperature" in self.parameters:
                    model_kwargs["temperature"] = self.parameters["temperature"]
                if "max_tokens" in self.parameters:
                    model_kwargs["max_tokens"] = self.parameters["max_tokens"]
            
            # å¯¹äº Claude ä½¿ç”¨ /anthropic ç«¯ç‚¹çš„æƒ…å†µï¼ŒLangChain çš„ ChatOpenAI å¯èƒ½ä¸æ”¯æŒ
            # å¦‚æœ base_url åŒ…å« /anthropicï¼Œå°è¯•ä½¿ç”¨ OpenAI å…¼å®¹åè®®ï¼ˆ/v1ï¼‰
            # æˆ–è€…è®©ä»£ç†å¹³å°è‡ªåŠ¨å¤„ç†åè®®è½¬æ¢
            base_url = self.openai_base_url
            if base_url and "/anthropic" in base_url:
                # å¦‚æœä»£ç†å¹³å°æ”¯æŒï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ /v1 ç«¯ç‚¹ï¼ˆOpenAI å…¼å®¹åè®®ï¼‰
                # ä½† thinking å‚æ•°å¯èƒ½æ— æ³•é€šè¿‡ OpenAI å…¼å®¹åè®®ä¼ é€’
                # è¿™é‡Œä¿æŒåŸæ ·ï¼Œè®©ä»£ç†å¹³å°å¤„ç†
                print(f"âš ï¸  æ³¨æ„: ä½¿ç”¨ /anthropic ç«¯ç‚¹ï¼Œå¦‚æœé‡åˆ° 404 é”™è¯¯ï¼Œè¯·æ£€æŸ¥ä»£ç†å¹³å°æ˜¯å¦æ”¯æŒæ­¤ç«¯ç‚¹")
            
            # æ£€æµ‹æ¨ç†å‚æ•°ï¼Œå¦‚æœæœ‰æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
            has_reasoning = "reasoning_effort" in extra_body or "reasoning" in extra_body or "max_completion_tokens" in extra_body
            # å¯¹äºæ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨1200ç§’ï¼ˆ20åˆ†é’Ÿï¼‰è¶…æ—¶ï¼Œç‰¹åˆ«æ˜¯å¯¹äºhigh reasoning_effortå’Œå¤§max_completion_tokensçš„æƒ…å†µ
            timeout_value = 1200 if has_reasoning else 720
            max_retries_value = 5 if has_reasoning else 3
            if has_reasoning:
                print(f"ğŸ¤– Detected reasoning parameters, using extended timeout: {timeout_value}s (20 minutes)")
            
            self.model = ChatOpenAI(
                model=self.basemodel,
                base_url=base_url,
                api_key=self.openai_api_key,
                max_retries=max_retries_value,
                timeout=timeout_value,
                extra_body=extra_body if extra_body else None,
                **model_kwargs
            )
            print(f"âœ… OpenAI-compatible model initialized" + (f" (parameters: {list(extra_body.keys())})" if extra_body else ""))
        
        print(f"âœ… Agent {self.signature} initialization completed")
    def _setup_logging(self, today_date: str, decision_time: str) -> str:
        """Set up (and reset) log file path for a specific decision time"""
        log_path = os.path.join(self.base_log_path, self.signature, 'log', today_date)
        os.makedirs(log_path, exist_ok=True)
        sanitized_time = decision_time.replace(":", "-").replace(" ", "_")
        log_file = os.path.join(log_path, f"{sanitized_time}.jsonl")
        with open(log_file, "w", encoding="utf-8") as f:
            f.write("")
        return log_file
    
    def _log_message(
        self,
        log_file: str,
        new_messages: List[Dict[str, str]],
        decision_time: Optional[str] = None,
        decision_count: Optional[int] = None
    ) -> None:
        """Log messages to log file"""
        sanitized_messages = self._sanitize_messages(new_messages)
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "signature": self.signature,
            "decision_time": decision_time,
            "decision_count": decision_count,
            "new_messages": sanitized_messages
        }
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

    def _log_snapshot_reference(
        self,
        log_file: str,
        decision_time: str,
        decision_count: int,
    ) -> None:
        if not self._current_snapshot_info:
            return
        info = self._current_snapshot_info
        snapshot_id = info.get("snapshot_id") or "unknown"
        snapshot_path = info.get("snapshot_path") or "local-memory"
        mode = info.get("mode") or "local"
        created_flag = info.get("snapshot_created")
        content = (
            f"Shared snapshot [{mode}] id={snapshot_id}, path={snapshot_path}, "
            f"created_now={created_flag}"
        )
        self._log_message(
            log_file,
            [{"role": "system", "content": content}],
            decision_time=decision_time,
            decision_count=decision_count,
        )
    
    def _sanitize_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Create a lightweight copy of messages for logging (short summary / truncation)."""
        sanitized: List[Dict[str, str]] = []
        for msg in messages:
            content = msg.get("content")
            if isinstance(content, str):
                sanitized_content = self._summarize_content(content)
                sanitized.append({**msg, "content": sanitized_content})
            else:
                sanitized.append(msg.copy())
        return sanitized
    
    def _summarize_content(self, content: str) -> str:
        """Summarize long or JSON-heavy content for logging."""
        if not content:
            return ""
        MAX_LEN = 500
        stripped = content.strip()
        if len(stripped) <= MAX_LEN and stripped.count("\n") <= 4:
            return stripped
        
        lines = [line for line in stripped.splitlines() if line.strip()]
        if len(lines) == 1:
            return self._summarize_line(lines[0])
        
        summaries = []
        for line in lines[:3]:
            summaries.append(self._summarize_line(line))
        if len(lines) > 3:
            summaries.append(f"... ({len(lines)} entries)")
        return " | ".join(summaries)
    
    def _summarize_line(self, line: str) -> str:
        """Summarize single line (try JSON first, fallback to truncated text)."""
        try:
            data = json.loads(line)
        except Exception:
            return self._truncate_text(line)
        
        if isinstance(data, dict):
            if "error" in data:
                return f"error: {self._truncate_text(str(data.get('error')))}"
            if data.get("success") is False:
                return f"failed: {self._truncate_text(str(data.get('message') or data))}"
            parts: List[str] = []
            if "success" in data:
                parts.append(f"success={data['success']}")
            for key in ("historical_count", "realtime_count", "total_count"):
                if key in data:
                    parts.append(f"{key}={data[key]}")
            if "message" in data:
                parts.append(self._truncate_text(str(data["message"])))
            if not parts:
                parts.append(self._truncate_text(json.dumps(data, ensure_ascii=False)))
            return ", ".join(parts)
        if isinstance(data, list):
            return f"list[{len(data)}]"
        return self._truncate_text(str(data))
    
    def _truncate_text(self, text: str, max_len: int = 180) -> str:
        text = text.strip()
        return text if len(text) <= max_len else text[:max_len] + "...(truncated)"
    
    def _content_to_text(self, content: Any) -> str:
        """Normalize message content (which can be str/list/dict) into plain text."""
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            parts: List[str] = []
            for item in content:
                if isinstance(item, dict):
                    if "text" in item and item["text"]:
                        parts.append(str(item["text"]))
                    elif "json" in item:
                        try:
                            parts.append(json.dumps(item["json"], ensure_ascii=False))
                        except Exception:
                            parts.append(str(item["json"]))
                    elif "data_pipeline" in item:
                        parts.append(str(item["data_pipeline"]))
                    else:
                        parts.append(str(item))
                else:
                    parts.append(str(item))
            return "\n".join(part for part in parts if part)
        if isinstance(content, dict):
            if "text" in content and content["text"]:
                return str(content["text"])
            if "json" in content:
                try:
                    return json.dumps(content["json"], ensure_ascii=False)
                except Exception:
                    return str(content["json"])
        return str(content)
    
    def _combine_tool_outputs(self, tool_messages: List[Any]) -> str:
        """Join tool outputs into a single string while handling different payload shapes."""
        outputs: List[str] = []
        for msg in tool_messages:
            if isinstance(msg, dict):
                content = msg.get("content")
            else:
                content = getattr(msg, "content", None)
            text = self._content_to_text(content)
            if text:
                outputs.append(text)
        return "\n".join(outputs)
    
    def _capture_akshare_response(self, symbol: str):
        """å°è¯•æ‹¦æˆª AKShare API çš„ HTTP è¯·æ±‚å’Œå“åº”ã€‚
        
        ä½¿ç”¨ requests çš„ monkey patching æ¥æ•è·å“åº”å†…å®¹ï¼ŒåŒ…æ‹¬åŸå§‹å­—èŠ‚å’Œå¤šç§è§£ç æ–¹å¼ã€‚
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            tuple: (capture_context, request_info_dict) - ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œè¯·æ±‚ä¿¡æ¯å­—å…¸ï¼ˆå¯å˜å¯¹è±¡ï¼‰
        """
        request_info = {
            'symbol': symbol,
            'url': None,
            'method': None,
            'status_code': None,
            'response_text': None,
            'response_content_raw': None,  # åŸå§‹å­—èŠ‚å†…å®¹
            'response_content_length': None,  # åŸå§‹å†…å®¹é•¿åº¦
            'response_content_decoded': None,  # æ‰‹åŠ¨è§£ç åçš„å†…å®¹
            'response_headers': None,
            'request_headers': None
        }
        
        try:
            import requests
            import gzip
            
            # å­˜å‚¨åŸå§‹æ–¹æ³•
            original_send = requests.Session.send
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨
            class ResponseCapture:
                def __init__(self, req_info):
                    self.request_info = req_info
                    self.original_send = original_send
                
                def __enter__(self):
                    # Monkey patch send æ–¹æ³•
                    def patched_send(self_session, request, **kwargs):
                        # è®°å½•è¯·æ±‚ä¿¡æ¯
                        req_info = self.request_info
                        req_info['url'] = request.url
                        req_info['method'] = request.method
                        
                        # ä¿®æ”¹è¯·æ±‚å¤´ï¼Œä½¿ç”¨çœŸå®çš„æµè§ˆå™¨ User-Agent å’Œå…¶ä»– headers
                        # é¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
                        if request.headers is None:
                            request.headers = {}
                        
                        # ä½¿ç”¨çœŸå®çš„ Chrome æµè§ˆå™¨ User-Agent
                        browser_user_agent = (
                            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) "
                            "Chrome/120.0.0.0 Safari/537.36"
                        )
                        
                        # ä¿®æ”¹æˆ–æ·»åŠ  headers
                        request.headers['User-Agent'] = browser_user_agent
                        request.headers['Accept'] = 'application/json, text/plain, */*'
                        request.headers['Accept-Language'] = 'zh-CN,zh;q=0.9,en;q=0.8'
                        request.headers['Accept-Encoding'] = 'gzip, deflate, br'
                        request.headers['Referer'] = 'http://quote.eastmoney.com/'
                        request.headers['Origin'] = 'http://quote.eastmoney.com'
                        request.headers['Connection'] = 'keep-alive'
                        
                        # è®°å½•ä¿®æ”¹åçš„è¯·æ±‚å¤´
                        if request.headers:
                            req_info['request_headers'] = dict(request.headers)
                        
                        if 'timeout' not in kwargs:
                            kwargs['timeout'] = 60
                        req_info['timeout'] = kwargs.get('timeout', 60)
                        
                        # å‘é€è¯·æ±‚
                        response = self.original_send(self_session, request, **kwargs)
                        
                        # è®°å½•å“åº”ä¿¡æ¯
                        try:
                            req_info['status_code'] = response.status_code
                            if response.headers:
                                req_info['response_headers'] = dict(response.headers)
                            
                            # ä¿å­˜åŸå§‹å­—èŠ‚å†…å®¹ï¼ˆåœ¨ requests è‡ªåŠ¨è§£å‹ä¹‹å‰ï¼‰
                            try:
                                # è·å–åŸå§‹å†…å®¹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è¢«è§£ç ï¼‰
                                if hasattr(response, 'content'):
                                    raw_content = response.content
                                    req_info['response_content_length'] = len(raw_content) if raw_content else 0
                                    
                                    # ä¿å­˜åŸå§‹å­—èŠ‚çš„å‰5000å­—èŠ‚ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                                    if raw_content:
                                        try:
                                            # å°è¯•è½¬æ¢ä¸ºå¯æ‰“å°çš„å­—ç¬¦ä¸²ï¼ˆåªä¿å­˜å‰1000å­—èŠ‚ä»¥é¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
                                            content_preview = raw_content[:1000]
                                            # å¦‚æœå†…å®¹æ˜¯äºŒè¿›åˆ¶ï¼Œå°è¯•ç¼–ç ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
                                            if isinstance(content_preview, bytes):
                                                req_info['response_content_raw'] = content_preview.hex()[:2000]  # é™åˆ¶é•¿åº¦
                                            else:
                                                req_info['response_content_raw'] = str(content_preview)[:2000]
                                        except Exception:
                                            req_info['response_content_raw'] = f"<æ— æ³•æ˜¾ç¤º: {len(raw_content)} å­—èŠ‚>"
                                    
                                    # å°è¯•æ‰‹åŠ¨è§£ç  gzip å†…å®¹ï¼ˆå¦‚æœå“åº”æ˜¯ gzip å‹ç¼©çš„ï¼‰
                                    content_encoding = response.headers.get('Content-Encoding', '').lower()
                                    if 'gzip' in content_encoding and raw_content:
                                        try:
                                            decoded_content = gzip.decompress(raw_content)
                                            decoded_text = decoded_content.decode('utf-8', errors='ignore')
                                            # åªä¿å­˜å‰2000ä¸ªå­—ç¬¦
                                            req_info['response_content_decoded'] = decoded_text[:2000]
                                        except Exception as gzip_err:
                                            req_info['gzip_decode_error'] = str(gzip_err)
                                else:
                                    req_info['response_content_length'] = 0
                            except Exception as content_err:
                                req_info['content_capture_error'] = str(content_err)
                            
                            # ä¿å­˜è§£ç åçš„æ–‡æœ¬å†…å®¹ï¼ˆrequests è‡ªåŠ¨å¤„ç†çš„ï¼‰
                            try:
                                text_content = response.text
                                req_info['response_text'] = text_content[:2000] if text_content else ""
                                req_info['response_text_length'] = len(text_content) if text_content else 0
                            except Exception as text_err:
                                req_info['response_text'] = ""
                                req_info['response_text_error'] = str(text_err)
                            
                            req_info['url'] = response.url or req_info.get('url')
                        except Exception as e:
                            req_info['capture_error'] = str(e)
                        
                        return response
                    
                    # æ›¿æ¢ send æ–¹æ³•
                    requests.Session.send = patched_send
                    return self
                
                def __exit__(self, exc_type, exc_val, exc_tb):
                    # æ¢å¤åŸå§‹æ–¹æ³•
                    requests.Session.send = self.original_send
                    return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸
            
            return ResponseCapture(request_info), request_info
            
        except Exception as e:
            # å¦‚æœæ‹¦æˆªå¤±è´¥ï¼Œè¿”å›ç©ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œé”™è¯¯ä¿¡æ¯
            request_info['capture_error'] = str(e)
            
            class EmptyCapture:
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    return False
            
            return EmptyCapture(), request_info

    def _save_akshare_error(self, debug_dir: str, symbol: str, attempt: int, max_retries: int,
                            error_type: str, error_msg: str, traceback_str: str, context: dict,
                            request_info: Optional[dict] = None):
        """ä¿å­˜ AKShare API é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼Œä¾¿äºåç»­åˆ†æã€‚
        
        Args:
            debug_dir: è°ƒè¯•æ–‡ä»¶å¤¹è·¯å¾„
            symbol: è‚¡ç¥¨ä»£ç 
            attempt: å½“å‰å°è¯•æ¬¡æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            error_type: é”™è¯¯ç±»å‹
            error_msg: é”™è¯¯æ¶ˆæ¯
            traceback_str: å †æ ˆè·Ÿè¸ª
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæŸ¥è¯¢å‚æ•°ç­‰ï¼‰
            request_info: è¯·æ±‚ä¿¡æ¯ï¼ˆURLã€å‚æ•°ã€å“åº”å†…å®¹ç­‰ï¼‰
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"akshare_error_{symbol}_{timestamp}_attempt{attempt}.json"
            filepath = os.path.join(debug_dir, filename)
            
            # è·å–ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
            import sys
            import platform
            try:
                import akshare as ak
                akshare_version = getattr(ak, '__version__', 'unknown')
            except Exception:
                akshare_version = 'unknown'
            
            error_data = {
                'timestamp': timestamp,
                'datetime': datetime.now().isoformat(),
                'symbol': symbol,
                'attempt': attempt,
                'max_retries': max_retries,
                'error_type': error_type,
                'error_message': error_msg,
                'context': context,
                'traceback': traceback_str,
                'environment': {
                    'python_version': sys.version,
                    'platform': platform.platform(),
                    'akshare_version': akshare_version
                },
                'request_info': request_info or {}
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(error_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {filepath}")
            print(f"ğŸ“‹ è¯·å°†æ–‡ä»¶å‘é€ç»™å¼€å‘è€…è¿›è¡Œåˆ†æ: {filename}")
        except Exception as save_err:
            print(f"âš ï¸ ä¿å­˜é”™è¯¯ä¿¡æ¯å¤±è´¥: {save_err}")
    
    def _extract_tool_errors(self, tool_response: str) -> List[str]:
        """ä»å·¥å…·è¿”å›ä¸­æå–çœŸæ­£çš„é”™è¯¯ä¿¡æ¯ï¼Œç”¨äºæœ€ç»ˆæ€»ç»“è¯´æ˜ã€‚
        æ³¨æ„ï¼šè­¦å‘Šã€ç©ºæ•°æ®æˆ–éƒ¨åˆ†æˆåŠŸä¸åº”è¢«è§†ä¸ºé”™è¯¯ã€‚
        """
        errors: List[str] = []
        if not tool_response:
            return errors
        
        # å…ˆå°è¯•è§£ææ•´ä¸ªå“åº”ä½œä¸º JSON
        try:
            data = json.loads(tool_response)
            if isinstance(data, dict):
                # åªæå–æ˜ç¡®çš„é”™è¯¯ï¼Œä¸åŒ…æ‹¬è­¦å‘Šæˆ–éƒ¨åˆ†æˆåŠŸ
                if data.get("error"):
                    error_msg = str(data["error"])
                    # æ’é™¤å¸¸è§çš„è­¦å‘Šä¿¡æ¯ï¼ˆå¦‚"è¿”å›å†å²æ•°æ®"ã€"å®¢æˆ·ç«¯ä¸å¯ç”¨"ç­‰ï¼‰
                    # è¿™äº›æ˜¯é™çº§å¤„ç†çš„æ­£å¸¸æƒ…å†µï¼Œä¸åº”è¯¥è§†ä¸ºé”™è¯¯
                    if "è¿”å›å†å²æ•°æ®" not in error_msg and "ä¸å¯ç”¨" not in error_msg:
                        # åªæœ‰åœ¨çœŸæ­£å¤±è´¥æ—¶æ‰è®°å½•ä¸ºé”™è¯¯
                        if "å¤±è´¥" in error_msg or "é”™è¯¯" in error_msg or "error" in error_msg.lower():
                            errors.append(error_msg)
                elif data.get("success") is False:
                    message = data.get("message", "")
                    if message and ("å¤±è´¥" in message or "é”™è¯¯" in message or "error" in message.lower()):
                        errors.append(str(message))
        except Exception:
            # å¦‚æœä¸æ˜¯ JSONï¼Œå°è¯•é€è¡Œè§£æ
            for raw_line in tool_response.splitlines():
                line = raw_line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    if isinstance(data, dict):
                        if data.get("error"):
                            error_msg = str(data["error"])
                            # æ’é™¤è­¦å‘Šä¿¡æ¯
                            if "è¿”å›å†å²æ•°æ®" not in error_msg and "è­¦å‘Š" not in error_msg:
                                errors.append(error_msg)
                        elif data.get("success") is False:
                            message = data.get("message", "")
                            if message and ("å¤±è´¥" in message or "é”™è¯¯" in message):
                                errors.append(str(message))
                except Exception:
                    continue
        
        return errors

    def _provider_downtime_message(self, provider_name: str) -> str:
        """Return a friendly message when upstream providers fail."""
        return f"{provider_name} æ•°æ®æºæš‚æ—¶ä¸å¯ç”¨ï¼Œç³»ç»Ÿå·²åˆ‡æ¢åˆ°å¤‡ä»½æ•°æ®ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    def _fix_mojibake(self, value: Any) -> Any:
        if isinstance(value, str):
            clean_value = value.replace("\ufeff", "").strip()
            for _ in range(3):
                if not any(ch in clean_value for ch in ("Ãƒ", "Ã‚", "Ã¯")) and not any(128 <= ord(ch) <= 255 for ch in clean_value):
                    break
                try:
                    decoded_value = clean_value.encode("latin1").decode("utf-8")
                except Exception:
                    break
                if decoded_value == clean_value:
                    break
                clean_value = decoded_value
            return clean_value
        return value

    def _sanitize_dataframe_text(self, df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:
        if df is None:
            return None
        sanitized = df.copy()
        raw_columns = [self._fix_mojibake(str(col)).strip() for col in sanitized.columns]
        unique_columns: List[str] = []
        column_indices: List[int] = []
        seen: set[str] = set()
        for idx, col_name in enumerate(raw_columns):
            base_name = col_name.split(".", 1)[0]
            if base_name not in seen:
                unique_columns.append(base_name)
                column_indices.append(idx)
                seen.add(base_name)
        sanitized = sanitized.iloc[:, column_indices]
        sanitized.columns = unique_columns
        for col in sanitized.columns:
            if sanitized[col].dtype == object:
                sanitized[col] = sanitized[col].apply(self._fix_mojibake)
        return sanitized

    def _normalize_symbol_value(self, value: Any) -> str:
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return ""
        raw = str(value).strip()
        if raw.endswith(".0"):
            raw = raw[:-2]
        if raw.isdigit() and len(raw) < 6:
            raw = raw.zfill(6)
        normalized = normalize_symbol(raw)
        return normalized or raw.upper()

    def _sanitize_news_dataframe(self, df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:
        sanitized = self._sanitize_dataframe_text(df)
        if sanitized is None:
            return None
        if "symbol" in sanitized.columns:
            sanitized["symbol"] = sanitized["symbol"].apply(self._normalize_symbol_value)
            sanitized = sanitized[sanitized["symbol"].astype(str).str.len() > 0]
        return sanitized
    
    def _get_previous_close(self, symbol: str, today_date: str) -> Optional[float]:
        if not self.dm:
            return None
        try:
            prev_date = calculate_previous_trading_date(today_date)
            target_time = f"{prev_date} 15:00:00"
            plain_symbol = strip_exchange_prefix(symbol) or symbol
            price = self.dm.get_price_at(plain_symbol, target_time)
            if price is None and plain_symbol != symbol:
                price = self.dm.get_price_at(symbol, target_time)
            return float(price) if price is not None else None
        except Exception as e:
            print(f"âš ï¸ è·å–å‰æ”¶å¤±è´¥: {e}")
            return None
    
    def _passes_price_limit_liquidity(
        self,
        action: str,
        price: Optional[float],
        limits: Optional[Dict[str, float]]
    ) -> Tuple[bool, Optional[str]]:
        if price is None or not limits:
            return True, None
        threshold = self.LIMIT_THRESHOLD_RATIO
        if action == "buy":
            upper = limits.get("upper")
            if upper is not None and price >= upper * threshold:
                if random.random() >= self.LIMIT_ORDER_SUCCESS_RATE:
                    return False, f"æ¥è¿‘æ¶¨åœä»· Â¥{upper:.2f}ï¼Œä¹°å•æˆäº¤æ¦‚ç‡ä»…10%ï¼Œæ­¤æ¬¡æ¨¡æ‹Ÿæœªæˆäº¤ã€‚"
        elif action == "sell":
            lower = limits.get("lower")
            if lower is not None and price <= lower / threshold:
                if random.random() >= self.LIMIT_ORDER_SUCCESS_RATE:
                    return False, f"æ¥è¿‘è·Œåœä»· Â¥{lower:.2f}ï¼Œå–å•æˆäº¤æ¦‚ç‡ä»…10%ï¼Œæ­¤æ¬¡æ¨¡æ‹Ÿæœªæˆäº¤ã€‚"
        return True, None
    
    def _compute_portfolio_metrics(
        self,
        positions: Dict[str, Any],
        today_date: str,
        current_time: Optional[str]
    ) -> Dict[str, Any]:
        metrics = {
            "cash": float(positions.get("CASH", 0.0) or 0.0),
            "position_value": 0.0,
            "unrealized_total": 0.0,
            "total_equity": 0.0,
            "holdings": [],
        }
        holdings_symbols: List[str] = []
        processed: List[Dict[str, Any]] = []
        is_open_slot = bool(current_time and current_time.endswith("10:30:00"))
        opening_price_cache: Dict[str, Optional[float]] = {}
        opening_reference_time = f"{today_date} 10:30:00" if is_open_slot else None
        
        if not positions:
            metrics["total_equity"] = metrics["cash"]
            return metrics
        
        for symbol, data in positions.items():
            if symbol == "CASH":
                continue
            if not isinstance(data, dict):
                continue
            shares = int(data.get("shares", 0) or 0)
            if shares <= 0:
                continue
            holdings_symbols.append(symbol)
            processed.append({
                "symbol": symbol,
                "shares": shares,
                "purchase_date": data.get("purchase_date"),
                "avg_price": data.get("avg_price")
            })
        
        price_lookup: Dict[str, Optional[float]] = {}
        target_time = current_time or f"{today_date} 15:00:00"
        if self.dm and holdings_symbols:
            try:
                price_lookup = self.dm.get_prices_at(holdings_symbols, target_time) or {}
            except Exception as e:
                print(f"âš ï¸ Failed to fetch current prices: {e}")
                price_lookup = {}
        
        for item in processed:
            symbol = item["symbol"]
            shares = item["shares"]
            purchase_date = item.get("purchase_date")
            avg_price = item.get("avg_price")
            current_price = None
            if price_lookup:
                current_price = price_lookup.get(symbol.upper()) or price_lookup.get(symbol)
            if current_price is None and self.dm:
                try:
                    current_price = self.dm.get_price_at(symbol, target_time)
                except Exception:
                    current_price = None

            if current_price is None and is_open_slot and self.dm and opening_reference_time:
                plain_symbol = strip_exchange_prefix(symbol) or symbol
                if plain_symbol not in opening_price_cache:
                    try:
                        df_open = self.dm.get_hourly_stock_data(
                            symbol=plain_symbol,
                            end_date=opening_reference_time,
                            lookback_hours=1
                        )
                        if df_open is not None and not df_open.empty:
                            latest_row = df_open.iloc[-1]
                            price_candidate = latest_row.get("open")
                            if price_candidate is None or pd.isna(price_candidate):
                                price_candidate = latest_row.get("close")
                            opening_price_cache[plain_symbol] = float(price_candidate) if price_candidate is not None else None
                        else:
                            opening_price_cache[plain_symbol] = None
                    except Exception as e:
                        print(f"âš ï¸ Failed to fetch opening price for {plain_symbol}: {e}")
                        opening_price_cache[plain_symbol] = None
                current_price = opening_price_cache.get(plain_symbol)
            
            if avg_price is None and self.dm and purchase_date:
                try:
                    approx_time = f"{purchase_date} 15:00:00"
                    avg_price = self.dm.get_price_at(symbol, approx_time)
                except Exception:
                    avg_price = None
            
            market_value = float(current_price or 0.0) * shares if current_price is not None else 0.0
            cost_basis = None
            if avg_price is not None:
                cost_basis = float(avg_price) * shares
            unrealized = None
            if cost_basis is not None and current_price is not None:
                unrealized = market_value - cost_basis
                metrics["unrealized_total"] += unrealized
            
            metrics["position_value"] += market_value
            metrics["holdings"].append({
                "symbol": symbol,
                "shares": shares,
                "avg_price": avg_price,
                "current_price": current_price,
                "market_value": market_value,
                "unrealized": unrealized
            })
        
        metrics["total_equity"] = metrics["cash"] + metrics["position_value"]
        return metrics

    def _parse_timestamp(self, value: Any) -> Optional[datetime]:
        if value is None:
            return None
        try:
            parsed = pd.to_datetime(value)
            if hasattr(parsed, "to_pydatetime"):
                parsed = parsed.to_pydatetime()
            if isinstance(parsed, datetime):
                return parsed.replace(tzinfo=None)
        except Exception:
            return None
        return None
    
    async def _ainvoke_with_retry(
        self,
        message: List[Dict[str, str]],
        recursion_limit: Optional[int] = None
    ) -> Any:
        """Agent invocation with retry"""
        limit = recursion_limit or self.max_steps or 20
        for attempt in range(1, self.max_retries + 1):
            try:
                return await self.agent.ainvoke(
                    {"messages": message}, 
                    {"recursion_limit": limit}
                )
            except Exception as e:
                # Handle Google API specific errors, especially thought_signature issues
                error_type = type(e).__name__
                error_msg = str(e)
                if "google" in str(type(e).__module__).lower():
                    print(f"âš ï¸ Google API error ({error_type}): {e}")
                    # Check if this is a thought_signature error
                    if "thought_signature" in error_msg.lower() or "thoughtsignature" in error_msg.lower():
                        print(f"ğŸ’¡ Hint: Gemini 3 requires thought_signature in function calls.")
                        print(f"ğŸ’¡ Ensure langchain-google-genai >= 3.2.0 is installed and conversation history is properly managed.")
                        if "gemini-3" not in self.basemodel.lower():
                            print(f"âš ï¸ Warning: You may be using Gemini 3 but basemodel doesn't contain 'gemini-3'")
                else:
                    print(f"âš ï¸ Error ({error_type}): {e}")
                if attempt == self.max_retries:
                    print(f"âŒ All {self.max_retries} attempts failed")
                    raise e
                
                wait_time = self.base_delay * attempt
                print(f"âš ï¸ Attempt {attempt} failed, retrying after {wait_time} seconds...")
                await asyncio.sleep(wait_time)
    
    async def run_trading_session(
        self,
        today_date: str,
        current_time: str,
        decision_count: int = 1
    ) -> None:
        """
        Run single trading session
        
        Args:
            today_date: Trading date
            current_time: Current simulation time
            decision_count: Which decision this is (1-3)
        """
        print(f"ğŸ“ˆ Starting trading session: {current_time} (Decision {decision_count}/3)")
        
        # Ensure config values are set (é˜²æ­¢å¤–éƒ¨æµç¨‹é—æ¼å¯¼è‡´å·¥å…·æŠ¥é”™)
        write_runtime_config_value("TODAY_DATE", today_date)
        write_runtime_config_value("CURRENT_TIME", current_time)
        write_runtime_config_value("DECISION_COUNT", decision_count)
        self.runtime_context["TODAY_DATE"] = today_date
        self.runtime_context["CURRENT_TIME"] = current_time
        self.runtime_context["DECISION_COUNT"] = decision_count

        # Set up logging
        log_file = self._setup_logging(today_date, current_time)
        
        # Update system prompt with decision count
        self.agent = create_react_agent(
            self.model,
            tools=self.tools,
            prompt=get_agent_system_prompt(
                today_date,
                self.signature,
                dm=self.dm,
                current_time=current_time,
                decision_count=decision_count
            ),
        )
        
        snapshot_result = None
        snapshot_bundle: Optional[Dict[str, Any]] = None
        observation_summary = ""
        try:
            if not self.prefetch_coordinator:
                raise RuntimeError("Shared prefetch coordinator is not available")

            # snapshot åº”è¯¥åœ¨ run_intraday_trading ä¸­å·²ç»é¢„ç”Ÿæˆï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
            symbols_signature = self._symbols_signature()
            
            def _build_snapshot() -> Dict[str, Any]:
                bundle = self._collect_prefetch_bundle(today_date, current_time, decision_count)
                # æ°¸è¿œç”± LLM è‡ªå·±ç”Ÿæˆ Observation Summaryï¼šå…±äº«å¿«ç…§é‡Œä¸ä¿å­˜ observation_summary
                bundle.pop("observation_summary", None)
                return bundle

            snapshot_result = self.prefetch_coordinator.ensure_snapshot(
                today_date=today_date,
                current_time=current_time,
                symbols_signature=symbols_signature,
                builder=_build_snapshot,
            )
            snapshot_bundle = snapshot_result.data
            observation_summary = self._apply_prefetch_bundle(snapshot_bundle)
        except Exception as e:
            # æ°¸è¿œä¸å…è®¸ fallbackï¼šå¿…é¡»ä½¿ç”¨ shared snapshot
            print(f"âŒ Shared prefetch å¤±è´¥ï¼ˆ{e}ï¼‰ï¼Œå·²ç»ˆæ­¢ï¼ˆä¸å…è®¸ fallback åˆ° per-agentic workflow prefetchï¼‰ã€‚")
            raise

        snapshot_id = (snapshot_bundle or {}).get("snapshot_id")
        snapshot_path = snapshot_result.path if snapshot_result else None
        snapshot_created = snapshot_result.created if snapshot_result else False
        self._current_snapshot_info = {
            "snapshot_id": snapshot_id,
            "snapshot_path": snapshot_path,
            "snapshot_created": snapshot_created,
            "mode": "shared" if snapshot_result else "local",
        }
        self._log_snapshot_reference(log_file, current_time, decision_count)

        # Build dynamic context message with full positions snapshot
        try:
            latest_positions, _, latest_record = get_current_position(today_date, self.signature)
        except Exception as e:
            print(f"âš ï¸ Failed to load latest positions: {e}")
            latest_positions = {}
            latest_record = None

        positions_json = json.dumps(latest_positions, ensure_ascii=False, indent=2)
        last_action = None
        if latest_record:
            last_action = latest_record.get("this_action")

        if decision_count == 1:
            stage = "opening (observe & prepare)"
        elif decision_count == 2:
            stage = "midday (deploy capital)"
        else:
            stage = "afternoon (adjust/lock profits)"

        metrics = self._compute_portfolio_metrics(latest_positions, today_date, current_time)
        if metrics["cash"] <= 0 and not metrics["holdings"]:
            print("âš ï¸ Cash balance is zero with no holdings. Consider enabling FORCE_REPLAY to reset positions.")
        holdings_lines: List[str] = []
        for holding in metrics.get("holdings", []):
            sym = holding["symbol"]
            shares = holding["shares"]
            avg_price = holding.get("avg_price")
            current_price = holding.get("current_price")
            market_value = holding.get("market_value", 0.0)
            unrealized = holding.get("unrealized")
            line = f"  â€¢ {sym}: {shares} shares"
            if current_price is not None:
                line += f", Px Â¥{current_price:,.2f}"
            if avg_price is not None:
                line += f", Avg Â¥{avg_price:,.2f}"
            line += f", MV Â¥{market_value:,.2f}"
            if unrealized is not None:
                line += f", PnL Â¥{unrealized:,.2f}"
            holdings_lines.append(line)
        if not holdings_lines:
            holdings_lines.append("  â€¢ (no equity positions)")

        # æ°¸è¿œç»™ LLM å…¨é‡æ•°æ®ï¼Œè®©å®ƒè‡ªå·±åšè§‚å¯Ÿæ€»ç»“ï¼ˆæ¯ä¸ªæ¨¡å‹è¾“å‡ºä¼šä¸åŒï¼‰
        snapshot_for_llm = copy.deepcopy(snapshot_bundle or {})
        snapshot_for_llm.pop("observation_summary", None)  # é¿å…æŠŠç¨‹åºç”Ÿæˆçš„æ‘˜è¦å¡ç»™æ¨¡å‹
        snapshot_json_compact = json.dumps(snapshot_for_llm, ensure_ascii=False, separators=(",", ":"))
        required_symbols = ", ".join(self.stock_symbols)
        
        # ç»Ÿä¸€ä½¿ç”¨æ›´ä¸¥æ ¼çš„ Observation Summary æ ¼å¼è¦æ±‚ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼‰
        observation_block = (
            "ã€ä»»åŠ¡æ­¥éª¤1ã€‘è¯·å…ˆåˆ†æä»¥ä¸‹å¸‚åœºæ•°æ®å¹¶ç”Ÿæˆ Observation Summaryï¼š\n\n"
            "æ•°æ®è¯´æ˜ï¼ˆå·²é¢„å¤„ç†å¥½ï¼‰ï¼š\n"
            "  - ä»¥ä¸‹JSONåŒ…å« news/prices/indicators ç­‰å¸‚åœºæ•°æ®\n"
            "  - æ–°é—»æ•°æ®ï¼šå½“å¤© + è¿‡å»2å¤©ï¼ˆå…±3å¤©ï¼‰ï¼Œåªä½¿ç”¨ titleï¼›å·²è¿‡æ»¤åˆ° <= current_time\n"
            "  - ä»·æ ¼/æŠ€æœ¯æŒ‡æ ‡ï¼šå…±3å¤©ï¼Œå¯¹é½åˆ°å½“å‰å†³ç­–æ—¶åˆ»ï¼›åªå…³æ³¨ RSI_3 ä¸ MACD_12_26_9ï¼ˆä¸ä½¿ç”¨ OBVï¼‰\n"
            "\n"
            "è¯·ç«‹å³æ‰§è¡Œï¼šç”Ÿæˆã€Observation Summaryã€‘ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
            "```\n"
            "Observation Summary:\n"
            "\n"
            f"1. {self.stock_symbols[0] if self.stock_symbols else 'SH688008'}\n"
            "   - æŠ€æœ¯æŒ‡æ ‡: RSI_3=XX, MACD_12_26_9=XX (ç®€è¦åˆ†æ)\n"
            "   - æ–°é—»: [æ€»ç»“æ–°é—»æ ‡é¢˜çš„å½±å“ï¼Œè‹¥æ— æ–°é—»å†™\"æ— ç›¸å…³æ–°é—»\"]\n"
            "\n"
            f"2. {self.stock_symbols[1] if len(self.stock_symbols) > 1 else 'SH688111'}\n"
            "   - æŠ€æœ¯æŒ‡æ ‡: RSI_3=XX, MACD_12_26_9=XX (ç®€è¦åˆ†æ)\n"
            "   - æ–°é—»: [æ€»ç»“æ–°é—»æ ‡é¢˜çš„å½±å“ï¼Œè‹¥æ— æ–°é—»å†™\"æ— ç›¸å…³æ–°é—»\"]\n"
            "\n"
            "... (å¿…é¡»è¦†ç›–æ‰€æœ‰è‚¡ç¥¨)\n"
            "```\n"
            "\n"
            f"ã€å¿…é¡»è¦†ç›–ã€‘ä»¥ä¸‹å…¨éƒ¨è‚¡ç¥¨ï¼ˆæŒ‰é¡ºåºï¼Œä¸å¯é—æ¼ï¼‰ï¼š{required_symbols}\n"
            "  âœ“ æ¯åªè‚¡ç¥¨å¿…é¡»åŒ…å«ï¼šæŠ€æœ¯æŒ‡æ ‡åˆ†æï¼ˆRSI_3ã€MACD_12_26_9çš„å…·ä½“æ•°å€¼å’Œç®€è¦åˆ¤æ–­ï¼‰+ æ–°é—»å½±å“åˆ†æ\n"
            "  âœ“ è‹¥æŸåªè‚¡ç¥¨åœ¨ JSON ä¸­ç¼ºå°‘æ•°æ®ï¼šå¿…é¡»è¯´æ˜ç¼ºå¤±çš„æ˜¯ prices / indicators / news ä¸­çš„å“ªä¸€å—\n"
            "  âœ“ å¿…é¡»æŒ‰ç…§ä¸Šè¿°æ ¼å¼ï¼Œé€åªè‚¡ç¥¨åˆ—å‡ºï¼Œä¸èƒ½åˆå¹¶æˆ–çœç•¥\n"
            "\n"
            "ã€ä»»åŠ¡æ­¥éª¤2ã€‘å®Œæˆ Observation Summary åï¼ŒåŸºäºåˆ†æç»“æœè¿›è¡Œäº¤æ˜“å†³ç­–ã€‚\n"
            "\n"
            "å¸‚åœºæ•°æ®ï¼ˆJSONæ ¼å¼ï¼Œä¾›åˆ†æä½¿ç”¨ï¼Œè¯·å‹¿åœ¨è¾“å‡ºä¸­å®Œæ•´å¤è¿°ï¼‰ï¼š\n"
            f"{snapshot_json_compact}\n"
        )
        
        context_message = (
            f"è¯·æ‰§è¡Œä»¥ä¸‹äº¤æ˜“å†³ç­–ä»»åŠ¡ï¼ˆ{today_date} {current_time}ï¼‰ï¼š\n\n"
            f"ã€å½“å‰çŠ¶æ€ã€‘\n"
            f"- Decision index: {decision_count}/3 â€” stage: {stage}\n"
            f"- Latest recorded action: {json.dumps(last_action, ensure_ascii=False) if last_action else 'N/A'}\n"
            f"- Cash: Â¥{metrics['cash']:,.2f}\n"
            f"- Position value: Â¥{metrics['position_value']:,.2f}\n"
            f"- Total equity: Â¥{metrics['total_equity']:,.2f}\n"
            f"- Unrealized PnL: Â¥{metrics['unrealized_total']:,.2f}\n"
            f"- Holdings detail:\n{chr(10).join(holdings_lines)}\n\n"
            f"{observation_block}"
            f"- Full positions JSON (from position file, do not trim):\n{positions_json}\n\n"
            "ã€æ‰§è¡Œè¦æ±‚ã€‘\n"
            "è¯·ä¸¥æ ¼æŒ‰ç…§observation_blockä¸­çš„è¦æ±‚æ‰§è¡Œä»»åŠ¡ï¼Œä¸è¦å¤è¿°è¾“å…¥å†…å®¹ã€‚å¿…é¡»ï¼š\n"
            "1. ç”ŸæˆObservation Summaryï¼ˆè¦†ç›–æ‰€æœ‰è‚¡ç¥¨ï¼‰\n"
            "2. åŸºäºåˆ†æè¿›è¡Œäº¤æ˜“å†³ç­–\n"
            "3. ä½¿ç”¨ <FINISH_SIGNAL> ç»“æŸ"
        )

        user_query = [{"role": "user", "content": context_message}]
        message = user_query.copy()
        
        # Log initial message
        self._log_message(log_file, user_query, decision_time=current_time, decision_count=decision_count)
        
        final_agent_summary: Optional[str] = None
        collected_tool_errors: List[str] = []
        
        try:
            response = await self._ainvoke_with_retry(message, recursion_limit=self.max_steps)
            
            # Extract agentic workflow response
            agent_response = extract_llm_conversation(response, "final")
            if agent_response and agent_response.strip():
                final_agent_summary = agent_response
                if STOP_SIGNAL in agent_response:
                    print("âœ… Received stop signal, trading session ended")
                else:
                    print("â„¹ï¸ Agent completed without explicit stop signal")
                    collected_tool_errors.append("Missing stop signal in agentic workflow response")
                print(agent_response)
                self._log_message(
                    log_file,
                    [{"role": "assistant", "content": agent_response}],
                    decision_time=current_time,
                    decision_count=decision_count,
                )
            else:
                print("âš ï¸ Agent produced no final response")
                final_agent_summary = "NO_TRADE: æ¨¡å‹æœªæä¾›æœ‰æ•ˆè¾“å‡ºã€‚"
                collected_tool_errors.append("Agent produced no final response")
                self._log_message(
                    log_file,
                    [{"role": "system", "content": "Agent produced no final response."}],
                    decision_time=current_time,
                    decision_count=decision_count,
                )

            # Extract and summarize tool outputs for logging/error tracking
            tool_msgs = extract_llm_tool_messages(response)
            tool_response = self._combine_tool_outputs(tool_msgs)
            tool_summary = "(no tool output)"
            if tool_response:
                collected_tool_errors.extend(self._extract_tool_errors(tool_response))
                tool_summary = self._summarize_content(tool_response) or self._truncate_text(tool_response, 600)
            self._log_message(
                log_file,
                [{"role": "system", "content": f"Tool summary: {tool_summary}"}],
                decision_time=current_time,
                decision_count=decision_count,
            )
        except Exception as e:
            print(f"âŒ Trading session error: {str(e)}")
            print(f"Error details: {e}")
            import traceback

            traceback.print_exc()
            # å³ä½¿å‡ºé”™ä¹Ÿè¦è®°å½•é”™è¯¯ä¿¡æ¯åˆ°æ—¥å¿—
            try:
                error_msg = f"Trading session failed: {str(e)}"
                self._log_message(
                    log_file,
                    [{"role": "assistant", "content": error_msg}],
                    decision_time=current_time,
                    decision_count=decision_count,
                )
            except Exception:
                pass  # å¦‚æœæ—¥å¿—è®°å½•ä¹Ÿå¤±è´¥ï¼Œè‡³å°‘ä¸å½±å“ä¸»æµç¨‹
            raise
        finally:
            # æ— è®ºæˆåŠŸä¸å¦éƒ½å°è¯•å¤„ç†äº¤æ˜“ç»“æœå¹¶è®°å½•çŠ¶æ€
            try:
                await self._handle_trading_result(
                    today_date,
                    current_time,
                    decision_count,
                    log_file,
                    final_agent_summary,
                    collected_tool_errors,
                )
            except Exception as e:
                print(f"âš ï¸ Error handling trading result: {e}")
                try:
                    error_msg = f"Error handling trading result: {str(e)}"
                    self._log_message(
                        log_file,
                        [{"role": "system", "content": error_msg}],
                        decision_time=current_time,
                        decision_count=decision_count,
                    )
                except Exception:
                    pass
            try:
                from utils.backup_utils import run_backup_snapshot, save_pnl_snapshot
                # Windows æ–‡ä»¶åæ¸…ç†ï¼šå°†æ—¶é—´ä¸­çš„å†’å·å’Œç©ºæ ¼æ›¿æ¢ä¸ºè¿å­—ç¬¦å’Œä¸‹åˆ’çº¿
                safe_time = current_time.replace(":", "-").replace(" ", "_")
                reason = f"decision_{decision_count}_{today_date}_{safe_time}"
                ok = run_backup_snapshot(reason=reason)
                if ok:
                    try:
                        print(f"[OK] Backup completed for decision {decision_count} on {today_date}")
                    except UnicodeEncodeError:
                        print(f"Backup completed for decision {decision_count} on {today_date}")
                else:
                    try:
                        print(f"[WARNING] Backup failed for decision {decision_count} on {today_date}. Check logs for details.")
                    except UnicodeEncodeError:
                        print(f"WARNING: Backup failed for decision {decision_count} on {today_date}. Check logs for details.")
                save_pnl_snapshot(reason=reason)  # é¢å¤–ä¿å­˜æ”¶ç›Šæ›²çº¿
            except Exception as e:
                try:
                    print(f"[WARNING] Error during backup: {e}")
                    import traceback
                    print(f"[WARNING] Backup traceback: {traceback.format_exc()}")
                except UnicodeEncodeError:
                    print(f"WARNING: Error during backup: {e}")
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œå…³é—­ TinySoft å®¢æˆ·ç«¯ï¼Œä»¥ä¿æŒä¼šè¯å¤ç”¨
            # ä¼šè¯ä¼šåœ¨ run_date_range ç»“æŸæ—¶ç»Ÿä¸€å…³é—­ï¼Œé¿å…é¢‘ç¹ç™»å½•
            pass
    async def _handle_trading_result(
        self,
        today_date: str,
        decision_time: str,
        decision_count: int,
        log_file: str,
        final_agent_summary: Optional[str],
        collected_tool_errors: Optional[List[str]]
    ) -> None:
        """Handle trading results"""
        if_trade = get_runtime_config_value("IF_TRADE")
        if if_trade:
            write_runtime_config_value("IF_TRADE", False)
            print("âœ… Trading completed")
            self._log_message(
                log_file,
                [{"role": "system", "content": "Trade executed during this session."}],
                decision_time=decision_time,
                decision_count=decision_count
            )
        else:
            print("ğŸ“Š No trading, maintaining positions")
            try:
                add_no_trade_record(today_date, decision_time, decision_count, self.signature)
            except NameError as e:
                print(f"âŒ NameError: {e}")
                raise
            write_runtime_config_value("IF_TRADE", False)
            needs_followup = not final_agent_summary or not str(final_agent_summary).strip()
            if needs_followup:
                reason_text = None
                if collected_tool_errors:
                    filtered = [msg for msg in collected_tool_errors if msg]
                    if filtered:
                        reason_text = "; ".join(dict.fromkeys(filtered))  # å»é‡ä¿æŒé¡ºåº
                if reason_text is None:
                    reason_text = "å·¥å…·æ‰§è¡ŒæœªæˆåŠŸå®Œæˆ"
                clarification = (
                    "æœ¬è½®äº¤æ˜“æœ€ç»ˆæœªæˆäº¤ï¼Œå·²è®°å½•æ— äº¤æ˜“ã€‚"
                    f"åŸå› ï¼š{reason_text}ã€‚"
                )
                self._log_message(
                    log_file,
                    [{"role": "assistant", "content": clarification}],
                    decision_time=decision_time,
                    decision_count=decision_count
                )
    
    def _get_trading_hours(self, today_date: str) -> List[str]:
        return [
            f"{today_date} 10:30:00",  # é¦–æ¬¡å°æ—¶çº¿å¯ç”¨
            f"{today_date} 11:30:00",  # åˆé—´å‰ï¼ˆä¸Šåˆæœ€åä¸€æ ¹ï¼‰
            f"{today_date} 14:00:00",  # åˆåæ ¸å¿ƒæ—¶æ®µ
        ]

    def _get_market_closures_2026(self) -> set:
        """
        è¿”å›2026å¹´ä¸Šäº¤æ‰€ä¼‘å¸‚æ—¥æœŸé›†åˆï¼ˆç¡¬ç¼–ç ï¼‰
        åŒ…æ‹¬èŠ‚å‡æ—¥å’Œå‘¨æœ«ä¼‘å¸‚æ—¥
        """
        return {
            "2026-01-01", "2026-01-02", "2026-01-03", "2026-01-04",
            "2026-02-14", "2026-02-15", "2026-02-16", "2026-02-17", "2026-02-18", "2026-02-19", "2026-02-20", "2026-02-21", "2026-02-22", "2026-02-23", "2026-02-28",
            "2026-04-04", "2026-04-05", "2026-04-06",
            "2026-05-01", "2026-05-02", "2026-05-03", "2026-05-04", "2026-05-05", "2026-05-09",
            "2026-06-19", "2026-06-20", "2026-06-21",
            "2026-09-25", "2026-09-26", "2026-09-27",
            "2026-10-01", "2026-10-02", "2026-10-03", "2026-10-04", "2026-10-05", "2026-10-06", "2026-10-07", "2026-10-10",
        }

    def _is_trading_day(self, date_str: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ "YYYY-MM-DD"
            
        Returns:
            True: æ˜¯äº¤æ˜“æ—¥
            False: æ˜¯ä¼‘å¸‚æ—¥ï¼ˆå‘¨æœ«æˆ–èŠ‚å‡æ—¥ï¼‰
        """
        try:
            date_obj = datetime.strptime(date_str, "%Y-%m-%d").date()
        except Exception:
            return False
        
        # 1. æ£€æŸ¥å‘¨æœ«ï¼ˆå‘¨å…­ã€å‘¨æ—¥ï¼‰
        if date_obj.weekday() >= 5:
            return False
        
        # 2. æ£€æŸ¥2026å¹´ä¼‘å¸‚æ—¥
        if date_obj.year == 2026:
            closures = self._get_market_closures_2026()
            if date_str in closures:
                return False
        
        return True

    def _get_next_trading_day(self, date_str: str) -> Optional[str]:
        """
        è·å–ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
        
        Args:
            date_str: å½“å‰æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ "YYYY-MM-DD"
            
        Returns:
            ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
        """
        try:
            current_dt = datetime.strptime(date_str, "%Y-%m-%d").date()
        except Exception:
            return None
        
        # æœ€å¤šæŸ¥æ‰¾30å¤©
        for i in range(1, 31):
            next_dt = current_dt + timedelta(days=i)
            next_str = next_dt.strftime("%Y-%m-%d")
            if self._is_trading_day(next_str):
                return next_str
        
        return None

    def _now_cn(self) -> datetime:
        """
        è¿”å›å½“å‰ä¸­å›½æ—¶é—´ï¼ˆAsia/Shanghaiï¼‰çš„ naive datetimeï¼Œä¾¿äºä¸ "YYYY-MM-DD HH:MM:SS" æ¯”è¾ƒã€‚
        """
        try:
            return datetime.now(ZoneInfo("Asia/Shanghai")).replace(tzinfo=None)
        except Exception:
            return datetime.now()

    async def _ensure_snapshot_prefetched(self, today_date: str, current_time: str, decision_count: int) -> None:
        """
        ç¡®ä¿ snapshot å·²é¢„ç”Ÿæˆã€‚å¦‚æœä¸å­˜åœ¨ï¼Œè¿è¡Œé¢„ç”Ÿæˆè„šæœ¬ï¼ˆç‹¬ç«‹è¿›ç¨‹ï¼‰ã€‚
        ä½¿ç”¨é”æœºåˆ¶ç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹è¿è¡Œé¢„ç”Ÿæˆè„šæœ¬ã€‚
        """
        if not self.prefetch_coordinator:
            return
        
        symbols_signature = self._symbols_signature()
        snapshot_path = self.prefetch_coordinator._snapshot_path(today_date, current_time, symbols_signature)
        
        # æ£€æŸ¥ snapshot æ˜¯å¦å­˜åœ¨ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
        if snapshot_path.exists():
            return
        # å°è¯•æ—§æ ¼å¼ï¼ˆä½¿ç”¨åŸå§‹ | åˆ†éš”ç¬¦ï¼‰
        sanitized_time = current_time.replace(":", "-").replace(" ", "_")
        old_format_path = self.prefetch_coordinator.snapshots_dir / today_date / f"{sanitized_time}_{symbols_signature}.json"
        if old_format_path.exists():
            return
        
        # ä½¿ç”¨é”æœºåˆ¶ï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹è¿è¡Œé¢„ç”Ÿæˆè„šæœ¬
        from agent_engine.shared_prefetch import _FileLock
        prefetch_lock_key = self.prefetch_coordinator._decision_key(today_date, current_time, symbols_signature)
        prefetch_lock_path = self.prefetch_coordinator._lock_path(f"prefetch_{prefetch_lock_key}")
        
        # å°è¯•è·å–é”ï¼ˆé˜»å¡ç­‰å¾…ï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹è¿è¡Œé¢„ç”Ÿæˆè„šæœ¬ï¼‰
        prefetch_lock = _FileLock(str(prefetch_lock_path), timeout=0.0)  # æ— é™ç­‰å¾…
        if prefetch_lock.acquire(timeout=0.0):  # æ— é™ç­‰å¾…
            try:
                # å†æ¬¡æ£€æŸ¥ snapshot æ˜¯å¦å­˜åœ¨ï¼ˆå¯èƒ½åœ¨ç­‰å¾…é”æœŸé—´å·²è¢«å…¶ä»–è¿›ç¨‹ç”Ÿæˆï¼‰
                if snapshot_path.exists():
                    print(f"ğŸ“„ Snapshot å·²å­˜åœ¨ï¼ˆç”±å…¶ä»–è¿›ç¨‹ç”Ÿæˆï¼‰: {current_time}")
                    return
                
                print(f"ğŸ“¦ Snapshot ä¸å­˜åœ¨ï¼Œè¿è¡Œé¢„ç”Ÿæˆè„šæœ¬: {current_time}")
                
                # è¿è¡Œé¢„ç”Ÿæˆè„šæœ¬ï¼ˆç‹¬ç«‹è¿›ç¨‹ï¼‰
                prefetch_script = Path(__file__).resolve().parents[2] / "utilities" / "prefetch_snapshots.py"
                if not prefetch_script.exists():
                    print(f"âš ï¸ é¢„ç”Ÿæˆè„šæœ¬ä¸å­˜åœ¨: {prefetch_script}")
                    return
                
                cmd = [
                    sys.executable,
                    str(prefetch_script),
                ]
                env = os.environ.copy()
                env["TODAY_DATE"] = today_date
                env["CURRENT_TIME"] = current_time
                env["DECISION_COUNT"] = str(decision_count)
                
                try:
                    result = subprocess.run(
                        cmd,
                        env=env,
                        cwd=str(Path(__file__).resolve().parents[2]),
                        capture_output=True,
                        text=True,
                        timeout=300,  # 5 åˆ†é’Ÿè¶…æ—¶
                    )
                    
                    if result.returncode == 0:
                        print(f"âœ… Snapshot é¢„ç”ŸæˆæˆåŠŸ: {current_time}")
                    else:
                        print(f"âš ï¸ Snapshot é¢„ç”Ÿæˆå¤±è´¥: {current_time}")
                        if result.stderr:
                            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr[:500]}")
                except subprocess.TimeoutExpired:
                    print(f"âš ï¸ Snapshot é¢„ç”Ÿæˆè¶…æ—¶: {current_time}")
                except Exception as e:
                    print(f"âš ï¸ Snapshot é¢„ç”Ÿæˆå¼‚å¸¸: {e}")
            finally:
                prefetch_lock.release()
        else:
            # æ— æ³•è·å–é”ï¼Œè¯´æ˜å…¶ä»–è¿›ç¨‹æ­£åœ¨è¿è¡Œé¢„ç”Ÿæˆè„šæœ¬ï¼Œç­‰å¾… snapshot ç”Ÿæˆ
            print(f"â³ ç­‰å¾…å…¶ä»–è¿›ç¨‹ç”Ÿæˆ snapshot: {current_time}")
            wait_count = 0
            max_wait = 300  # æœ€å¤šç­‰å¾… 5 åˆ†é’Ÿ
            while not snapshot_path.exists() and wait_count < max_wait:
                await asyncio.sleep(1)
                wait_count += 1
            
            if not snapshot_path.exists():
                print(f"âš ï¸ ç­‰å¾…è¶…æ—¶ï¼Œsnapshot ä»æœªç”Ÿæˆ: {current_time}")
                # æ³¨æ„ï¼šè¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©åç»­çš„ ensure_snapshot ä½œä¸º fallback å¤„ç†
                # ensure_snapshot æœ‰é”æœºåˆ¶ï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹ç”Ÿæˆ

    async def run_intraday_trading(self, today_date: str, start_index: int = 0) -> bool:
        """
        æ¯å¤©è¿›è¡Œ3æ¬¡äº¤æ˜“å†³ç­–ï¼š10:30ã€11:30ã€14:00ã€‚

        Returns:
            True: ç”±äº REALTIME_MODE=stop ä¸”é‡åˆ°æœªæ¥æ—¶ç‚¹ï¼Œæå‰ç»“æŸï¼ˆç”¨äºè®©ä¸Šå±‚æ—¥æœŸå¾ªç¯ä¹Ÿåœæ­¢ï¼‰
            False: æ­£å¸¸å®Œæˆ/æˆ–æœªå¯ç”¨ stop æ¨¡å¼
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        if not self._is_trading_day(today_date):
            realtime_mode = str(get_runtime_config_value("REALTIME_MODE") or "").strip().lower()
            if realtime_mode == "wait":
                # wait mode: ç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥çš„ç¬¬ä¸€ä¸ªå†³ç­–æ—¶ç‚¹ï¼ˆ10:30ï¼‰
                next_trading_day = self._get_next_trading_day(today_date)
                if next_trading_day:
                    # ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ‰€æœ‰å†³ç­–æ—¶ç‚¹
                    next_trading_hours = self._get_trading_hours(next_trading_day)
                    now_dt = self._now_cn()
                    
                    # æ‰¾åˆ°ä¸‹ä¸€ä¸ªè¿˜æœªè¿‡çš„å†³ç­–æ—¶ç‚¹
                    next_decision_time = None
                    for decision_time in next_trading_hours:
                        decision_dt = datetime.strptime(decision_time, "%Y-%m-%d %H:%M:%S")
                        if now_dt < decision_dt:
                            next_decision_time = decision_time
                            break
                    
                    if next_decision_time:
                        # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœªè¿‡çš„å†³ç­–æ—¶ç‚¹ï¼Œç­‰å¾…åˆ°è¯¥æ—¶ç‚¹
                        next_decision_dt = datetime.strptime(next_decision_time, "%Y-%m-%d %H:%M:%S")
                        delta = (next_decision_dt - now_dt).total_seconds()
                        hours = int(delta / 3600)
                        minutes = int((delta % 3600) / 60)
                        print(f"â¸ï¸ {today_date} æ˜¯ä¼‘å¸‚æ—¥ï¼ŒREALTIME_MODE=waitï¼šç­‰å¾…åˆ°ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ {next_trading_day} {next_decision_time.split()[1]}ï¼ˆçº¦ {hours} å°æ—¶ {minutes} åˆ†é’Ÿï¼‰...")
                        await asyncio.sleep(delta)
                        # é€’å½’è°ƒç”¨ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä»å¯¹åº”çš„å†³ç­–æ—¶ç‚¹å¼€å§‹
                        decision_index = next_trading_hours.index(next_decision_time)
                        return await self.run_intraday_trading(next_trading_day, start_index=decision_index)
                    else:
                        # ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ‰€æœ‰å†³ç­–æ—¶ç‚¹éƒ½å·²è¿‡ï¼Œç»§ç»­æ‰¾ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥
                        # é¿å…æ— é™é€’å½’ï¼šæœ€å¤šæŸ¥æ‰¾30ä¸ªäº¤æ˜“æ—¥
                        max_iterations = 30
                        current_check_date = next_trading_day
                        iteration = 0
                        while iteration < max_iterations:
                            iteration += 1
                            next_check_day = self._get_next_trading_day(current_check_date)
                            if not next_check_day:
                                print(f"â¸ï¸ {today_date} æ˜¯ä¼‘å¸‚æ—¥ï¼Œä¸”æ‰¾ä¸åˆ°æ›´å¤šäº¤æ˜“æ—¥ï¼Œè·³è¿‡")
                                return False
                            
                            check_trading_hours = self._get_trading_hours(next_check_day)
                            for check_time in check_trading_hours:
                                check_dt = datetime.strptime(check_time, "%Y-%m-%d %H:%M:%S")
                                if now_dt < check_dt:
                                    # æ‰¾åˆ°æœªè¿‡çš„å†³ç­–æ—¶ç‚¹
                                    delta = (check_dt - now_dt).total_seconds()
                                    hours = int(delta / 3600)
                                    minutes = int((delta % 3600) / 60)
                                    print(f"â¸ï¸ {today_date} æ˜¯ä¼‘å¸‚æ—¥ï¼ŒREALTIME_MODE=waitï¼šç­‰å¾…åˆ°äº¤æ˜“æ—¥ {next_check_day} {check_time.split()[1]}ï¼ˆçº¦ {hours} å°æ—¶ {minutes} åˆ†é’Ÿï¼‰...")
                                    await asyncio.sleep(delta)
                                    decision_index = check_trading_hours.index(check_time)
                                    return await self.run_intraday_trading(next_check_day, start_index=decision_index)
                            
                            # è¿™ä¸ªäº¤æ˜“æ—¥çš„æ‰€æœ‰æ—¶ç‚¹éƒ½å·²è¿‡ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                            current_check_date = next_check_day
                        
                        # æŸ¥æ‰¾äº†30ä¸ªäº¤æ˜“æ—¥éƒ½æ²¡æ‰¾åˆ°æœªè¿‡çš„æ—¶ç‚¹ï¼Œè·³è¿‡
                        print(f"â¸ï¸ {today_date} æ˜¯ä¼‘å¸‚æ—¥ï¼ŒæŸ¥æ‰¾äº† {max_iterations} ä¸ªäº¤æ˜“æ—¥éƒ½æœªæ‰¾åˆ°æœªè¿‡çš„å†³ç­–æ—¶ç‚¹ï¼Œè·³è¿‡")
                        return False
                else:
                    print(f"â¸ï¸ {today_date} æ˜¯ä¼‘å¸‚æ—¥ï¼Œä¸”æ‰¾ä¸åˆ°ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œè·³è¿‡")
                    return False
            else:
                # stop mode æˆ–å…¶ä»–æ¨¡å¼: ç›´æ¥è·³è¿‡
                print(f"â¸ï¸ {today_date} æ˜¯ä¼‘å¸‚æ—¥ï¼ˆèŠ‚å‡æ—¥æˆ–å‘¨æœ«ä¼‘å¸‚ï¼‰ï¼Œè·³è¿‡")
                return False
        
        trading_hours = self._get_trading_hours(today_date)

        # å¯é€‰ï¼šåªè·‘æŸä¸€æ¬¡å†³ç­–ï¼ˆç”¨äºè¡¥è·‘/é‡è·‘å•ä¸ªæ—¶ç‚¹ï¼‰ã€‚
        # ä¾‹å¦‚ ONLY_DECISION_COUNT=2 åªè·‘ 11:30ï¼Œä¸ç»§ç»­è·‘ 14:00ã€‚
        only_decision_count = None
        only_raw = str(get_runtime_config_value("ONLY_DECISION_COUNT") or "").strip()
        if only_raw:
            try:
                only_decision_count = int(float(only_raw))
            except Exception:
                only_decision_count = None
        if only_decision_count not in (1, 2, 3):
            only_decision_count = None

        for idx, current_time in enumerate(trading_hours[start_index:], start_index + 1):
            if only_decision_count is not None:
                if idx < only_decision_count:
                    continue
                if idx > only_decision_count:
                    break

            # realtime æ¨¡å¼ï¼šé‡åˆ°æœªæ¥æ—¶ç‚¹å°±â€œåœæ­¢â€æˆ–â€œç­‰å¾…åˆ°ç‚¹â€
            # - REALTIME_MODE=stop: æœªåˆ°ç‚¹å°±ç»“æŸæœ¬æ¬¡è¿›ç¨‹ï¼ˆé€‚åˆå¤–éƒ¨å®šæ—¶å™¨/æ‰‹åŠ¨å¤šæ¬¡è§¦å‘ï¼‰
            # - REALTIME_MODE=wait: æœªåˆ°ç‚¹å°± sleep ç­‰åˆ°ç‚¹ï¼ˆä¸€æ¬¡å¯åŠ¨è·‘å®Œæ•´å¤©ï¼‰
            realtime_mode = str(get_runtime_config_value("REALTIME_MODE") or "").strip().lower()
            if realtime_mode in ("stop", "wait"):
                try:
                    target_dt = datetime.strptime(current_time, "%Y-%m-%d %H:%M:%S")
                except Exception:
                    target_dt = None

                if target_dt is not None:
                    now_dt = self._now_cn()
                    if now_dt < target_dt:
                        delta = (target_dt - now_dt).total_seconds()
                        if realtime_mode == "stop":
                            print(f"â¹ï¸ REALTIME_MODE=stopï¼šæœªåˆ° {current_time}ï¼ˆè¿˜å·® {int(delta)}sï¼‰ï¼Œåœæ­¢ï¼Œé¿å…æå‰è·‘æœªæ¥æ—¶ç‚¹ã€‚")
                            return True
                        if delta > 0:
                            print(f"â³ REALTIME_MODE=waitï¼šç­‰å¾…åˆ° {current_time}ï¼ˆçº¦ {int(delta)}sï¼‰...")
                            await asyncio.sleep(delta)

            print(f"ğŸ•’ ç¬¬ {idx}/3 æ¬¡å†³ç­– - æ—¶é—´: {current_time}")
            write_runtime_config_value("CURRENT_TIME", current_time)
            write_runtime_config_value("DECISION_COUNT", idx)  # è®°å½•ç¬¬å‡ æ¬¡å†³ç­–
            
            # åœ¨å†³ç­–æ—¶ç‚¹åˆ°è¾¾æ—¶ï¼Œå…ˆé¢„ç”Ÿæˆ snapshotï¼Œç„¶åå†æ‰§è¡Œäº¤æ˜“
            # ä½¿ç”¨é”æœºåˆ¶ç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹è¿è¡Œé¢„ç”Ÿæˆè„šæœ¬
            await self._ensure_snapshot_prefetched(today_date, current_time, idx)
            
            await self.run_session_with_retry(today_date, current_time, decision_count=idx)
        return False

    def register_agent(self) -> None:
        """Register new agentic workflow, create initial positions"""
        position_exists = os.path.exists(self.position_file)
        if position_exists and not self.force_replay:
            print(f"âš ï¸ Position file {self.position_file} already exists, skipping registration")
            return
        if position_exists and self.force_replay:
            print(f"ğŸ—‘ï¸ force_replay=True, clearing existing data_pipeline for {self.signature}")
            self._reset_agent_storage()
        
        # Ensure directory structure exists
        os.makedirs(self.data_path, exist_ok=True)
        position_dir = os.path.join(self.data_path, "position")
        log_dir = os.path.join(self.data_path, "log")
        os.makedirs(position_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        
        # Determine initial record date & decision time
        seed_date = self.init_date
        seed_decision_time = f"{self.init_date} 00:00:00"
        if not seed_date:
            seed_date = datetime.now().strftime("%Y-%m-%d")
            seed_decision_time = f"{seed_date} 00:00:00"
        
        # Create initial positions
        init_position = {symbol: {"shares": 0, "purchase_date": None} for symbol in self.stock_symbols}
        init_position['CASH'] = self.initial_cash
        
        with open(self.position_file, "w") as f:  # Use "w" mode to ensure creating new file
            f.write(json.dumps({
                "date": seed_date,
                "decision_time": seed_decision_time,
                "decision_count": 0,
                "id": 0, 
                "seed": True,
                "this_action": {"action": "seed", "symbol": "", "amount": 0},
                "positions": init_position
            }) + "\n")
        
        print(f"âœ… Agent {self.signature} registration completed")
        print(f"ğŸ“ Position file: {self.position_file}")
        print(f"ğŸ’° Initial cash: ${self.initial_cash}")
        print(f"ğŸ“Š Number of stocks: {len(self.stock_symbols)}")
    
    def get_trading_dates(self, init_date: str, end_date: str) -> List[str]:
        """
        Get trading date list
        
        Args:
            init_date: Start date
            end_date: End date
            
        Returns:
            List of trading dates
        """
        if not os.path.exists(self.position_file):
            self.register_agent()
        
        start_dt = datetime.strptime(init_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        last_completed: Optional[datetime] = None
        
        if os.path.exists(self.position_file):
            with open(self.position_file, "r") as f:
                for line in f:
                    try:
                        doc = json.loads(line)
                    except Exception:
                        continue
                    if doc.get("seed"):
                        continue
                    date_str = doc.get("date")
                    if not date_str:
                        continue
                    try:
                        decision_dt = datetime.strptime(date_str, "%Y-%m-%d")
                    except Exception:
                        continue
                    if decision_dt < start_dt:
                        continue
                    if (last_completed is None) or (decision_dt > last_completed):
                        last_completed = decision_dt
        
        current_dt = start_dt
        if last_completed and last_completed >= start_dt:
            next_day = last_completed + timedelta(days=1)
            if next_day > current_dt:
                current_dt = next_day
        
        trading_dates: List[str] = []
        while current_dt <= end_dt:
            date_str = current_dt.strftime("%Y-%m-%d")
            # ä½¿ç”¨ _is_trading_day æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆåŒ…æ‹¬å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰
            if self._is_trading_day(date_str):
                trading_dates.append(date_str)
            current_dt += timedelta(days=1)
        
        return trading_dates
    
    def _determine_resume_point(self, init_date: str, end_date: str) -> Optional[tuple[str, int]]:
        """
        æ ¹æ®æŒä»“è®°å½•åˆ¤æ–­æ˜¯å¦éœ€è¦åœ¨æŸä¸ªæ—¥æœŸ/æ—¶é—´ç‚¹é‡æ–°å¼€å§‹ã€‚
        è¿”å› (date, start_index)ï¼›start_index å¯¹åº” trading_hours ä¸­çš„ç´¢å¼•ã€‚
        """
        if not os.path.exists(self.position_file):
            return None

        try:
            init_dt = datetime.strptime(init_date, "%Y-%m-%d").date()
            end_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
        except Exception:
            return None

        # å¼ºåˆ¶é‡è·‘å•æ—¥çš„æŸä¸€æ¬¡å†³ç­–ï¼ˆå³ä½¿è¯¥æ—¥å·²â€œè·‘å®Œâ€ä¹Ÿå…è®¸é‡è·‘ï¼‰ã€‚
        # ç”¨æ³•ç¤ºä¾‹ï¼š
        #   INIT_DATE=2026-01-13 END_DATE=2026-01-13 ONLY_DECISION_COUNT=3  -> åªé‡è·‘ 14:00
        only_decision_count = None
        only_raw = str(get_runtime_config_value("ONLY_DECISION_COUNT") or "").strip()
        if only_raw:
            try:
                only_decision_count = int(float(only_raw))
            except Exception:
                only_decision_count = None
        if only_decision_count in (1, 2, 3) and init_date == end_date:
            return init_date, only_decision_count - 1

        candidate_records: List[Dict[str, Any]] = []
        with open(self.position_file, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    doc = json.loads(line)
                except Exception:
                    continue

                decision_time = doc.get("decision_time")
                decision_date = doc.get("date")
                if not decision_time or not decision_date:
                    continue

                try:
                    decision_date_obj = datetime.strptime(decision_date, "%Y-%m-%d").date()
                except Exception:
                    continue

                if decision_date_obj < init_dt or decision_date_obj > end_dt:
                    continue

                candidate_records.append(doc)

        if not candidate_records:
            return None

        candidate_records.sort(
            key=lambda item: (
                item.get("date", ""),
                item.get("decision_time", ""),
                item.get("id", 0),
            )
        )

        last_date = candidate_records[-1].get("date")
        if not last_date:
            return None

        trading_hours = self._get_trading_hours(last_date)
        recorded_times = {
            item.get("decision_time")
            for item in candidate_records
            if item.get("date") == last_date and item.get("decision_time")
        }

        if not recorded_times:
            return None

        # æ‰¾åˆ°"ç¬¬ä¸€ä¸ªç¼ºå¤±çš„äº¤æ˜“æ—¶ç‚¹"ï¼Œä»è¿™é‡Œå¼€å§‹ç»§ç»­è·‘ï¼Œé¿å…é‡è·‘å·²å®Œæˆçš„æ—¶ç‚¹ã€‚
        for idx, decision_time in enumerate(trading_hours):
            if decision_time not in recorded_times:
                return last_date, idx

        # æœ€åä¸€ä¸ªäº¤æ˜“æ—¥å·²å…¨éƒ¨å®Œæˆ
        return None
    
    async def run_session_with_retry(self, today_date: str, current_time: str, decision_count: int = 1) -> None:
        """Run method with retry"""
        for attempt in range(1, self.max_retries + 1):
            try:
                print(f"ğŸ”„ Attempting to run {self.signature} - {current_time} (Decision {decision_count}/3, Attempt {attempt})")
                await self.run_trading_session(today_date, current_time, decision_count)
                print(f"âœ… {self.signature} - {current_time} run successful")
                return
            except Exception as e:
                print(f"âŒ Attempt {attempt} failed: {str(e)}")
                if attempt == self.max_retries:
                    print(f"ğŸ’¥ {self.signature} - {current_time} all retries failed")
                    raise
                else:
                    wait_time = self.base_delay * attempt
                    print(f"â³ Waiting {wait_time} seconds before retry...")
                    await asyncio.sleep(wait_time)
    
    async def run_date_range(self, init_date: str, end_date: str) -> None:
        """
        Run all trading days in date range
        
        Args:
            init_date: Start date
            end_date: End date
        """
        print(f"ğŸ“… Running date range: {init_date} to {end_date}")
        
        trading_dates = self.get_trading_dates(init_date, end_date)
        resume_point = self._determine_resume_point(init_date, end_date)

        # æ‰“å°æ–­ç‚¹é‡è¿ä¿¡æ¯
        if resume_point:
            resume_date, resume_index = resume_point
            trading_hours = self._get_trading_hours(resume_date)
            resume_time = trading_hours[resume_index] if resume_index < len(trading_hours) else "unknown"
            print(f"ğŸ”„ æ£€æµ‹åˆ°æ–­ç‚¹é‡è¿: ä» {resume_date} çš„ {resume_time} (ç´¢å¼• {resume_index}) ç»§ç»­")
        else:
            if os.path.exists(self.position_file):
                print(f"â„¹ï¸  æœªæ‰¾åˆ°éœ€è¦ç»§ç»­çš„æ–­ç‚¹ï¼ˆæ‰€æœ‰æ—¥æœŸå·²å®Œæˆæˆ–ä»æ–°æ—¥æœŸå¼€å§‹ï¼‰")
            else:
                print(f"â„¹ï¸  é¦–æ¬¡è¿è¡Œï¼Œä» {init_date} å¼€å§‹")

        start_indices: Dict[str, int] = {}
        dates_to_process: List[str] = []

        if resume_point:
            resume_date, resume_index = resume_point
            dates_to_process.append(resume_date)
            start_indices[resume_date] = resume_index

        for date in trading_dates:
            dates_to_process.append(date)

        # ä¿æŒé¡ºåºå¹¶å»é‡
        ordered_dates: List[str] = []
        seen_dates = set()
        for date in dates_to_process:
            if date not in seen_dates:
                ordered_dates.append(date)
                seen_dates.add(date)

        if not ordered_dates:
            print(f"â„¹ï¸ No trading days to process")
            if os.path.exists(self.position_file):
                print("ğŸ’¡ Hint: set FORCE_REPLAY=true (or agent_config.force_replay) to reset state for replays.")
            return
        
        print(f"ğŸ“Š Trading days to process: {ordered_dates}")
        
        # Process each trading day
        for date in ordered_dates:
            print(f"ğŸ”„ Processing {self.signature} - Date: {date}")
            
            # Set configuration
            write_runtime_config_value("TODAY_DATE", date)
            write_runtime_config_value("SIGNATURE", self.signature)
            
            try:
                start_index = start_indices.get(date, 0)
                stopped_early = await self.run_intraday_trading(date, start_index=start_index)
                if stopped_early:
                    # stop æ¨¡å¼ä¸‹é‡åˆ°æœªæ¥æ—¶ç‚¹ï¼šç›´æ¥ç»“æŸæ•´ä¸ªæ—¥æœŸåŒºé—´å¾ªç¯ï¼ˆé¿å…éå†æœªæ¥æ—¥æœŸåˆ·å±ï¼‰
                    break
            except Exception as e:
                print(f"âŒ Error processing {self.signature} - Date: {date}")
                print(e)
                raise
        if self.dm:
            try:
                self.dm.close_ts_client(force=True)
            except Exception as close_err:
                print(f"âš ï¸ Failed to close TinySoft session: {close_err}")
        
        # åœ¨æ—¥æœŸèŒƒå›´ç»“æŸæ—¶åˆ›å»ºæœ€ç»ˆå¤‡ä»½
        try:
            from utils.backup_utils import run_backup_snapshot
            reason = f"date_range_complete_{self.signature}_{init_date}_to_{end_date}"
            ok = run_backup_snapshot(reason=reason)
            if ok:
                try:
                    print(f"[OK] Final backup completed for {self.signature} after date range {init_date} to {end_date}")
                except UnicodeEncodeError:
                    print(f"Final backup completed for {self.signature} after date range {init_date} to {end_date}")
            else:
                try:
                    print(f"[WARNING] Final backup failed for {self.signature}. Latest data may not be backed up.")
                except UnicodeEncodeError:
                    print(f"WARNING: Final backup failed for {self.signature}. Latest data may not be backed up.")
        except Exception as e:
            try:
                print(f"[WARNING] Error during final backup: {e}")
            except UnicodeEncodeError:
                print(f"WARNING: Error during final backup: {e}")
        
        print(f"âœ… {self.signature} processing completed")
    
    def get_position_summary(self) -> Dict[str, Any]:
        """Get position summary"""
        if not os.path.exists(self.position_file):
            return {"error": "Position file does not exist"}
        
        positions: List[Dict[str, Any]] = []
        bad_lines = 0
        with open(self.position_file, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    positions.append(json.loads(line))
                except Exception:
                    bad_lines += 1
                    continue
        
        if not positions:
            return {"error": "No position records"}
        
        latest_position = positions[-1]
        if bad_lines:
            # ä¸ä¸­æ–­ä¸»æµç¨‹ï¼Œä½†æç¤ºæ•°æ®æ–‡ä»¶å¯èƒ½è¢«ä¸­é€”å†™å/å¹¶å‘å†™å†²çª
            print(f"âš ï¸ Detected {bad_lines} invalid JSON line(s) in {self.position_file}; skipped.")
        return {
            "signature": self.signature,
            "latest_date": latest_position.get("date"),
            "positions": latest_position.get("positions", {}),
            "total_records": len(positions)
        }
    
    def __str__(self) -> str:
        return f"AgenticWorkflow(signature='{self.signature}', basemodel='{self.basemodel}', stocks={len(self.stock_symbols)})"
    
    def __repr__(self) -> str:
        return self.__str__()
