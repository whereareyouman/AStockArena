"""
BaseAgent class - Base class for trading agents
Encapsulates core functionality including local tool management, AI agent creation, and trading execution
"""

import copy
import os
import sys
import json
import asyncio
import threading
import random
import shutil
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
import pandas_ta as ta
from dotenv import load_dotenv
import threading

from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.prebuilt import create_react_agent

# Import project tools
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

load_dotenv()

from data_manager import DataManager
from agent.shared_prefetch import SharedPrefetchCoordinator
from tools.general_tools import extract_conversation, extract_tool_messages, get_config_value, write_config_value
from tools.json_file_manager import safe_read_json
from tools.price_tools import (
    add_no_trade_record,
    get_yesterday_date,
    get_latest_position,
    normalize_decision_time,
    normalize_positions,
    normalize_symbol,
    strip_exchange_prefix,
    upsert_position_record,
    get_price_limits,
)
from prompts.agent_prompt import get_agent_system_prompt, STOP_SIGNAL

# å¹¶å‘å®‰å…¨ï¼šç”¨äºä¿æŠ¤ news.csv å†™å…¥
NEWS_FILE_LOCK = threading.Lock()


class BaseAgent:
    """
    Base class for trading agents
    
    Main functionalities:
    1. Local tool management (DataManager and price tools)
    2. AI agent creation and configuration
    3. Trading execution and decision loops
    4. Logging and management
    5. Position and configuration management
    """
    
    # ç§‘åˆ›æ¿ä»£è¡¨æ€§è‚¡ç¥¨ï¼ˆSTAR Market Stocksï¼‰
    DEFAULT_STOCK_SYMBOLS = [
        "SH688008",  # æ¾œèµ·ç§‘æŠ€ *
        "SH688111",  # é‡‘å±±åŠå…¬ *
        "SH688009",  # ä¸­å›½é€šå· *
        "SH688981",  # ä¸­èŠ¯å›½é™… *
        "SH688256",  # å¯’æ­¦çºª *
        "SH688271",  # è”å½±åŒ»ç–— *
        "SH688047",  # é¾™èŠ¯ä¸­ç§‘ *
        "SH688617",  # æƒ æ³°åŒ»ç–— *
        "SH688303",  # å¤§å…¨èƒ½æº *
        "SH688180",  # å›å®ç”Ÿç‰© *
    ]
    MINI_CANDLE_COUNT = 6
    RECENT_CLOSE_COUNT = 6
    LIMIT_ORDER_SUCCESS_RATE = 0.10
    LIMIT_THRESHOLD_RATIO = 0.999
    def __init__(
        self,
        signature: str,
        basemodel: str,
        stock_symbols: Optional[List[str]] = None,
        stock_json_path: str = "./data/ai_stock_data.json",
        news_csv_path: str = "./data/news.csv",
        macro_csv_path: Optional[str] = None,
        log_path: Optional[str] = None,
        max_steps: int = 10,
        max_retries: int = 3,
        base_delay: float = 0.5,
        openai_base_url: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        google_api_key: Optional[str] = None,
        safety_settings: Optional[Dict[str, str]] = None,
        initial_cash: float = 1000000.0,
        init_date: Optional[str] = None,
        trading_rules: Optional[Dict[str, float]] = None,
        risk_management: Optional[Dict[str, float]] = None,
        force_replay: bool = False,
    ):
        """
        Initialize BaseAgent
        
        Args:
            signature: Agent signature/name
            basemodel: Base model name
            stock_symbols: List of stock symbols
            stock_json_path: Path to stock price JSON file (ai_stock_data.json)
            news_csv_path: Path to news CSV file
            macro_csv_path: Path to macro news CSV file
            log_path: Log path, defaults to ./data/agent_data
            max_steps: Maximum reasoning steps
            max_retries: Maximum retry attempts
            base_delay: Base delay time for retries
            openai_base_url: OpenAI API base URL
            openai_api_key: OpenAI API key
            google_api_key: Google Gemini API key
            safety_settings: Google Gemini safety settings
            initial_cash: Initial cash amount
            init_date: Initialization date
            trading_rules: Dictionary with trading rule settings
            risk_management: Dictionary with risk management settings
        """
        self.signature = signature
        self.basemodel = basemodel
        self.stock_symbols = stock_symbols or self.DEFAULT_STOCK_SYMBOLS
        self.allowed_symbols = {
            normalize_symbol(sym) for sym in self.stock_symbols if normalize_symbol(sym)
        }

        def _resolve_path(input_path: Optional[str], default_subpath: Optional[str]) -> Optional[str]:
            target = input_path or default_subpath
            if not target:
                return None
            if os.path.isabs(target):
                return target
            cleaned = target.lstrip("./")
            return os.path.join(project_root, cleaned)

        self.stock_json_path = _resolve_path(stock_json_path, "data/ai_stock_data.json")
        self.news_csv_path = _resolve_path(news_csv_path, "data/news.csv")
        self.macro_csv_path = _resolve_path(macro_csv_path, None)
        self._prefetched_news: Dict[str, Dict[str, Any]] = {}
        self._prefetched_prices: Dict[str, Dict[str, Any]] = {}
        self._prefetched_indicators: Dict[str, Dict[str, Any]] = {}
        self._last_prefetch_bundle: Optional[Dict[str, Any]] = None
        self._current_snapshot_info: Dict[str, Any] = {}
        self.max_steps = max_steps
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.initial_cash = initial_cash
        if not init_date:
            raise ValueError("init_date must be provided for BaseAgent")
        self.init_date = init_date
        self.force_replay = force_replay
        
        # Load trading rules and risk management from config
        self.trading_rules = trading_rules or {}
        self.risk_management = risk_management or {}
        
        # Set log path
        self.base_log_path = _resolve_path(log_path, "data/agent_data")
        os.makedirs(self.base_log_path, exist_ok=True)
        shared_override = os.getenv("SHARED_PREFETCH_DIR")
        self.shared_prefetch_root = _resolve_path(shared_override, "data/agent_data/shared")
        if self.shared_prefetch_root:
            os.makedirs(self.shared_prefetch_root, exist_ok=True)
        self.prefetch_coordinator = SharedPrefetchCoordinator(base_dir=self.shared_prefetch_root)
        
        # Set OpenAI configuration
        if openai_base_url is None:
            if self.basemodel.startswith("qwen"):
                self.openai_base_url = (
                    os.getenv("QWEN_API_BASE")
                    or os.getenv("DASHSCOPE_API_BASE")
                    or "https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
            else:
                self.openai_base_url = os.getenv("OPENAI_API_BASE")
        else:
            self.openai_base_url = openai_base_url

        if openai_api_key is None:
            if self.basemodel.startswith("qwen"):
                self.openai_api_key = (
                    os.getenv("QWEN_API_KEY")
                    or os.getenv("DASHSCOPE_API_KEY")
                    or os.getenv("OPENAI_API_KEY")
                )
            else:
                self.openai_api_key = os.getenv("OPENAI_API_KEY")
        else:
            self.openai_api_key = openai_api_key
        
        # Set Google Gemini configuration
        if google_api_key is None:
            self.google_api_key = os.getenv("GEMINI_API_KEY")
        else:
            self.google_api_key = google_api_key
        self.safety_settings = safety_settings
        
        # é¢„å…ˆæ¸…ç†æ–°é—»ç¼“å­˜ï¼Œé¿å…åŠ è½½éç™½åå•è‚¡ç¥¨
        if self.news_csv_path:
            self._purge_news_csv(self.news_csv_path)
        
        # Initialize DataManagerï¼ˆç°åœ¨ä¸éœ€è¦CSVï¼Œå®Œå…¨ä¾èµ–TinySoftå®æ—¶æ•°æ®ï¼‰
        try:
            self.dm = DataManager(
                stock_csv_path=None,  # ä¸å†ä½¿ç”¨CSVï¼Œå®Œå…¨ä¾èµ–TinySoftå®æ—¶è·å–
                news_csv_path=self.news_csv_path,
                macro_csv_path=self.macro_csv_path
            )
            print(f"âœ… DataManager åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨TinySoftå®æ—¶æ•°æ®æºï¼‰")
            if self.dm.news_df is not None:
                self.dm.news_df = self._filter_allowed_news_df(self.dm.news_df)
        except Exception as e:
            print(f"âŒ DataManager åˆå§‹åŒ–å¤±è´¥: {e}")
            self.dm = None
        
        # Initialize components
        self.local_tools: List = []  # æœ¬åœ° DataManager å·¥å…·
        self.tools: List = []  # åˆå¹¶åçš„æ‰€æœ‰å·¥å…·
        self.model: Optional[Any] = None
        self.agent: Optional[Any] = None
        
        # Data paths
        self.data_path = os.path.join(self.base_log_path, self.signature)
        self.position_file = os.path.join(self.data_path, "position", "position.jsonl")
        
        # --- å¹¶å‘è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ˆæ›¿ä»£å¯¹å…¨å±€ runtime_env.json çš„è¯»å–ä¾èµ–ï¼‰ ---
        self.runtime_context: Dict[str, Any] = {
            "TODAY_DATE": None,
            "CURRENT_TIME": None,
            "DECISION_COUNT": 0
        }
    
    def _reset_agent_storage(self) -> None:
        """Remove existing agent data directory (positions + logs) when replaying."""
        agent_path = Path(self.data_path)
        if agent_path.exists():
            shutil.rmtree(agent_path, ignore_errors=True)
    
    def _get_context_value(self, key: str):
        """ä¼˜å…ˆè¯»å–å®ä¾‹çº§ä¸Šä¸‹æ–‡ï¼Œå…¶æ¬¡å›é€€åˆ°å…¨å±€é…ç½®ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰"""
        if key in self.runtime_context and self.runtime_context[key] is not None:
            return self.runtime_context[key]
        return get_config_value(key)
    
    def _is_allowed_symbol(self, symbol: Optional[str], *, allow_sell_existing: bool = False) -> bool:
        """
        åˆ¤æ–­ç¬¦å·æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­ã€‚
        allow_sell_existing=True æ—¶ï¼Œå¦‚æœç¬¦å·å½“å‰æŒä»“ä¸­å­˜åœ¨ï¼Œä¹Ÿå…è®¸ã€‚
        """
        normalized = normalize_symbol(symbol) if symbol else None
        if normalized and normalized in self.allowed_symbols:
            return True
        if allow_sell_existing and normalized:
            latest_positions, _, _ = get_latest_position(
                self._get_context_value("TODAY_DATE") or self.init_date,
                self.signature
            )
            return normalized in latest_positions
        return False

    def _allowed_symbol_list(self) -> List[str]:
        return sorted(sym for sym in self.allowed_symbols if sym)

    def _filter_allowed_news_df(self, df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:
        if df is None or df.empty or 'symbol' not in df.columns:
            return df
        filtered = df[df['symbol'].astype(str).apply(
            lambda sym: self._is_allowed_symbol(normalize_symbol(sym))
        )]
        return filtered

    def _filter_allowed_news_records(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        filtered: List[Dict[str, Any]] = []
        for item in records:
            sym = normalize_symbol(item.get("symbol"))
            if self._is_allowed_symbol(sym):
                filtered.append(item)
        return filtered

    def _prefetch_all_news(self, today_date: str, current_time: str, max_retries: int = 2) -> None:
        print("ğŸ“° Prefetching news for all whitelisted symbols...")
        cutoff = pd.to_datetime(today_date) - pd.Timedelta(days=3)
        for sym in self.stock_symbols:
            normalized = normalize_symbol(sym)
            if not normalized:
                continue
            raw_query = f"{strip_exchange_prefix(normalized) or normalized} æœ€æ–°æ¶ˆæ¯"
            result_str = self.search_stock_news(raw_query, max_retries=max_retries)
            try:
                payload = json.loads(result_str)
            except Exception:
                continue
            if not payload.get("success"):
                continue

            entries = payload.get("historical_news", []) + payload.get("realtime_news", [])
            entries = self._filter_allowed_news_records(entries)
            recent_items: List[Dict[str, Any]] = []
            for item in entries:
                title = str(item.get("title") or item.get("æ–°é—»æ ‡é¢˜") or "").strip()
                if not title:
                    continue
                publish_raw = item.get("publish_time") or item.get("å‘å¸ƒæ—¶é—´")
                publish_dt = pd.to_datetime(publish_raw, errors="coerce")
                if publish_dt is not None and publish_dt.tzinfo is not None:
                    publish_dt = publish_dt.tz_convert("Asia/Shanghai").tz_localize(None)
                if publish_dt is not None and publish_dt < cutoff:
                    continue
                display_time = publish_dt.strftime("%Y-%m-%d %H:%M") if publish_dt is not None else str(publish_raw)
                recent_items.append({
                    "title": title[:120],
                    "publish_time": display_time
                })

            recent_items.sort(key=lambda item: item.get("publish_time", ""), reverse=True)
            truncated_items = recent_items[:5]
            self._prefetched_news[normalized] = {
                "news": truncated_items,
                "count": len(truncated_items)
            }

    def _prefetch_all_prices(self, current_time: str) -> None:
        print("ğŸ’¹ Prefetching hourly price snapshots...")
        for sym in self.stock_symbols:
            normalized = normalize_symbol(sym)
            if not normalized:
                continue
            try:
                payload_str = self.get_hourly_stock_data(normalized, current_time)
                payload = json.loads(payload_str)
            except Exception as e:
                print(f"âš ï¸ é¢„æŠ“å–ä»·æ ¼å¤±è´¥ï¼š{normalized} - {e}")
                continue
            if "summary" in payload:
                self._prefetched_prices[normalized] = payload

    def _prefetch_all_indicators(self, today_date: str, current_time: str) -> None:
        print("ğŸ“ˆ Prefetching technical indicators...")
        end_date = current_time.split(" ")[0]
        for sym in self.stock_symbols:
            normalized = normalize_symbol(sym)
            if not normalized:
                continue
            try:
                payload_str = self.get_technical_indicators(normalized, end_date, lookback_days=10)
                payload = json.loads(payload_str)
            except Exception as e:
                print(f"âš ï¸ é¢„æŠ“å–æŒ‡æ ‡å¤±è´¥ï¼š{normalized} - {e}")
                continue
            indicators = payload.get("indicators") if isinstance(payload, dict) else None
            if indicators:
                self._prefetched_indicators[normalized] = payload

    def _build_observation_summary(self) -> str:
        if not self.stock_symbols:
            return "  â€¢ (no whitelisted symbols configured)"

        lines: List[str] = []
        for sym in self.stock_symbols:
            normalized = normalize_symbol(sym)
            if not normalized:
                continue

            price_payload = self._prefetched_prices.get(normalized, {})
            price_summary = price_payload.get("summary") if isinstance(price_payload, dict) else None
            price_text = "Px: -"
            if price_summary:
                close = price_summary.get("close")
                change_pct = price_summary.get("change_pct")
                if close is not None:
                    price_text = f"Px Â¥{close:,.2f}"
                if change_pct is not None:
                    sign = "+" if change_pct >= 0 else ""
                    price_text += f" ({sign}{change_pct:.2f}%)"

            indicator_payload = self._prefetched_indicators.get(normalized, {})
            indicators = indicator_payload.get("indicators") if isinstance(indicator_payload, dict) else None
            indicator_parts: List[str] = []
            if indicators:
                if indicators.get("SMA_10") is not None:
                    indicator_parts.append(f"SMA10 {indicators['SMA_10']:.2f}")
                if indicators.get("RSI_10") is not None:
                    indicator_parts.append(f"RSI10 {indicators['RSI_10']:.1f}")
                macd_val = indicators.get("MACD_12_26_9")
                if macd_val is not None:
                    indicator_parts.append(f"MACD {macd_val:.2f}")
            indicator_text = "; ".join(indicator_parts) if indicator_parts else "Indicators: -"

            news_payload = self._prefetched_news.get(normalized, {})
            news_titles = [item.get("title", "") for item in news_payload.get("news", [])]
            news_titles = [title for title in news_titles if title]
            if news_titles:
                news_text = " | ".join(news_titles[:2])
            else:
                news_text = "no recent news (â‰¤3d)"

            line = f"  â€¢ {normalized}: {price_text}; {indicator_text}; News: {news_text}"
            lines.append(line)

        if not lines:
            return "  â€¢ (prefetch unavailable)"
        return "\n".join(lines)

    def _symbols_signature(self) -> str:
        normalized = sorted(sym for sym in self.allowed_symbols if sym)
        return "|".join(normalized)

    def _collect_prefetch_bundle(
        self,
        today_date: str,
        current_time: str,
        decision_count: int,
    ) -> Dict[str, Any]:
        """
        Run all prefetch helpers and return a serializable snapshot bundle
        for shared caching/logging.
        """
        self._prefetched_news.clear()
        self._prefetched_prices.clear()
        self._prefetched_indicators.clear()
        self._prefetch_all_news(today_date, current_time, max_retries=2)
        self._prefetch_all_prices(current_time)
        self._prefetch_all_indicators(today_date, current_time)

        snapshot_id = f"{today_date}_{current_time.replace(':', '-').replace(' ', '_')}"
        summary = self._build_observation_summary()
        now_iso = datetime.now(timezone.utc).isoformat()

        bundle: Dict[str, Any] = {
            "snapshot_id": snapshot_id,
            "schema_version": 1,
            "today_date": today_date,
            "decision_time": current_time,
            "decision_count": decision_count,
            "generated_at": now_iso,
            "source_agent": self.signature,
            "symbols": list(self.stock_symbols),
            "normalized_symbols": sorted(sym for sym in self.allowed_symbols if sym),
            "symbols_signature": self._symbols_signature(),
            "news": copy.deepcopy(self._prefetched_news),
            "prices": copy.deepcopy(self._prefetched_prices),
            "indicators": copy.deepcopy(self._prefetched_indicators),
            "observation_summary": summary,
            "prefetch_config": {
                "news_lookback_days": 3,
                "indicator_lookback_days": 10,
                "hourly_candles_expected": 40,
            },
        }
        self._last_prefetch_bundle = bundle
        return bundle

    def _apply_prefetch_bundle(self, bundle: Dict[str, Any]) -> str:
        """
        Load prefetched structures from a snapshot bundle back into the agent.
        Returns the observation summary text.
        """
        self._last_prefetch_bundle = bundle
        news_payload = bundle.get("news") or {}
        prices_payload = bundle.get("prices") or {}
        indicators_payload = bundle.get("indicators") or {}
        self._prefetched_news = copy.deepcopy(news_payload)
        self._prefetched_prices = copy.deepcopy(prices_payload)
        self._prefetched_indicators = copy.deepcopy(indicators_payload)
        summary = bundle.get("observation_summary")
        if not summary:
            summary = self._build_observation_summary()
        return summary

    def _purge_news_csv(self, csv_path: Optional[str]) -> None:
        if not csv_path or not os.path.exists(csv_path):
            return
        with NEWS_FILE_LOCK:
            df = None
            for encoding in ['utf-8', 'utf-8-sig', 'gbk', 'gb18030', 'latin1']:
                try:
                    df = pd.read_csv(csv_path, encoding=encoding)
                    print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å– {csv_path} ä»¥è¿›è¡Œæ¸…ç†")
                    break
                except Exception:
                    continue
            if df is None or df.empty:
                return
            df = self._sanitize_news_dataframe(df)
            filtered_df = self._filter_allowed_news_df(df)
            if filtered_df is None:
                return
            if len(filtered_df) != len(df):
                filtered_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                print(f"ğŸ§¹ å·²æ¸…ç† {csv_path} ä¸­çš„éç™½åå•æ–°é—»è®°å½•")
    
    # --- æœ¬åœ°å·¥å…·å‡½æ•°å®šä¹‰ (DM Functions) ---
    
    def get_current_stock_prices(self, symbols: List[str], target_time: str) -> str:
        """
        è·å–å¤šä¸ªè‚¡ç¥¨åœ¨æŒ‡å®šæ—¶é—´ç‚¹æˆ–ä¹‹å‰æœ€æ–°çš„æ”¶ç›˜ä»·æ ¼ã€‚
        
        Args:
            symbols (List[str]): è‚¡ç¥¨ä»£ç åˆ—è¡¨(ä¾‹å¦‚ ["MSFT", "AAPL"])ã€‚
            target_time (str): ç›®æ ‡æ—¶é—´(YYYY-MM-DD HH:MM:SS)ã€‚
        
        Returns:
            str: JSON å­—ç¬¦ä¸²æ ¼å¼çš„å­—å…¸,æ˜ å°„è‚¡ç¥¨ä»£ç åˆ°ä»·æ ¼(å¦‚æœæœªæ‰¾åˆ°åˆ™ä¸º null)ã€‚
        """
        if not self.dm:
            return json.dumps({"error": "DataManager æœªåˆå§‹åŒ–"})
        try:
            prices_dict = self.dm.get_prices_at(symbols=symbols, target_time=target_time)
            return json.dumps(prices_dict)
        except Exception as e:
            return json.dumps({"error": f"è·å–å½“å‰ä»·æ ¼æ—¶å‡ºé”™: {str(e)}"})
    
    def get_hourly_stock_data(self, symbol: str, end_time: str, lookback_hours: Optional[int] = 24) -> str:
        """
        è·å–å•ä¸ªè‚¡ç¥¨çš„å°æ—¶çº¿æ•°æ®ï¼ˆ60åˆ†é’Ÿ K çº¿ï¼‰ä»¥åŠç®€è¦æ‘˜è¦ã€‚
        æ‘˜è¦åŒ…å«ï¼šæœ€æ–°ä¸€æ ¹ K çº¿ã€ä¸Šä¸€æ ¹æ”¶ç›˜ä»·ã€æœ€è¿‘è‹¥å¹²æ”¶ç›˜ä»·ï¼›åŒæ—¶è¿”å›ä¸€å°æ®µæœ€æ–°çš„å°æ—¶ K çº¿åˆ—è¡¨ã€‚
        """
        if not self.dm:
            return json.dumps({"error": "DataManager æœªåˆå§‹åŒ–"})

        normalized_symbol = normalize_symbol(symbol)
        if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
            return json.dumps({
                "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„ç ”ç©¶åå•ä¸­",
                "allowed_symbols": self._allowed_symbol_list(),
                "symbol": normalized_symbol
            }, ensure_ascii=False)

        def _to_float(value: Any) -> Optional[float]:
            try:
                if value is None:
                    return None
                return float(value)
            except (TypeError, ValueError):
                return None

        try:
            window = lookback_hours or 24
            plain_symbol = strip_exchange_prefix(normalized_symbol) if normalized_symbol else symbol
            query_symbol = plain_symbol or symbol
            df = self.dm.get_hourly_stock_data(
                symbol=query_symbol,
                end_date=end_time, 
                lookback_hours=window
            )
            if df.empty:
                fallback_df = self._build_single_price_dataframe(normalized_symbol or symbol, end_time)
                if fallback_df is not None:
                    df = fallback_df
                else:
                    return json.dumps({"error": f"æœªæ‰¾åˆ° {normalized_symbol or symbol} çš„å°æ—¶çº¿æ•°æ®"})

            df = df.sort_index()
            latest = df.iloc[-1]
            latest_ts = latest.name
            latest_close = _to_float(latest.get("close"))
            prev_close = _to_float(df.iloc[-2]["close"]) if len(df) > 1 and "close" in df.columns else None

            recent_closes: List[Optional[float]] = []
            if "close" in df.columns:
                closes_series = df["close"].dropna().tail(min(len(df), self.RECENT_CLOSE_COUNT))
                recent_closes = [_to_float(val) for val in closes_series]

            summary: Dict[str, Any] = {
                "timestamp": str(latest_ts),
                "open": _to_float(latest.get("open", latest_close)),
                "high": _to_float(latest.get("high", latest_close)),
                "low": _to_float(latest.get("low", latest_close)),
                "close": latest_close,
                "volume": _to_float(latest.get("volume")),
                "previous_close": prev_close,
                "recent_closes": recent_closes,
            }
            if prev_close is not None and latest_close is not None and prev_close != 0:
                summary["change"] = round(latest_close - prev_close, 4)
                summary["change_pct"] = round(((latest_close - prev_close) / prev_close) * 100, 4)

            mini_count = min(len(df), self.MINI_CANDLE_COUNT)
            candles: List[Dict[str, Any]] = []
            candles_df = df.tail(mini_count).reset_index()
            for _, row in candles_df.iterrows():
                candles.append({
                    "timestamp": str(row.get("timestamp")),
                    "open": _to_float(row.get("open", row.get("close"))),
                    "high": _to_float(row.get("high", row.get("close"))),
                    "low": _to_float(row.get("low", row.get("close"))),
                    "close": _to_float(row.get("close")),
                    "volume": _to_float(row.get("volume"))
                })

            payload = {
                "symbol": symbol,
                "lookback_hours": window,
                "total_candles_available": int(len(df)),
                "summary": summary,
                "candles": candles
            }
            return json.dumps(payload, ensure_ascii=False)
        except Exception as e:
            return json.dumps({"error": f"è·å– {symbol} å°æ—¶çº¿æ•°æ®æ—¶å‡ºé”™: {str(e)}"})

    def _build_single_price_dataframe(self, symbol: str, timestamp: str) -> Optional[pd.DataFrame]:
        """
        æ„é€ ä»…å«ä¸€æ¡è®°å½•çš„å°æ—¶çº¿ DataFrameï¼Œä½¿ç”¨å‰ä¸€äº¤æ˜“æ—¥æ”¶ç›˜ä»·æˆ– DataManager å¯è·å¾—çš„ä»·æ ¼ã€‚
        """
        normalized_symbol = normalize_symbol(symbol) or symbol
        plain_symbol = strip_exchange_prefix(normalized_symbol) or normalized_symbol

        try:
            ts = pd.to_datetime(timestamp)
        except Exception:
            ts = pd.Timestamp(datetime.now())

        date_str = ts.strftime("%Y-%m-%d")
        fallback_price = self._get_previous_close(normalized_symbol, date_str)

        if (fallback_price is None) and self.dm:
            try:
                fallback_price = self.dm.get_price_at(plain_symbol, ts)
            except Exception:
                fallback_price = None

        if fallback_price is None:
            return None

        record = {
            "timestamp": ts,
            "symbol": plain_symbol,
            "open": float(fallback_price),
            "high": float(fallback_price),
            "low": float(fallback_price),
            "close": float(fallback_price),
            "volume": 0.0,
        }
        df = pd.DataFrame([record])
        df.set_index("timestamp", inplace=True)
        return df
    
    def get_technical_indicators(self, symbol: str, end_date: str, lookback_days: int = 10) -> str:
        """
        è·å–æŠ€æœ¯æŒ‡æ ‡ï¼šè¯»å–å†å²å°æ—¶çº¿æŒ‡æ ‡ + è®¡ç®—å®æ—¶å°æ—¶çº¿æŒ‡æ ‡ + ä¿å­˜æ›´æ–°åˆ° ai_stock_data.json
        
        Args:
            symbol (str): è‚¡ç¥¨ä»£ç 
            end_date (str): ç»“æŸæ—¥æœŸ (YYYY-MM-DD) æˆ–ç»“æŸæ—¶é—´ (YYYY-MM-DD HH:MM:SS)
            lookback_days (int): ç”¨äºè®¡ç®—æŒ‡æ ‡çš„å†å²æ•°æ®å¤©æ•°ï¼Œé»˜è®¤30å¤©ï¼ˆè½¬æ¢ä¸ºå°æ—¶æ•°ï¼‰
            
        Returns:
            str: JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å†å²å°æ—¶çº¿æŒ‡æ ‡å’Œå®æ—¶å°æ—¶çº¿æŒ‡æ ‡
        """
        if not self.dm:
            return json.dumps({"error": "DataManager æœªåˆå§‹åŒ–"})
        
        normalized_symbol = normalize_symbol(symbol)
        if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
            return json.dumps({
                "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„ç ”ç©¶åå•ä¸­",
                "allowed_symbols": self._allowed_symbol_list(),
                "symbol": normalized_symbol
            }, ensure_ascii=False)
        
        try:
            # 1. è¯»å–å†å²å°æ—¶çº¿æŒ‡æ ‡ä¸è¡Œæƒ…ï¼ˆä» ai_stock_data.jsonï¼Œä½¿ç”¨JsonFileManagerï¼‰
            historical_indicators: List[Dict[str, Any]] = []
            historical_price_data: List[Dict[str, Any]] = []
            ai_stock_data_path = self.stock_json_path or os.path.join(project_root, "data", "ai_stock_data.json")
            from tools.json_file_manager import safe_read_json
            
            all_data = safe_read_json(ai_stock_data_path, default={})
            stock_entry: Optional[Dict[str, Any]] = None
            for key in [symbol, f"SH{symbol}", f"SZ{symbol}"]:
                entry = all_data.get(key)
                if entry:
                    stock_entry = entry
                    break
            
            def _reload_stock_entry() -> Optional[Dict[str, Any]]:
                refreshed = safe_read_json(ai_stock_data_path, default={})
                for candidate in [symbol, f"SH{symbol}", f"SZ{symbol}"]:
                    entry = refreshed.get(candidate)
                    if entry:
                        return entry
                return None

            if stock_entry:
                if "å°æ—¶çº¿æŒ‡æ ‡" in stock_entry:
                    historical_indicators = stock_entry["å°æ—¶çº¿æŒ‡æ ‡"] or []
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_indicators)} æ¡å†å²å°æ—¶çº¿æŠ€æœ¯æŒ‡æ ‡ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
                if "å°æ—¶çº¿è¡Œæƒ…" in stock_entry:
                    historical_price_data = stock_entry["å°æ—¶çº¿è¡Œæƒ…"] or []
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_price_data)} æ¡å†å²å°æ—¶çº¿è¡Œæƒ…ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
            else:
                print(f"âš ï¸ æœªåœ¨ {ai_stock_data_path} æ‰¾åˆ° {symbol} çš„å†å²è®°å½•ï¼Œå°è¯•ä» TinySoft å›å¡«ã€‚")

            max_expected_candles = lookback_days * 4
            if not historical_price_data or len(historical_price_data) < max_expected_candles:
                try:
                    ndays = max(lookback_days, 10)
                    self.dm.save_ts_data(symbols=[symbol], ndays=ndays, out_path=ai_stock_data_path)
                    stock_entry = _reload_stock_entry()
                    if stock_entry and "å°æ—¶çº¿è¡Œæƒ…" in stock_entry:
                        historical_price_data = stock_entry["å°æ—¶çº¿è¡Œæƒ…"] or []
                        print(f"ğŸ“¡ é€šè¿‡ TinySoft å›å¡« {len(historical_price_data)} æ¡å†å²å°æ—¶çº¿è¡Œæƒ…ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
                except Exception as e:
                    print(f"âš ï¸ å›å¡«å°æ—¶çº¿è¡Œæƒ…å¤±è´¥ï¼š{e}")

            if not historical_indicators and stock_entry and "å°æ—¶çº¿æŒ‡æ ‡" in stock_entry:
                historical_indicators = stock_entry["å°æ—¶çº¿æŒ‡æ ‡"] or []
                if historical_indicators:
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_indicators)} æ¡å†å²å°æ—¶çº¿æŠ€æœ¯æŒ‡æ ‡ï¼ˆè‚¡ç¥¨ï¼š{symbol}ï¼‰")
            
            # 2. ç»„è£…æŒ‡æ ‡å­—æ®µç™½åå•
            indicator_keys = [
                'SMA_10',
                'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9',
                'RSI_10',
                'BBL_10_2.0', 'BBM_10_2.0', 'BBU_10_2.0'
            ]

            def build_indicator_payload(source_label: str, indicator_dict: Dict[str, Any], timestamp_hint: Optional[Any] = None, include_saved_path: bool = False) -> str:
                summary = {key: indicator_dict.get(key) for key in indicator_keys}
                payload: Dict[str, Any] = {
                    "source": source_label,
                    "symbol": symbol,
                    "timestamp": str(timestamp_hint) if timestamp_hint else str(end_date),
                    "indicators": summary
                }
                if include_saved_path:
                    payload["saved_to"] = ai_stock_data_path
                return json.dumps(payload, ensure_ascii=False)
            
            # 3. è·å–å®æ—¶å°æ—¶çº¿æ•°æ®ï¼ˆè½¬æ¢ä¸ºå°æ—¶æ•°ï¼šæ¯å¤©4å°æ—¶äº¤æ˜“æ—¶é—´ï¼Œä¹˜ä»¥å¤©æ•°ï¼‰
            lookback_hours = lookback_days * 4  # æ¯å¤©4å°æ—¶äº¤æ˜“æ—¶é—´ï¼ˆ9:30-11:30, 13:00-15:00ï¼‰
            # å¦‚æœ end_date åªåŒ…å«æ—¥æœŸï¼Œæ·»åŠ å½“å‰æ—¶é—´
            if len(end_date) == 10:  # YYYY-MM-DD
                end_time = f"{end_date} 15:00:00"  # ä½¿ç”¨æ”¶ç›˜æ—¶é—´
            else:
                end_time = end_date
            
            df_realtime = self.dm.get_hourly_stock_data(
                symbol=symbol, 
                end_date=end_time, 
                lookback_hours=lookback_hours
            )
            
            # 4. åˆå¹¶å†å²+å®æ—¶æ•°æ®
            df = pd.DataFrame()  # æœ€ç»ˆåˆå¹¶åçš„DataFrame
            
            # å¦‚æœæœ‰å†å²ä»·æ ¼æ•°æ®ï¼Œå…ˆè½¬æ¢ä¸ºDataFrame
            if historical_price_data:
                if len(historical_price_data) > max_expected_candles:
                    historical_price_data = historical_price_data[-max_expected_candles:]
                try:
                    df_historical = pd.DataFrame(historical_price_data)
                    # è½¬æ¢æ—¥æœŸåˆ—ä¸ºdatetimeï¼ˆnaiveï¼Œæ— æ—¶åŒºï¼‰
                    df_historical['timestamp'] = pd.to_datetime(df_historical['date'], utc=False)
                    # å¦‚æœè½¬æ¢åæœ‰æ—¶åŒºï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯ï¼ˆè½¬ä¸ºnaiveï¼‰
                    if df_historical['timestamp'].dt.tz is not None:
                        df_historical['timestamp'] = df_historical['timestamp'].dt.tz_localize(None)
                    # é‡å‘½ååˆ—ä»¥åŒ¹é…DataFrameæ ¼å¼
                    df_historical = df_historical.rename(columns={
                        'close': 'close',
                        'vol': 'volume'
                    })
                    # åªä¿ç•™éœ€è¦çš„åˆ—
                    if 'close' in df_historical.columns:
                        df_historical = df_historical[['timestamp', 'close', 'volume']]
                        df_historical.set_index('timestamp', inplace=True)
                        df = df_historical.copy()
                        print(f"ğŸ“š å·²åŠ è½½ {len(df)} æ¡å†å²å°æ—¶çº¿ä»·æ ¼æ•°æ®ï¼ˆnaive datetimeï¼‰")
                except Exception as e:
                    print(f"âš ï¸ è½¬æ¢å†å²ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            
            # åˆå¹¶å®æ—¶æ•°æ®ï¼ˆç¡®ä¿æ—¶åŒºä¸€è‡´ï¼‰
            if not df_realtime.empty:
                # åªä½¿ç”¨closeå’Œvolumeåˆ—
                df_realtime_subset = df_realtime[['close', 'volume']].copy()
                # ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼ˆè½¬ä¸ºnaive datetimeï¼Œé¿å…æ—¶åŒºæ¯”è¾ƒé”™è¯¯ï¼‰
                if df_realtime_subset.index.tz is not None:
                    df_realtime_subset.index = df_realtime_subset.index.tz_localize(None)
                # åˆå¹¶ï¼ˆå»é‡ï¼Œä¿ç•™æœ€æ–°çš„ï¼‰
                if not df.empty:
                    # åˆå¹¶ï¼Œå®æ—¶æ•°æ®è¦†ç›–å†å²æ•°æ®ä¸­çš„ç›¸åŒæ—¶é—´æˆ³
                    df = pd.concat([df, df_realtime_subset])
                    df = df[~df.index.duplicated(keep='last')]  # ä¿ç•™æœ€æ–°çš„
                    df = df.sort_index()
                else:
                    df = df_realtime_subset
                print(f"ğŸ“¡ å·²åˆå¹¶ {len(df_realtime)} æ¡å®æ—¶å°æ—¶çº¿ä»·æ ¼æ•°æ®ï¼Œæ€»è®¡ {len(df)} æ¡ï¼ˆå·²ç»Ÿä¸€ä¸ºnaive datetimeï¼‰")
            
            # é™åˆ¶çª—å£å¤§å°
            if not df.empty and len(df) > max_expected_candles:
                df = df.tail(max_expected_candles)
            
            # æ£€æŸ¥åˆå¹¶åçš„æ•°æ®é‡ï¼ˆå°æ—¶çº¿è‡³å°‘éœ€è¦10ä¸ªäº¤æ˜“æ—¥ * 4å°æ—¶ = 40æ¡ï¼‰
            min_hours_needed = max_expected_candles
            if df.empty:
                # å¦‚æœåˆå¹¶åä»ä¸ºç©ºï¼Œè¿”å›å†å²æŒ‡æ ‡
                if historical_indicators:
                    print(f"âš ï¸ æ— ä»·æ ¼æ•°æ®ï¼Œè¿”å›å†å²æŒ‡æ ‡")
                    latest_hist = historical_indicators[-1] if historical_indicators else {}
                    ts_hint = latest_hist.get("timestamp") or latest_hist.get("date")
                    return build_indicator_payload("historical_only", latest_hist, ts_hint)
                else:
                    return json.dumps({"error": f"æœªæ‰¾åˆ° {symbol} çš„è‚¡ç¥¨æ•°æ®"})
            
            # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            if len(df) < min_hours_needed:
                if historical_indicators:
                    print(f"âš ï¸ åˆå¹¶åæ•°æ®ä¸è¶³ï¼ˆ{len(df)}æ¡ < {min_hours_needed}æ¡ï¼‰ï¼Œè¿”å›å†å²æŒ‡æ ‡")
                    latest_hist = historical_indicators[-1] if historical_indicators else {}
                    ts_hint = latest_hist.get("timestamp") or latest_hist.get("date")
                    return build_indicator_payload("historical_only", latest_hist, ts_hint)
                else:
                    return json.dumps({
                        "error": f"æ•°æ®é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘{min_hours_needed}æ¡å°æ—¶çº¿æ•°æ®ï¼Œåˆå¹¶ååªæœ‰{len(df)}æ¡"
                    }, ensure_ascii=False)
            
            # 5. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆä½¿ç”¨åˆå¹¶åçš„å®Œæ•´å°æ—¶çº¿æ•°æ®ï¼‰
            try:
                # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
                df = df.sort_index()
                
                # ç¡®ä¿DataFrameæœ‰.taå±æ€§ï¼ˆpandas_taéœ€è¦æ­£ç¡®åˆå§‹åŒ–ï¼‰
                if not hasattr(df, 'ta'):
                    # å¦‚æœ.taå±æ€§ä¸å­˜åœ¨ï¼Œå°è¯•é‡æ–°å¯¼å…¥å¹¶æ³¨å†Œ
                    try:
                        import pandas_ta as ta
                        # pandas_taé€šè¿‡monkey patchçš„æ–¹å¼æ·»åŠ åˆ°DataFrameï¼Œç¡®ä¿å·²åŠ è½½
                        if not hasattr(pd.DataFrame, 'ta'):
                            import pandas_ta.core as pta
                    except Exception as e:
                        print(f"âš ï¸ pandas_taåˆå§‹åŒ–å¤±è´¥: {e}")
                        raise Exception(f"pandas_taä¸å¯ç”¨: {e}")
                
                # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—ç”¨äºè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                if 'close' not in df.columns:
                    raise Exception("DataFrameç¼ºå°‘'close'åˆ—")
                
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆåŸºäºå°æ—¶çº¿æ•°æ®ï¼‰
                # ä½¿ç”¨10å¤©å‚æ•°è®¡ç®—æŒ‡æ ‡
                df.ta.sma(length=10, append=True)
                df.ta.macd(append=True)
                df.ta.rsi(length=10, append=True)
                df.ta.bbands(length=10, append=True)
                print(f"ğŸ“Š åŸºäºåˆå¹¶å°æ—¶çº¿æ•°æ®ï¼ˆå†å²+å®æ—¶ï¼Œå…±{len(df)}æ¡ï¼‰è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ10å¤©ï¼‰")
            except Exception as e:
                print(f"âš ï¸ è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
                if historical_indicators:
                    latest_hist = historical_indicators[-1] if historical_indicators else {}
                    ts_hint = latest_hist.get("timestamp") or latest_hist.get("date")
                    return build_indicator_payload("historical_only", latest_hist, ts_hint)
                else:
                    return json.dumps({"error": f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {str(e)}"})
            
            # 6. æå–æœ€æ–°çš„æŒ‡æ ‡
            latest_row = df.iloc[-1]
            latest_indicators: Dict[str, Any] = {}
            for key in indicator_keys:
                value = latest_row.get(key) if hasattr(latest_row, "get") else latest_row[key] if key in latest_row else None
                latest_indicators[key] = value
            
            # æ¸…ç† NaN å€¼
            for key, value in latest_indicators.items():
                if pd.isna(value):
                    latest_indicators[key] = None

            print(f"ğŸ“Š å·²è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆåŸºäºåˆå¹¶å°æ—¶çº¿æ•°æ®ï¼šå†å²+å®æ—¶ï¼Œå…±{len(df)}æ¡ï¼‰")
            
            # 7. ä¿å­˜åˆ° ai_stock_data.jsonï¼ˆä½¿ç”¨ DataManager çš„ save_ts_dataï¼Œè‡ªåŠ¨åˆå¹¶å†å²ï¼‰
            try:
                self.dm.save_ts_data(symbols=[symbol], ndays=60, out_path=ai_stock_data_path)
                print(f"ğŸ’¾ å·²æ›´æ–°å°æ—¶çº¿æŠ€æœ¯æŒ‡æ ‡åˆ° {ai_stock_data_path}ï¼ˆåˆå¹¶å†å²æ•°æ®ï¼‰")
                
                # é‡æ–°è¯»å–åˆå¹¶åçš„å®Œæ•´æŒ‡æ ‡æ•°æ®ï¼ˆä½¿ç”¨JsonFileManagerï¼‰
                all_data_updated = safe_read_json(ai_stock_data_path, default={})
                for key in [symbol, f"SH{symbol}", f"SZ{symbol}"]:
                    if key in all_data_updated and "å°æ—¶çº¿æŒ‡æ ‡" in all_data_updated[key]:
                        # ä½¿ç”¨å®Œæ•´çš„åˆå¹¶åæ•°æ®
                        historical_indicators = all_data_updated[key]["å°æ—¶çº¿æŒ‡æ ‡"]
                        print(f"âœ… åˆå¹¶åæŒ‡æ ‡ï¼š{len(historical_indicators)} æ¡")
                        break
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            
            # 8. è¿”å›ç»„åˆç»“æœï¼ˆä»…æœ€æ–°æŒ‡æ ‡ï¼‰
            indicator_timestamp = None
            if not df.empty:
                indicator_timestamp = df.index[-1]

            return build_indicator_payload(
                "combined_merged",
                latest_indicators,
                indicator_timestamp,
                include_saved_path=True
            )
        except Exception as e:
            return json.dumps({"error": f"è·å–æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}"})

    def get_latest_position_tool(self, today_date: str) -> str:
        """ä»æŒä»“æ–‡ä»¶ä¸­è·å–ä»£ç†çš„æœ€æ–°äº¤æ˜“æŒä»“ã€‚æ­¤å‡½æ•°è¯»å–ä»£ç†çš„æŒä»“æ–‡ä»¶ä»¥æ‰¾åˆ°å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æŒä»“ã€‚"""
        try:
            positions, _, _ = get_latest_position(today_date, self.signature)
            return json.dumps(positions)
        except Exception as e:
            return json.dumps({"error": f"è·å–æœ€æ–°æŒä»“æ—¶å‡ºé”™: {str(e)}"})
    
    def add_no_trade_record_tool(self, today_date: str) -> str:
        """ä¸ºå½“å‰ä»£ç†åœ¨ç»™å®šæ—¥æœŸè®°å½•"æ— äº¤æ˜“"æ“ä½œã€‚æ­¤å‡½æ•°æ›´æ–°ä»£ç†çš„æŒä»“æ–‡ä»¶ä»¥å»¶ç»­å‰ä¸€å¤©çš„æŒä»“ã€‚"""
        try:
            # ä¼˜å…ˆä½¿ç”¨å®ä¾‹ä¸Šä¸‹æ–‡ï¼Œé¿å…å¹¶å‘çŠ¶æ€æ±¡æŸ“
            decision_time = self._get_context_value("CURRENT_TIME") or f"{today_date} 00:00:00"
            decision_count_raw = self._get_context_value("DECISION_COUNT")
            decision_count = int(decision_count_raw) if decision_count_raw is not None else 0
            add_no_trade_record(today_date, decision_time, decision_count, self.signature)
            return json.dumps({
                "success": True,
                "action": "no_trade",
                "date": today_date,
                "decision_time": decision_time,
                "agent": self.signature
            })
        except Exception as e:
            return json.dumps({"error": f"æ·»åŠ æ— äº¤æ˜“è®°å½•æ—¶å‡ºé”™: {str(e)}"})
    
    def buy_stock(self, symbol: str, amount: int) -> str:
        """
        ä¹°å…¥è‚¡ç¥¨ï¼ˆä½¿ç”¨å½“å‰å°æ—¶çº§ä»·æ ¼ï¼‰ã€‚
        
        æ­¤å‡½æ•°æ¨¡æ‹Ÿè‚¡ç¥¨ä¹°å…¥æ“ä½œï¼ŒåŒ…æ‹¬ï¼š
        1. è·å–å½“å‰æŒä»“å’Œæ“ä½œID
        2. è·å–å½“å‰å°æ—¶çš„è‚¡ç¥¨ä»·æ ¼ï¼ˆä¼˜å…ˆå°æ—¶çº§ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·ï¼‰
        3. éªŒè¯ä¹°å…¥æ¡ä»¶ï¼ˆç°é‡‘æ˜¯å¦å……è¶³ï¼‰
        4. æ›´æ–°æŒä»“ï¼ˆå¢åŠ è‚¡ç¥¨æ•°é‡ï¼Œå‡å°‘ç°é‡‘ï¼‰
        5. è®°å½•äº¤æ˜“åˆ° position.jsonl æ–‡ä»¶
        
        Args:
            symbol (str): è‚¡ç¥¨ä»£ç ï¼Œå¦‚ "600519"
            amount (int): ä¹°å…¥æ•°é‡ï¼Œå¿…é¡»æ˜¯100çš„æ•´æ•°å€
        
        Returns:
            str: JSON å­—ç¬¦ä¸²ï¼ŒæˆåŠŸæ—¶è¿”å›æ–°æŒä»“ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
        """
        try:
            normalized_symbol = normalize_symbol(symbol)
            if not normalized_symbol:
                return json.dumps({"error": "æ— æ•ˆçš„è‚¡ç¥¨ä»£ç "})
            data_symbol = strip_exchange_prefix(normalized_symbol) or normalized_symbol

            if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
                return json.dumps({
                    "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„æŒä»“/äº¤æ˜“åå•ä¸­",
                    "allowed_symbols": self._allowed_symbol_list(),
                    "symbol": normalized_symbol
                }, ensure_ascii=False)

            if not self._is_allowed_symbol(normalized_symbol):
                return json.dumps({
                    "error": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„äº¤æ˜“åå•ä¸­",
                    "allowed_symbols": self._allowed_symbol_list(),
                    "symbol": normalized_symbol
                }, ensure_ascii=False)
            
            # ä½¿ç”¨å®ä¾‹ä¸Šä¸‹æ–‡ï¼Œé¿å…ä»å…±äº« runtime_env.json è¯»å–
            today_date = self._get_context_value("TODAY_DATE")
            current_time = self._get_context_value("CURRENT_TIME")
            decision_time = current_time or f"{today_date} 00:00:00"
            decision_count_raw = self._get_context_value("DECISION_COUNT")
            decision_count = int(decision_count_raw) if decision_count_raw is not None else 0
            if not today_date:
                return json.dumps({"error": "æœªè®¾ç½® TODAY_DATE"})
            decision_time = normalize_decision_time(today_date, decision_time)
            
            # äº¤æ˜“å•ä½æ£€æŸ¥ (100è‚¡çš„æ•´æ•°å€)
            if amount <= 0 or amount % 100 != 0:
                return json.dumps({
                    "error": "ä¹°å…¥æ•°é‡å¿…é¡»æ˜¯100çš„æ•´æ•°å€ä¸”å¤§äº0",
                    "symbol": normalized_symbol,
                    "amount": amount
                })

            # è·å–å½“å‰æŒä»“å’Œæ“ä½œID
            current_position, current_action_id, latest_record = get_latest_position(today_date, self.signature)
            
            # è·å–å½“å‰æ—¶åˆ»çš„è‚¡ç¥¨ä»·æ ¼ - ä¼˜å…ˆå°æ—¶çº§æ•°æ®
            try:
                this_symbol_price = None
                # ä¼˜å…ˆå°è¯•è·å–å°æ—¶çº§ä»·æ ¼
                if current_time:
                    hourly_data = self.dm.get_hourly_stock_data(symbol=data_symbol, end_date=current_time, lookback_hours=1)
                    if not hourly_data.empty:
                        this_symbol_price = float(hourly_data['close'].iloc[-1])
                        print(f"ğŸ’¹ ä½¿ç”¨å°æ—¶çº§ä»·æ ¼: {normalized_symbol} = Â¥{this_symbol_price} ({current_time})")
                
                # å¦‚æœæ²¡æœ‰å°æ—¶çº§æ•°æ®ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·
                if this_symbol_price is None or pd.isna(this_symbol_price):
                    stock_data = self.dm.get_stock_data(symbol=data_symbol, end_date=today_date, lookback_days=1)
                    if stock_data.empty:
                        return json.dumps({"error": f"æœªæ‰¾åˆ°è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®", "symbol": normalized_symbol, "date": today_date})
                    this_symbol_price = float(stock_data['open'].iloc[-1]) if 'open' in stock_data.columns else float(stock_data['close'].iloc[-1])
                    print(f"ğŸ’¹ ä½¿ç”¨å¼€ç›˜ä»·: {normalized_symbol} = Â¥{this_symbol_price}")
                
                if pd.isna(this_symbol_price) or this_symbol_price <= 0:
                    return json.dumps({"error": f"è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®æ— æ•ˆ", "symbol": normalized_symbol, "date": today_date, "price": this_symbol_price})
            except Exception as e:
                return json.dumps({"error": f"è·å–è‚¡ç¥¨ä»·æ ¼å¤±è´¥: {str(e)}", "symbol": normalized_symbol, "date": today_date})
            
            limit_info: Optional[Dict[str, float]] = None
            prev_close = self._get_previous_close(normalized_symbol, today_date)
            limit_info = get_price_limits(normalized_symbol, prev_close)
            allowed, reason = self._passes_price_limit_liquidity("sell", this_symbol_price, limit_info)
            if not allowed:
                return json.dumps({
                    "error": reason,
                    "symbol": normalized_symbol,
                    "price": this_symbol_price,
                    "limit_info": limit_info
                }, ensure_ascii=False)
            
            # --- é£é™©ç®¡ç†æ£€æŸ¥ ---
            single_stock_max_position = self.risk_management.get("single_stock_max_position", 0.20)
            total_assets = current_position.get("CASH", 0)
            for stock, data in current_position.items():
                if stock != "CASH":
                    # å‡è®¾æˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»·æ ¼æ¥ä¼°ç®—å½“å‰è‚¡ç¥¨ä»·å€¼ï¼Œè¿™é‡Œç”¨ä»Šå¤©çš„å¼€ç›˜ä»·
                    # åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„ä»·æ ¼è·å–é€»è¾‘
                    stock_value = data.get("shares", 0) * this_symbol_price # ä¼°ç®—
                    total_assets += stock_value

            required_cash = this_symbol_price * amount
            if (required_cash / total_assets) > single_stock_max_position:
                 return json.dumps({
                    "error": f"å•åªè‚¡ç¥¨æŒä»“è¶…è¿‡ä¸Šé™ ({single_stock_max_position * 100}%)",
                    "symbol": normalized_symbol,
                    "max_allowed_investment": total_assets * single_stock_max_position,
                    "requested_investment": required_cash
                })

            # --- äº¤æ˜“æˆæœ¬è®¡ç®— ---
            commission_rate = self.trading_rules.get("commission_rate", 0.0003)
            min_commission = self.trading_rules.get("min_commission", 5.0)
            commission = max(required_cash * commission_rate, min_commission)
            total_cost = required_cash + commission

            # éªŒè¯ä¹°å…¥æ¡ä»¶
            cash_left = current_position.get("CASH", 0) - total_cost
            
            if cash_left < 0:
                return json.dumps({
                    "error": "ç°é‡‘ä¸è¶³ï¼ˆå·²è€ƒè™‘äº¤æ˜“è´¹ç”¨ï¼‰ï¼äº¤æ˜“ä¸è¢«å…è®¸ã€‚",
                    "total_cost": total_cost,
                    "cash_available": current_position.get("CASH", 0),
                    "symbol": normalized_symbol,
                    "date": today_date
                })
            
            limit_info: Optional[Dict[str, float]] = None
            prev_close = self._get_previous_close(normalized_symbol, today_date)
            limit_info = get_price_limits(normalized_symbol, prev_close)
            allowed, reason = self._passes_price_limit_liquidity("buy", this_symbol_price, limit_info)
            if not allowed:
                return json.dumps({
                    "error": reason,
                    "symbol": normalized_symbol,
                    "price": this_symbol_price,
                    "limit_info": limit_info
                }, ensure_ascii=False)
            
            # æ‰§è¡Œä¹°å…¥æ“ä½œ
            new_position = copy.deepcopy(current_position)
            new_position["CASH"] = cash_left
            
            # æ›´æ–°æŒä»“ï¼ˆç»´æŠ¤åŠ æƒå¹³å‡æˆæœ¬ï¼‰
            if normalized_symbol in new_position:
                existing_entry = new_position[normalized_symbol]
                current_shares = existing_entry.get("shares", 0)
                existing_avg = existing_entry.get("avg_price")
                if existing_avg is None:
                    existing_avg = this_symbol_price
                total_shares = current_shares + amount
                if total_shares > 0:
                    weighted_avg = ((existing_avg * current_shares) + (this_symbol_price * amount)) / total_shares
                else:
                    weighted_avg = this_symbol_price
                existing_entry["shares"] = total_shares
                existing_entry["avg_price"] = weighted_avg
            else:
                new_position[normalized_symbol] = {
                    "shares": amount,
                    "purchase_date": today_date,
                    "avg_price": this_symbol_price
                }

            new_position = normalize_positions(new_position)

            # è®°å½•äº¤æ˜“
            record: Dict[str, Any] = {
                "date": today_date,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "this_action": {"action": "buy", "symbol": normalized_symbol, "amount": amount},
                "positions": new_position
            }
            if latest_record and latest_record.get("decision_time") == decision_time:
                record["id"] = latest_record.get("id")
            else:
                record["id"] = current_action_id + 1
            upsert_position_record(self.signature, record)
            
            write_config_value("IF_TRADE", True)
            return json.dumps({
                "success": True,
                "action": "buy",
                "symbol": normalized_symbol,
                "amount": amount,
                "price": this_symbol_price,
                "cost": required_cash,
                "commission": commission,
                "price_limit": limit_info,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "new_position": new_position
            })
        
        except Exception as e:
            return json.dumps({"error": f"ä¹°å…¥è‚¡ç¥¨æ—¶å‡ºé”™: {str(e)}"})
    
    def search_stock_news(self, query: str, max_retries: int = 3) -> str:
        """
        æœç´¢è‚¡ç¥¨ç›¸å…³çš„å®æ—¶æ–°é—» + è¯»å–å†å²æ–°é—»ï¼Œä½¿ç”¨ AKShareï¼Œå¤±è´¥é‡è¯•ã€‚
        åŒæ—¶ä¼šä» DataManager è¯»å–å†å²æ–°é—»ï¼Œå¹¶å°†æ–°é—»ä¿å­˜åˆ° news.csv
        
        Args:
            query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "600519 æœ€æ–°æ¶ˆæ¯"ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
        
        Returns:
            str: JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å†å²æ–°é—»å’Œå®æ—¶æ–°é—»
        """
        # æå– 6 ä½ä»£ç 
        symbol = None
        if query.isdigit() and len(query) == 6:
            symbol = query
        else:
            import re
            m = re.search(r'\b(\d{6})\b', query)
            if m:
                symbol = m.group(1)
        if not symbol:
            return json.dumps({'success': False, 'message': 'è¯·æä¾›6ä½Aè‚¡ä»£ç ï¼Œä¾‹å¦‚ 600519'}, ensure_ascii=False)

        normalized_symbol = normalize_symbol(symbol)
        symbol_for_query = strip_exchange_prefix(normalized_symbol) or symbol

        if not self._is_allowed_symbol(normalized_symbol, allow_sell_existing=True):
            return json.dumps({
                "success": False,
                "message": "è¯¥è‚¡ç¥¨ä¸åœ¨å…è®¸çš„ç ”ç©¶åå•ä¸­ï¼Œè¯·èšç„¦é¢„è®¾çš„ç§‘åˆ›æ¿åˆ—è¡¨ã€‚",
                "allowed_symbols": self._allowed_symbol_list()
            }, ensure_ascii=False)

        cached_news = self._prefetched_news.get(normalized_symbol)
        if cached_news:
            return json.dumps(cached_news, ensure_ascii=False)

        today_date = get_config_value("TODAY_DATE")
        current_time = get_config_value("CURRENT_TIME")
        if current_time:
            search_time = current_time
        elif today_date:
            search_time = f"{today_date} 00:00:00"
        else:
            search_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # å¢é‡è¿‡æ»¤é€»è¾‘å·²ç§»é™¤ï¼Œæ”¹å›è¿”å›å®Œæ•´å†å²+å®æ—¶æ–°é—»

        # 1. è¯»å–å†å²æ–°é—»ï¼ˆä» DataManagerï¼‰
        historical_news = []
        if self.dm and self.dm.news_df is not None:
            try:
                news_df = self.dm.get_news(end_date=today_date, symbols=[symbol_for_query], limit=20)
                news_df = self._filter_allowed_news_df(news_df)
                if news_df is not None and not news_df.empty:
                    # å°†æ‰€æœ‰åˆ—è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… Timestamp åºåˆ—åŒ–é—®é¢˜
                    cleaned_news_df = self._sanitize_news_dataframe(news_df.astype(str))
                    historical_news = self._filter_allowed_news_records(cleaned_news_df.to_dict('records'))
                    print(f"ğŸ“š è¯»å–åˆ° {len(historical_news)} æ¡å†å²æ–°é—»ï¼ˆè‚¡ç¥¨ï¼š{normalized_symbol or symbol_for_query}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ è¯»å–å†å²æ–°é—»å¤±è´¥: {e}")

        # 2. è·å–å®æ—¶æ–°é—»ï¼ˆAKShareï¼Œå¸¦é‡è¯•ï¼‰
        realtime_results = []
        csv_path = os.path.join('data', 'news.csv')
        
        # åˆ›å»ºè°ƒè¯•æ–‡ä»¶å¤¹ç”¨äºä¿å­˜é”™è¯¯ä¿¡æ¯
        debug_dir = os.path.join('data', 'debug', 'akshare_errors')
        os.makedirs(debug_dir, exist_ok=True)
        
        for attempt in range(1, max_retries + 1):
            # åˆå§‹åŒ–è¯·æ±‚ä¿¡æ¯ï¼ˆåœ¨æ‰€æœ‰ä»£ç è·¯å¾„ä¸­éƒ½éœ€è¦ï¼‰
            request_info = {
                'symbol': symbol_for_query,
                'function': 'stock_news_em',
                'parameters': {'symbol': symbol_for_query}
            }
            
            try:
                import akshare as ak
                import time
                import traceback
                
                # é‡è¯•å‰æ·»åŠ å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
                # ç¬¬ä¸€æ¬¡è¯·æ±‚ä¹Ÿå»¶è¿Ÿ1ç§’ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if attempt == 1:
                    wait_time_before = 1  # ç¬¬ä¸€æ¬¡å»¶è¿Ÿ1ç§’
                    print(f"â³ è¯·æ±‚å‰ç­‰å¾… {wait_time_before} ç§’ï¼ˆé¿å…é¢‘ç‡é™åˆ¶ï¼‰...")
                    time.sleep(wait_time_before)
                else:
                    wait_time_before = (attempt - 1) * 5  # é€’å¢å»¶è¿Ÿï¼š5ç§’ã€10ç§’
                    print(f"â³ è¯·æ±‚å‰ç­‰å¾… {wait_time_before} ç§’ï¼ˆé¿å…é¢‘ç‡é™åˆ¶ï¼‰...")
                    time.sleep(wait_time_before)
                
                print(f"ğŸ“¡ å°è¯•è·å–å®æ—¶æ–°é—»ï¼ˆå°è¯• {attempt}/{max_retries}ï¼Œè‚¡ç¥¨ä»£ç ï¼š{symbol_for_query}ï¼‰...")
                
                # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆåº”è¯¥æ˜¯6ä½æ•°å­—ï¼‰
                if not symbol_for_query.isdigit() or len(symbol_for_query) != 6:
                    print(f"âš ï¸ è‚¡ç¥¨ä»£ç æ ¼å¼é”™è¯¯: {symbol_for_query}ï¼Œåº”è¯¥æ˜¯6ä½æ•°å­—")
                    break
                
                # å°è¯•æ‹¦æˆª HTTP å“åº”
                capture_context, request_info_captured = self._capture_akshare_response(symbol_for_query)
                
                # è°ƒç”¨ AKShare APIï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ•è·å“åº”ï¼‰
                # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½ä¼šåœ¨ä¸Šä¸‹æ–‡ä¸­æ›´æ–° request_info_captured
                try:
                    with capture_context:
                        news_df = ak.stock_news_em(symbol=symbol_for_query)
                    # åˆå¹¶æ•è·çš„è¯·æ±‚ä¿¡æ¯ï¼ˆrequest_info_captured æ˜¯å¯å˜å¯¹è±¡ï¼Œå·²åœ¨ä¸Šä¸‹æ–‡ä¸­æ›´æ–°ï¼‰
                    if request_info_captured:
                        request_info.update(request_info_captured)
                except Exception:
                    # å¼‚å¸¸å‘ç”Ÿæ—¶ï¼Œè¯·æ±‚ä¿¡æ¯å·²ç»åœ¨ä¸Šä¸‹æ–‡ä¸­è¢«æ•è·ï¼Œåˆå¹¶å®ƒ
                    if request_info_captured:
                        request_info.update(request_info_captured)
                    raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å¤–å±‚ç»Ÿä¸€å¤„ç†
                
                if news_df is not None and not news_df.empty:
                    news_df = news_df.head(5)
                    for _, row in news_df.iterrows():
                        realtime_results.append({
                            'title': str(row.get('æ–°é—»æ ‡é¢˜', '')),
                            'content': str(row.get('æ–°é—»å†…å®¹', '')),
                            'publish_time': str(row.get('å‘å¸ƒæ—¶é—´', '')),
                            'source': 'AKShare-ä¸œæ–¹è´¢å¯Œ',
                            'url': str(row.get('æ–°é—»é“¾æ¥', '')),
                            'symbol': normalized_symbol or symbol_for_query,
                            'query': query,
                            'search_time': search_time
                        })
                    
                    realtime_results = self._filter_allowed_news_records(realtime_results)
                    print(f"âœ… æˆåŠŸè·å– {len(realtime_results)} æ¡å®æ—¶æ–°é—»ï¼ˆè‚¡ç¥¨ï¼š{normalized_symbol or symbol_for_query}ï¼‰")
                    
                    # 3. ä¿å­˜å®æ—¶æ–°é—»åˆ° data/news.csvï¼ˆè¿½åŠ æ¨¡å¼ï¼Œå»é‡ï¼‰â€”â€”å¹¶å‘å®‰å…¨å†™å…¥
                    try:
                        self._purge_news_csv(csv_path)
                        with NEWS_FILE_LOCK:
                            os.makedirs('data', exist_ok=True)
                            df_new = pd.DataFrame(realtime_results)
                            df_new = self._sanitize_news_dataframe(df_new)
                            df_new = self._filter_allowed_news_df(df_new)
                            if df_new is None or df_new.empty:
                                continue
                            if 'search_time' not in df_new.columns:
                                df_new['search_time'] = str(search_time)
                            else:
                                df_new['search_time'] = df_new['search_time'].astype(str)
                            dedupe_subset = ['symbol', 'title', 'search_time', 'query']
                            df_new = df_new.drop_duplicates(subset=dedupe_subset, keep='last')
                            
                            if os.path.exists(csv_path):
                                # å°è¯•å¤šç§ç¼–ç è¯»å–ç°æœ‰æ–‡ä»¶
                                old = None
                                for encoding in ['utf-8', 'utf-8-sig', 'gbk', 'gb18030', 'latin1']:
                                    try:
                                        old = pd.read_csv(csv_path, encoding=encoding)
                                        print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å– news.csv")
                                        break
                                    except Exception:
                                        continue
                                
                                if old is not None:
                                    old = self._sanitize_news_dataframe(old)
                                    old = self._filter_allowed_news_df(old)
                                    if 'search_time' not in old.columns:
                                        old['search_time'] = pd.NA
                                    mask = ~(
                                        (old['symbol'].astype(str) == str(normalized_symbol or symbol_for_query)) &
                                        (old['query'].astype(str) == str(query)) &
                                        (old['search_time'].astype(str) == str(search_time))
                                    )
                                    old = old[mask]
                                    # åˆå¹¶å¹¶å»é‡
                                    combined = pd.concat([old, df_new], axis=0, ignore_index=True)
                                    combined = self._sanitize_news_dataframe(combined)
                                    combined = combined.drop_duplicates(subset=dedupe_subset, keep='last')
                                    combined.to_csv(csv_path, index=False, encoding='utf-8-sig')
                                    print(f"ğŸ’¾ å·²å°†æ–°é—»è¿½åŠ åˆ° {csv_path}ï¼ˆå»é‡åï¼‰")
                                else:
                                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œç›´æ¥è¦†ç›–
                                    df_new.to_csv(csv_path, index=False, encoding='utf-8-sig')
                                    print(f"âš ï¸ æ— æ³•è¯»å–æ—§æ–‡ä»¶ï¼Œå·²åˆ›å»ºæ–°æ–‡ä»¶ {csv_path}")
                            else:
                                df_new.to_csv(csv_path, index=False, encoding='utf-8-sig')
                                print(f"ğŸ’¾ å·²åˆ›å»ºæ–°é—»æ–‡ä»¶ {csv_path}")
                    except Exception as e:
                        print(f"âš ï¸ ä¿å­˜æ–°é—»åˆ°CSVå¤±è´¥: {e}")
                    
                    # è¿”å›ç»„åˆç»“æœï¼šå†å² + å®æ—¶
                    return json.dumps({
                        'success': True,
                        'source': 'akshare',
                        'historical_count': len(historical_news),
                        'realtime_count': len(realtime_results),
                        'total_count': len(historical_news) + len(realtime_results),
                        'historical_news': historical_news[:10],
                        'realtime_news': realtime_results,
                        'saved_to': csv_path
                    }, ensure_ascii=False)
                else:
                    print(f"âš ï¸ AKShare è¿”å›ç©ºæ•°æ®ï¼ˆå°è¯• {attempt}/{max_retries}ï¼Œè‚¡ç¥¨ä»£ç ï¼š{symbol_for_query}ï¼‰")
                    print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼š1) è¯¥è‚¡ç¥¨æš‚æ— æ–°é—» 2) APIè¿”å›ç©ºæ•°æ® 3) ç½‘ç»œé—®é¢˜")
                    
            except json.JSONDecodeError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare JSONè§£æå¤±è´¥ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šAPIè¿”å›äº†éJSONå†…å®¹ï¼ˆå¦‚HTMLé”™è¯¯é¡µé¢ï¼‰ï¼Œé€šå¸¸æ˜¯ç½‘ç»œé—®é¢˜æˆ–åçˆ¬è™«é™åˆ¶")
                if attempt < max_retries:
                    print(f"ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except UnicodeDecodeError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare ç¼–ç é”™è¯¯ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šè¿”å›å†…å®¹çš„ç¼–ç æ ¼å¼ä¸åŒ¹é…")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except AttributeError as e:
                error_msg = str(e)
                if "'NoneType' object has no attribute" in error_msg:
                    print(f"âš ï¸ AKShare è¿”å›Noneï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                    print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šAPIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›äº†Noneï¼Œé€šå¸¸æ˜¯ç½‘ç»œé—®é¢˜æˆ–APIé™åˆ¶")
                else:
                    print(f"âš ï¸ AKShare å±æ€§é”™è¯¯ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except ConnectionError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare è¿æ¥é”™è¯¯ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é˜²ç«å¢™è®¾ç½®")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except TimeoutError as e:
                error_msg = str(e)
                print(f"âš ï¸ AKShare è¯·æ±‚è¶…æ—¶ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: {error_msg}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šAPIå“åº”æ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½æ˜¯ç½‘ç»œæ…¢æˆ–æœåŠ¡å™¨è´Ÿè½½é«˜")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    type(e).__name__, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
            except Exception as e:
                error_type = type(e).__name__
                error_msg = str(e)
                print(f"âš ï¸ AKShare å¤±è´¥ï¼ˆå°è¯• {attempt}/{max_retries}ï¼‰: [{error_type}] {error_msg}")
                
                # ç‰¹æ®Šå¤„ç† JSON è§£æé”™è¯¯ï¼ˆå¯èƒ½è¢«åŒ…è£…åœ¨å…¶ä»–å¼‚å¸¸ä¸­ï¼‰
                if "Expecting value" in error_msg or "JSON" in error_msg.upper():
                    print(f"ğŸ’¡ æ£€æµ‹åˆ°JSONè§£æé—®é¢˜ï¼Œå¯èƒ½æ˜¯APIè¿”å›äº†ç©ºå“åº”æˆ–é”™è¯¯é¡µé¢")
                    print(f"ğŸ’¡ å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) éªŒè¯è‚¡ç¥¨ä»£ç  {symbol_for_query} æ˜¯å¦æœ‰æ•ˆ")
                    print(f"ğŸ’¡ å¯ä»¥å°è¯•ï¼šæ‰‹åŠ¨è®¿é—®ä¸œæ–¹è´¢å¯Œç½‘ç«™æŸ¥çœ‹è¯¥è‚¡ç¥¨æ˜¯å¦æœ‰æ–°é—»")
                
                # ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆåŒ…å«è¯·æ±‚ä¿¡æ¯ï¼‰
                self._save_akshare_error(
                    debug_dir, symbol_for_query, attempt, max_retries,
                    error_type, error_msg, traceback.format_exc(),
                    {'query': query, 'normalized_symbol': normalized_symbol, 'symbol_for_query': symbol_for_query},
                    request_info
                )
                
                # åœ¨æœ€åä¸€æ¬¡å°è¯•æ—¶æ‰“å°å®Œæ•´å †æ ˆ
                if attempt == max_retries:
                    print(f"ğŸ“‹ å®Œæ•´é”™è¯¯ä¿¡æ¯:")
                    traceback.print_exc()
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
            # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œé¿å…é¢‘ç‡é™åˆ¶
            if attempt < max_retries:
                wait_time = attempt * 5  # é€’å¢å»¶è¿Ÿï¼š5ç§’ã€10ç§’
                print(f"â³ ç­‰å¾… {wait_time} ç§’åè¿›è¡Œç¬¬ {attempt + 1} æ¬¡å°è¯•...")
                import time
                time.sleep(wait_time)
            else:
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
                print(f"âŒ AKShare æ‰€æœ‰ {max_retries} æ¬¡å°è¯•å‡å¤±è´¥ï¼Œå°†è¿”å›å†å²æ–°é—»")
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›å†å²æ–°é—»ï¼ˆå¦‚æœæœ‰ï¼‰
        # æ›´æ–°æœ€æ–°æ—¶é—´æˆ³
        if realtime_results or historical_news:
            return json.dumps({
                'success': True,
                'source': 'akshare',
                'historical_count': len(historical_news),
                'realtime_count': len(realtime_results),
                'total_count': len(historical_news) + len(realtime_results),
                'historical_news': historical_news[:10],
                'realtime_news': realtime_results
            }, ensure_ascii=False)

        if historical_news:
            print(f"âš ï¸ AKShare é‡è¯•{max_retries}æ¬¡å‡å¤±è´¥ï¼Œè¿”å›å†å²æ–°é—»")
            return json.dumps({
                'success': True,
                'source': 'historical_only',
                'historical_count': len(historical_news),
                'realtime_count': 0,
                'total_count': len(historical_news),
                'historical_news': historical_news[:10],
                'realtime_news': [],
                'message': self._provider_downtime_message("AKShare")
            }, ensure_ascii=False)
        else:
            return json.dumps({
                'success': False, 
                'message': self._provider_downtime_message("AKShare")
            }, ensure_ascii=False)
    
    def sell_stock(self, symbol: str, amount: int) -> str:
        """
        å–å‡ºè‚¡ç¥¨ï¼ˆä½¿ç”¨å½“å‰å°æ—¶çº§ä»·æ ¼ï¼‰ã€‚
        
        æ­¤å‡½æ•°æ¨¡æ‹Ÿè‚¡ç¥¨å–å‡ºæ“ä½œï¼ŒåŒ…æ‹¬ï¼š
        1. è·å–å½“å‰æŒä»“å’Œæ“ä½œID
        2. è·å–å½“å‰å°æ—¶çš„è‚¡ç¥¨ä»·æ ¼ï¼ˆä¼˜å…ˆå°æ—¶çº§ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·ï¼‰
        3. éªŒè¯å–å‡ºæ¡ä»¶ï¼ˆæ˜¯å¦æŒæœ‰è¯¥è‚¡ç¥¨ï¼Œæ•°é‡æ˜¯å¦å……è¶³ï¼‰
        4. æ›´æ–°æŒä»“ï¼ˆå‡å°‘è‚¡ç¥¨æ•°é‡ï¼Œå¢åŠ ç°é‡‘ï¼‰
        5. è®°å½•äº¤æ˜“åˆ° position.jsonl æ–‡ä»¶
        
        Args:
            symbol (str): è‚¡ç¥¨ä»£ç ï¼Œå¦‚ "600519"
            amount (int): å–å‡ºæ•°é‡ï¼Œå¿…é¡»æ˜¯æ­£æ•´æ•°
        
        Returns:
            str: JSON å­—ç¬¦ä¸²ï¼ŒæˆåŠŸæ—¶è¿”å›æ–°æŒä»“ï¼Œå¤±è´¥æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
        """
        try:
            normalized_symbol = normalize_symbol(symbol)
            if not normalized_symbol:
                return json.dumps({"error": "æ— æ•ˆçš„è‚¡ç¥¨ä»£ç "})
            data_symbol = strip_exchange_prefix(normalized_symbol) or normalized_symbol
            
            # ä½¿ç”¨å®ä¾‹ä¸Šä¸‹æ–‡ï¼Œé¿å…ä»å…±äº« runtime_env.json è¯»å–
            today_date = self._get_context_value("TODAY_DATE")
            current_time = self._get_context_value("CURRENT_TIME")
            decision_time = current_time or f"{today_date} 00:00:00"
            decision_count_raw = self._get_context_value("DECISION_COUNT")
            decision_count = int(decision_count_raw) if decision_count_raw is not None else 0
            if not today_date:
                return json.dumps({"error": "æœªè®¾ç½® TODAY_DATE"})
            decision_time = normalize_decision_time(today_date, decision_time)
            
            # è·å–å½“å‰æŒä»“å’Œæ“ä½œID
            current_position, current_action_id, latest_record = get_latest_position(today_date, self.signature)
            
            # --- T+1 è§„åˆ™æ£€æŸ¥ ---
            if normalized_symbol in current_position and isinstance(current_position[normalized_symbol], dict):
                purchase_date = current_position[normalized_symbol].get("purchase_date")
                if purchase_date == today_date:
                    return json.dumps({
                        "error": "T+1è§„åˆ™é™åˆ¶ï¼šä»Šæ—¥ä¹°å…¥çš„è‚¡ç¥¨ä¸èƒ½åœ¨å½“æ—¥å–å‡º",
                        "symbol": normalized_symbol,
                        "purchase_date": purchase_date
                    })

            # è·å–å½“å‰æ—¶åˆ»çš„è‚¡ç¥¨ä»·æ ¼ - ä¼˜å…ˆå°æ—¶çº§æ•°æ®
            try:
                this_symbol_price = None
                # ä¼˜å…ˆå°è¯•è·å–å°æ—¶çº§ä»·æ ¼
                if current_time:
                    hourly_data = self.dm.get_hourly_stock_data(symbol=data_symbol, end_date=current_time, lookback_hours=1)
                    if not hourly_data.empty:
                        this_symbol_price = float(hourly_data['close'].iloc[-1])
                        print(f"ğŸ’¹ ä½¿ç”¨å°æ—¶çº§ä»·æ ¼: {normalized_symbol} = Â¥{this_symbol_price} ({current_time})")
                
                # å¦‚æœæ²¡æœ‰å°æ—¶çº§æ•°æ®ï¼Œå›é€€åˆ°æ—¥çº¿å¼€ç›˜ä»·
                if this_symbol_price is None or pd.isna(this_symbol_price):
                    stock_data = self.dm.get_stock_data(symbol=data_symbol, end_date=today_date, lookback_days=1)
                    if stock_data.empty:
                        return json.dumps({"error": f"æœªæ‰¾åˆ°è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®", "symbol": normalized_symbol, "date": today_date})
                    this_symbol_price = float(stock_data['open'].iloc[-1]) if 'open' in stock_data.columns else float(stock_data['close'].iloc[-1])
                    print(f"ğŸ’¹ ä½¿ç”¨å¼€ç›˜ä»·: {normalized_symbol} = Â¥{this_symbol_price}")
                
                if pd.isna(this_symbol_price) or this_symbol_price <= 0:
                    return json.dumps({"error": f"è‚¡ç¥¨ {normalized_symbol} çš„ä»·æ ¼æ•°æ®æ— æ•ˆ", "symbol": normalized_symbol, "date": today_date, "price": this_symbol_price})
            except Exception as e:
                return json.dumps({"error": f"è·å–è‚¡ç¥¨ä»·æ ¼å¤±è´¥: {str(e)}", "symbol": normalized_symbol, "date": today_date})
            
            limit_info: Optional[Dict[str, float]] = None
            prev_close = self._get_previous_close(normalized_symbol, today_date)
            limit_info = get_price_limits(normalized_symbol, prev_close)
            allowed, reason = self._passes_price_limit_liquidity("sell", this_symbol_price, limit_info)
            if not allowed:
                return json.dumps({
                    "error": reason,
                    "symbol": normalized_symbol,
                    "price": this_symbol_price,
                    "limit_info": limit_info
                }, ensure_ascii=False)
            
            # éªŒè¯å–å‡ºæ¡ä»¶
            if normalized_symbol not in current_position or not isinstance(current_position[normalized_symbol], dict):
                return json.dumps({"error": f"æœªæŒæœ‰è‚¡ç¥¨ {normalized_symbol}ï¼äº¤æ˜“ä¸è¢«å…è®¸ã€‚", "symbol": normalized_symbol, "date": today_date})
            
            current_shares = current_position.get(normalized_symbol, {}).get("shares", 0)
            if current_shares < amount:
                return json.dumps({
                    "error": "æŒè‚¡æ•°é‡ä¸è¶³ï¼äº¤æ˜“ä¸è¢«å…è®¸ã€‚",
                    "have": current_shares,
                    "want_to_sell": amount,
                    "symbol": normalized_symbol,
                    "date": today_date
                })
            
            # æ‰§è¡Œå–å‡ºæ“ä½œ
            new_position = copy.deepcopy(current_position)
            new_position[normalized_symbol]["shares"] = current_shares - amount
            
            # å¦‚æœè‚¡ç¥¨æ•°é‡ä¸º0ï¼Œåˆ™ä»æŒä»“ä¸­ç§»é™¤
            if new_position[normalized_symbol]["shares"] == 0:
                del new_position[normalized_symbol]

            revenue = this_symbol_price * amount
            
            # --- äº¤æ˜“æˆæœ¬è®¡ç®— ---
            commission_rate = self.trading_rules.get("commission_rate", 0.0003)
            min_commission = self.trading_rules.get("min_commission", 5.0)
            stamp_duty_rate = self.trading_rules.get("stamp_duty_rate", 0.0005)
            
            commission = max(revenue * commission_rate, min_commission)
            stamp_duty = revenue * stamp_duty_rate
            total_deduction = commission + stamp_duty
            
            net_revenue = revenue - total_deduction
            new_position["CASH"] = new_position.get("CASH", 0) + net_revenue
            new_position = normalize_positions(new_position)
            
            # è®°å½•äº¤æ˜“
            record: Dict[str, Any] = {
                "date": today_date,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "this_action": {"action": "sell", "symbol": normalized_symbol, "amount": amount},
                "positions": new_position
            }
            if latest_record and latest_record.get("decision_time") == decision_time:
                record["id"] = latest_record.get("id")
            else:
                record["id"] = current_action_id + 1
            upsert_position_record(self.signature, record)
            
            write_config_value("IF_TRADE", True)
            return json.dumps({
                "success": True,
                "action": "sell",
                "symbol": normalized_symbol,
                "amount": amount,
                "price": this_symbol_price,
                "revenue": revenue,
                "commission": commission,
                "stamp_duty": stamp_duty,
                "net_revenue": net_revenue,
                "price_limit": limit_info,
                "decision_time": decision_time,
                "decision_count": decision_count,
                "new_position": new_position
            })
        
        except Exception as e:
            return json.dumps({"error": f"å–å‡ºè‚¡ç¥¨æ—¶å‡ºé”™: {str(e)}"})
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ– AI æ¨¡å‹å’Œå·¥å…·ï¼ˆçº¯æœ¬åœ°æ¨¡å¼ï¼‰"""
        print(f"ğŸš€ åˆå§‹åŒ–ä»£ç†: {self.signature}")
        print("ğŸ’» ä½¿ç”¨çº¯æœ¬åœ°æ¨¡å¼...")
        if self.dm:
            self.tools = [
                tool(self.search_stock_news),            # ğŸ”„ å·²æ•´åˆï¼šå†å²+å®æ—¶æ–°é—»ï¼Œè‡ªåŠ¨ä¿å­˜
                tool(self.get_technical_indicators),     # ğŸ”„ å·²æ•´åˆï¼šå†å²+å®æ—¶æŒ‡æ ‡ï¼Œè‡ªåŠ¨ä¿å­˜
                tool(self.get_hourly_stock_data),
                tool(self.get_current_stock_prices),
                tool(self.get_latest_position_tool),
                tool(self.buy_stock),
                tool(self.sell_stock)
            ]
            print(f"âœ… å·²åŠ è½½ {len(self.tools)} ä¸ªæœ¬åœ°å·¥å…·ï¼ˆæ‰€æœ‰å·¥å…·å·²æ•´åˆå†å²+å®æ—¶æ•°æ®ï¼‰")
        else:
            print(f"âŒ DataManager æœªåˆå§‹åŒ–,æœªåŠ è½½ä»»ä½•å·¥å…·")
            self.tools = []
        
        if self.basemodel.startswith("gemini"):
            print(f"ğŸ¤– Initializing Google Gemini model: {self.basemodel}")
            gemini_safety_settings = None
            if self.safety_settings:
                from google.generativeai.types import HarmCategory, HarmBlockThreshold
                gemini_safety_settings = {}
                harm_category_map = {
                    "HARM_CATEGORY_HARASSMENT": HarmCategory.HARM_CATEGORY_HARASSMENT,
                    "HARM_CATEGORY_HATE_SPEECH": HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    "HARM_CATEGORY_SEXUALLY_EXPLICIT": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    "HARM_CATEGORY_DANGEROUS_CONTENT": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                }
                harm_threshold_map = {
                    "BLOCK_NONE": HarmBlockThreshold.BLOCK_NONE,
                    "BLOCK_LOW_AND_ABOVE": HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
                    "BLOCK_MEDIUM_AND_ABOVE": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                    "BLOCK_ONLY_HIGH": HarmBlockThreshold.BLOCK_ONLY_HIGH,
                }
                for category, threshold in self.safety_settings.items():
                    if category in harm_category_map and threshold in harm_threshold_map:
                        gemini_safety_settings[harm_category_map[category]] = harm_threshold_map[threshold]
            self.model = ChatGoogleGenerativeAI(
                model=self.basemodel,
                google_api_key=self.google_api_key,
                safety_settings=gemini_safety_settings,
                max_retries=5,
                timeout=60
            )
            print(f"âœ… Google Gemini model initialized with API key from {'environment' if self.google_api_key == os.getenv('GEMINI_API_KEY') else 'config'}")
        elif self.basemodel.startswith("qwen"):
            print(f"ğŸ¤– Initializing Qwen model: {self.basemodel}")
            dashscope_url = self.openai_base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            self.model = ChatOpenAI(
                model=self.basemodel,
                base_url=dashscope_url,
                api_key=self.openai_api_key,
                max_retries=5,
                timeout=180
            )
            print(f"âœ… Qwen model initialized via {dashscope_url}")
        else:
            print(f"ğŸ¤– Initializing OpenAI-compatible model: {self.basemodel}")
            self.model = ChatOpenAI(
                model=self.basemodel,
                base_url=self.openai_base_url,
                api_key=self.openai_api_key,
                max_retries=3,
                timeout=30
            )
            print(f"âœ… OpenAI-compatible model initialized")
        
        print(f"âœ… Agent {self.signature} initialization completed")
    def _setup_logging(self, today_date: str, decision_time: str) -> str:
        """Set up (and reset) log file path for a specific decision time"""
        log_path = os.path.join(self.base_log_path, self.signature, 'log', today_date)
        os.makedirs(log_path, exist_ok=True)
        sanitized_time = decision_time.replace(":", "-").replace(" ", "_")
        log_file = os.path.join(log_path, f"{sanitized_time}.jsonl")
        with open(log_file, "w", encoding="utf-8") as f:
            f.write("")
        return log_file
    
    def _log_message(
        self,
        log_file: str,
        new_messages: List[Dict[str, str]],
        decision_time: Optional[str] = None,
        decision_count: Optional[int] = None
    ) -> None:
        """Log messages to log file"""
        sanitized_messages = self._sanitize_messages(new_messages)
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "signature": self.signature,
            "decision_time": decision_time,
            "decision_count": decision_count,
            "new_messages": sanitized_messages
        }
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

    def _log_snapshot_reference(
        self,
        log_file: str,
        decision_time: str,
        decision_count: int,
    ) -> None:
        if not self._current_snapshot_info:
            return
        info = self._current_snapshot_info
        snapshot_id = info.get("snapshot_id") or "unknown"
        snapshot_path = info.get("snapshot_path") or "local-memory"
        mode = info.get("mode") or "local"
        created_flag = info.get("snapshot_created")
        content = (
            f"Shared snapshot [{mode}] id={snapshot_id}, path={snapshot_path}, "
            f"created_now={created_flag}"
        )
        self._log_message(
            log_file,
            [{"role": "system", "content": content}],
            decision_time=decision_time,
            decision_count=decision_count,
        )
    
    def _sanitize_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Create a lightweight copy of messages for logging (short summary / truncation)."""
        sanitized: List[Dict[str, str]] = []
        for msg in messages:
            content = msg.get("content")
            if isinstance(content, str):
                sanitized_content = self._summarize_content(content)
                sanitized.append({**msg, "content": sanitized_content})
            else:
                sanitized.append(msg.copy())
        return sanitized
    
    def _summarize_content(self, content: str) -> str:
        """Summarize long or JSON-heavy content for logging."""
        if not content:
            return ""
        MAX_LEN = 500
        stripped = content.strip()
        if len(stripped) <= MAX_LEN and stripped.count("\n") <= 4:
            return stripped
        
        lines = [line for line in stripped.splitlines() if line.strip()]
        if len(lines) == 1:
            return self._summarize_line(lines[0])
        
        summaries = []
        for line in lines[:3]:
            summaries.append(self._summarize_line(line))
        if len(lines) > 3:
            summaries.append(f"... ({len(lines)} entries)")
        return " | ".join(summaries)
    
    def _summarize_line(self, line: str) -> str:
        """Summarize single line (try JSON first, fallback to truncated text)."""
        try:
            data = json.loads(line)
        except Exception:
            return self._truncate_text(line)
        
        if isinstance(data, dict):
            if "error" in data:
                return f"error: {self._truncate_text(str(data.get('error')))}"
            if data.get("success") is False:
                return f"failed: {self._truncate_text(str(data.get('message') or data))}"
            parts: List[str] = []
            if "success" in data:
                parts.append(f"success={data['success']}")
            for key in ("historical_count", "realtime_count", "total_count"):
                if key in data:
                    parts.append(f"{key}={data[key]}")
            if "message" in data:
                parts.append(self._truncate_text(str(data["message"])))
            if not parts:
                parts.append(self._truncate_text(json.dumps(data, ensure_ascii=False)))
            return ", ".join(parts)
        if isinstance(data, list):
            return f"list[{len(data)}]"
        return self._truncate_text(str(data))
    
    def _truncate_text(self, text: str, max_len: int = 180) -> str:
        text = text.strip()
        return text if len(text) <= max_len else text[:max_len] + "...(truncated)"
    
    def _content_to_text(self, content: Any) -> str:
        """Normalize message content (which can be str/list/dict) into plain text."""
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            parts: List[str] = []
            for item in content:
                if isinstance(item, dict):
                    if "text" in item and item["text"]:
                        parts.append(str(item["text"]))
                    elif "json" in item:
                        try:
                            parts.append(json.dumps(item["json"], ensure_ascii=False))
                        except Exception:
                            parts.append(str(item["json"]))
                    elif "data" in item:
                        parts.append(str(item["data"]))
                    else:
                        parts.append(str(item))
                else:
                    parts.append(str(item))
            return "\n".join(part for part in parts if part)
        if isinstance(content, dict):
            if "text" in content and content["text"]:
                return str(content["text"])
            if "json" in content:
                try:
                    return json.dumps(content["json"], ensure_ascii=False)
                except Exception:
                    return str(content["json"])
        return str(content)
    
    def _combine_tool_outputs(self, tool_messages: List[Any]) -> str:
        """Join tool outputs into a single string while handling different payload shapes."""
        outputs: List[str] = []
        for msg in tool_messages:
            if isinstance(msg, dict):
                content = msg.get("content")
            else:
                content = getattr(msg, "content", None)
            text = self._content_to_text(content)
            if text:
                outputs.append(text)
        return "\n".join(outputs)
    
    def _capture_akshare_response(self, symbol: str):
        """å°è¯•æ‹¦æˆª AKShare API çš„ HTTP è¯·æ±‚å’Œå“åº”ã€‚
        
        ä½¿ç”¨ requests çš„ monkey patching æ¥æ•è·å“åº”å†…å®¹ï¼ŒåŒ…æ‹¬åŸå§‹å­—èŠ‚å’Œå¤šç§è§£ç æ–¹å¼ã€‚
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            tuple: (capture_context, request_info_dict) - ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œè¯·æ±‚ä¿¡æ¯å­—å…¸ï¼ˆå¯å˜å¯¹è±¡ï¼‰
        """
        request_info = {
            'symbol': symbol,
            'url': None,
            'method': None,
            'status_code': None,
            'response_text': None,
            'response_content_raw': None,  # åŸå§‹å­—èŠ‚å†…å®¹
            'response_content_length': None,  # åŸå§‹å†…å®¹é•¿åº¦
            'response_content_decoded': None,  # æ‰‹åŠ¨è§£ç åçš„å†…å®¹
            'response_headers': None,
            'request_headers': None
        }
        
        try:
            import requests
            import gzip
            
            # å­˜å‚¨åŸå§‹æ–¹æ³•
            original_send = requests.Session.send
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨
            class ResponseCapture:
                def __init__(self, req_info):
                    self.request_info = req_info
                    self.original_send = original_send
                
                def __enter__(self):
                    # Monkey patch send æ–¹æ³•
                    def patched_send(self_session, request, **kwargs):
                        # è®°å½•è¯·æ±‚ä¿¡æ¯
                        req_info = self.request_info
                        req_info['url'] = request.url
                        req_info['method'] = request.method
                        
                        # ä¿®æ”¹è¯·æ±‚å¤´ï¼Œä½¿ç”¨çœŸå®çš„æµè§ˆå™¨ User-Agent å’Œå…¶ä»– headers
                        # é¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
                        if request.headers is None:
                            request.headers = {}
                        
                        # ä½¿ç”¨çœŸå®çš„ Chrome æµè§ˆå™¨ User-Agent
                        browser_user_agent = (
                            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) "
                            "Chrome/120.0.0.0 Safari/537.36"
                        )
                        
                        # ä¿®æ”¹æˆ–æ·»åŠ  headers
                        request.headers['User-Agent'] = browser_user_agent
                        request.headers['Accept'] = 'application/json, text/plain, */*'
                        request.headers['Accept-Language'] = 'zh-CN,zh;q=0.9,en;q=0.8'
                        request.headers['Accept-Encoding'] = 'gzip, deflate, br'
                        request.headers['Referer'] = 'http://quote.eastmoney.com/'
                        request.headers['Origin'] = 'http://quote.eastmoney.com'
                        request.headers['Connection'] = 'keep-alive'
                        
                        # è®°å½•ä¿®æ”¹åçš„è¯·æ±‚å¤´
                        if request.headers:
                            req_info['request_headers'] = dict(request.headers)
                        
                        # å‘é€è¯·æ±‚
                        response = self.original_send(self_session, request, **kwargs)
                        
                        # è®°å½•å“åº”ä¿¡æ¯
                        try:
                            req_info['status_code'] = response.status_code
                            if response.headers:
                                req_info['response_headers'] = dict(response.headers)
                            
                            # ä¿å­˜åŸå§‹å­—èŠ‚å†…å®¹ï¼ˆåœ¨ requests è‡ªåŠ¨è§£å‹ä¹‹å‰ï¼‰
                            try:
                                # è·å–åŸå§‹å†…å®¹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è¢«è§£ç ï¼‰
                                if hasattr(response, 'content'):
                                    raw_content = response.content
                                    req_info['response_content_length'] = len(raw_content) if raw_content else 0
                                    
                                    # ä¿å­˜åŸå§‹å­—èŠ‚çš„å‰5000å­—èŠ‚ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                                    if raw_content:
                                        try:
                                            # å°è¯•è½¬æ¢ä¸ºå¯æ‰“å°çš„å­—ç¬¦ä¸²ï¼ˆåªä¿å­˜å‰1000å­—èŠ‚ä»¥é¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
                                            content_preview = raw_content[:1000]
                                            # å¦‚æœå†…å®¹æ˜¯äºŒè¿›åˆ¶ï¼Œå°è¯•ç¼–ç ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
                                            if isinstance(content_preview, bytes):
                                                req_info['response_content_raw'] = content_preview.hex()[:2000]  # é™åˆ¶é•¿åº¦
                                            else:
                                                req_info['response_content_raw'] = str(content_preview)[:2000]
                                        except Exception:
                                            req_info['response_content_raw'] = f"<æ— æ³•æ˜¾ç¤º: {len(raw_content)} å­—èŠ‚>"
                                    
                                    # å°è¯•æ‰‹åŠ¨è§£ç  gzip å†…å®¹ï¼ˆå¦‚æœå“åº”æ˜¯ gzip å‹ç¼©çš„ï¼‰
                                    content_encoding = response.headers.get('Content-Encoding', '').lower()
                                    if 'gzip' in content_encoding and raw_content:
                                        try:
                                            decoded_content = gzip.decompress(raw_content)
                                            decoded_text = decoded_content.decode('utf-8', errors='ignore')
                                            # åªä¿å­˜å‰2000ä¸ªå­—ç¬¦
                                            req_info['response_content_decoded'] = decoded_text[:2000]
                                        except Exception as gzip_err:
                                            req_info['gzip_decode_error'] = str(gzip_err)
                                else:
                                    req_info['response_content_length'] = 0
                            except Exception as content_err:
                                req_info['content_capture_error'] = str(content_err)
                            
                            # ä¿å­˜è§£ç åçš„æ–‡æœ¬å†…å®¹ï¼ˆrequests è‡ªåŠ¨å¤„ç†çš„ï¼‰
                            try:
                                text_content = response.text
                                req_info['response_text'] = text_content[:2000] if text_content else ""
                                req_info['response_text_length'] = len(text_content) if text_content else 0
                            except Exception as text_err:
                                req_info['response_text'] = ""
                                req_info['response_text_error'] = str(text_err)
                            
                            req_info['url'] = response.url or req_info.get('url')
                        except Exception as e:
                            req_info['capture_error'] = str(e)
                        
                        return response
                    
                    # æ›¿æ¢ send æ–¹æ³•
                    requests.Session.send = patched_send
                    return self
                
                def __exit__(self, exc_type, exc_val, exc_tb):
                    # æ¢å¤åŸå§‹æ–¹æ³•
                    requests.Session.send = self.original_send
                    return False  # ä¸æŠ‘åˆ¶å¼‚å¸¸
            
            return ResponseCapture(request_info), request_info
            
        except Exception as e:
            # å¦‚æœæ‹¦æˆªå¤±è´¥ï¼Œè¿”å›ç©ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨å’Œé”™è¯¯ä¿¡æ¯
            request_info['capture_error'] = str(e)
            
            class EmptyCapture:
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    return False
            
            return EmptyCapture(), request_info

    def _save_akshare_error(self, debug_dir: str, symbol: str, attempt: int, max_retries: int,
                            error_type: str, error_msg: str, traceback_str: str, context: dict,
                            request_info: Optional[dict] = None):
        """ä¿å­˜ AKShare API é”™è¯¯ä¿¡æ¯åˆ°æ–‡ä»¶ï¼Œä¾¿äºåç»­åˆ†æã€‚
        
        Args:
            debug_dir: è°ƒè¯•æ–‡ä»¶å¤¹è·¯å¾„
            symbol: è‚¡ç¥¨ä»£ç 
            attempt: å½“å‰å°è¯•æ¬¡æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            error_type: é”™è¯¯ç±»å‹
            error_msg: é”™è¯¯æ¶ˆæ¯
            traceback_str: å †æ ˆè·Ÿè¸ª
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæŸ¥è¯¢å‚æ•°ç­‰ï¼‰
            request_info: è¯·æ±‚ä¿¡æ¯ï¼ˆURLã€å‚æ•°ã€å“åº”å†…å®¹ç­‰ï¼‰
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"akshare_error_{symbol}_{timestamp}_attempt{attempt}.json"
            filepath = os.path.join(debug_dir, filename)
            
            # è·å–ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
            import sys
            import platform
            try:
                import akshare as ak
                akshare_version = getattr(ak, '__version__', 'unknown')
            except Exception:
                akshare_version = 'unknown'
            
            error_data = {
                'timestamp': timestamp,
                'datetime': datetime.now().isoformat(),
                'symbol': symbol,
                'attempt': attempt,
                'max_retries': max_retries,
                'error_type': error_type,
                'error_message': error_msg,
                'context': context,
                'traceback': traceback_str,
                'environment': {
                    'python_version': sys.version,
                    'platform': platform.platform(),
                    'akshare_version': akshare_version
                },
                'request_info': request_info or {}
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(error_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {filepath}")
            print(f"ğŸ“‹ è¯·å°†æ–‡ä»¶å‘é€ç»™å¼€å‘è€…è¿›è¡Œåˆ†æ: {filename}")
        except Exception as save_err:
            print(f"âš ï¸ ä¿å­˜é”™è¯¯ä¿¡æ¯å¤±è´¥: {save_err}")
    
    def _extract_tool_errors(self, tool_response: str) -> List[str]:
        """ä»å·¥å…·è¿”å›ä¸­æå–çœŸæ­£çš„é”™è¯¯ä¿¡æ¯ï¼Œç”¨äºæœ€ç»ˆæ€»ç»“è¯´æ˜ã€‚
        æ³¨æ„ï¼šè­¦å‘Šã€ç©ºæ•°æ®æˆ–éƒ¨åˆ†æˆåŠŸä¸åº”è¢«è§†ä¸ºé”™è¯¯ã€‚
        """
        errors: List[str] = []
        if not tool_response:
            return errors
        
        # å…ˆå°è¯•è§£ææ•´ä¸ªå“åº”ä½œä¸º JSON
        try:
            data = json.loads(tool_response)
            if isinstance(data, dict):
                # åªæå–æ˜ç¡®çš„é”™è¯¯ï¼Œä¸åŒ…æ‹¬è­¦å‘Šæˆ–éƒ¨åˆ†æˆåŠŸ
                if data.get("error"):
                    error_msg = str(data["error"])
                    # æ’é™¤å¸¸è§çš„è­¦å‘Šä¿¡æ¯ï¼ˆå¦‚"è¿”å›å†å²æ•°æ®"ã€"å®¢æˆ·ç«¯ä¸å¯ç”¨"ç­‰ï¼‰
                    # è¿™äº›æ˜¯é™çº§å¤„ç†çš„æ­£å¸¸æƒ…å†µï¼Œä¸åº”è¯¥è§†ä¸ºé”™è¯¯
                    if "è¿”å›å†å²æ•°æ®" not in error_msg and "ä¸å¯ç”¨" not in error_msg:
                        # åªæœ‰åœ¨çœŸæ­£å¤±è´¥æ—¶æ‰è®°å½•ä¸ºé”™è¯¯
                        if "å¤±è´¥" in error_msg or "é”™è¯¯" in error_msg or "error" in error_msg.lower():
                            errors.append(error_msg)
                elif data.get("success") is False:
                    message = data.get("message", "")
                    if message and ("å¤±è´¥" in message or "é”™è¯¯" in message or "error" in message.lower()):
                        errors.append(str(message))
        except Exception:
            # å¦‚æœä¸æ˜¯ JSONï¼Œå°è¯•é€è¡Œè§£æ
            for raw_line in tool_response.splitlines():
                line = raw_line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    if isinstance(data, dict):
                        if data.get("error"):
                            error_msg = str(data["error"])
                            # æ’é™¤è­¦å‘Šä¿¡æ¯
                            if "è¿”å›å†å²æ•°æ®" not in error_msg and "è­¦å‘Š" not in error_msg:
                                errors.append(error_msg)
                        elif data.get("success") is False:
                            message = data.get("message", "")
                            if message and ("å¤±è´¥" in message or "é”™è¯¯" in message):
                                errors.append(str(message))
                except Exception:
                    continue
        
        return errors

    def _provider_downtime_message(self, provider_name: str) -> str:
        """Return a friendly message when upstream providers fail."""
        return f"{provider_name} æ•°æ®æºæš‚æ—¶ä¸å¯ç”¨ï¼Œç³»ç»Ÿå·²åˆ‡æ¢åˆ°å¤‡ä»½æ•°æ®ï¼Œè¯·ç¨åé‡è¯•ã€‚"

    def _fix_mojibake(self, value: Any) -> Any:
        if isinstance(value, str):
            clean_value = value.replace("\ufeff", "").strip()
            for _ in range(3):
                if not any(ch in clean_value for ch in ("Ãƒ", "Ã‚", "Ã¯")) and not any(128 <= ord(ch) <= 255 for ch in clean_value):
                    break
                try:
                    decoded_value = clean_value.encode("latin1").decode("utf-8")
                except Exception:
                    break
                if decoded_value == clean_value:
                    break
                clean_value = decoded_value
            return clean_value
        return value

    def _sanitize_dataframe_text(self, df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:
        if df is None:
            return None
        sanitized = df.copy()
        raw_columns = [self._fix_mojibake(str(col)).strip() for col in sanitized.columns]
        unique_columns: List[str] = []
        column_indices: List[int] = []
        seen: set[str] = set()
        for idx, col_name in enumerate(raw_columns):
            base_name = col_name.split(".", 1)[0]
            if base_name not in seen:
                unique_columns.append(base_name)
                column_indices.append(idx)
                seen.add(base_name)
        sanitized = sanitized.iloc[:, column_indices]
        sanitized.columns = unique_columns
        for col in sanitized.columns:
            if sanitized[col].dtype == object:
                sanitized[col] = sanitized[col].apply(self._fix_mojibake)
        return sanitized

    def _normalize_symbol_value(self, value: Any) -> str:
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return ""
        raw = str(value).strip()
        if raw.endswith(".0"):
            raw = raw[:-2]
        if raw.isdigit() and len(raw) < 6:
            raw = raw.zfill(6)
        normalized = normalize_symbol(raw)
        return normalized or raw.upper()

    def _sanitize_news_dataframe(self, df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:
        sanitized = self._sanitize_dataframe_text(df)
        if sanitized is None:
            return None
        if "symbol" in sanitized.columns:
            sanitized["symbol"] = sanitized["symbol"].apply(self._normalize_symbol_value)
            sanitized = sanitized[sanitized["symbol"].astype(str).str.len() > 0]
        return sanitized
    
    def _get_previous_close(self, symbol: str, today_date: str) -> Optional[float]:
        if not self.dm:
            return None
        try:
            prev_date = get_yesterday_date(today_date)
            target_time = f"{prev_date} 15:00:00"
            plain_symbol = strip_exchange_prefix(symbol) or symbol
            price = self.dm.get_price_at(plain_symbol, target_time)
            if price is None and plain_symbol != symbol:
                price = self.dm.get_price_at(symbol, target_time)
            return float(price) if price is not None else None
        except Exception as e:
            print(f"âš ï¸ è·å–å‰æ”¶å¤±è´¥: {e}")
            return None
    
    def _passes_price_limit_liquidity(
        self,
        action: str,
        price: Optional[float],
        limits: Optional[Dict[str, float]]
    ) -> Tuple[bool, Optional[str]]:
        if price is None or not limits:
            return True, None
        threshold = self.LIMIT_THRESHOLD_RATIO
        if action == "buy":
            upper = limits.get("upper")
            if upper is not None and price >= upper * threshold:
                if random.random() >= self.LIMIT_ORDER_SUCCESS_RATE:
                    return False, f"æ¥è¿‘æ¶¨åœä»· Â¥{upper:.2f}ï¼Œä¹°å•æˆäº¤æ¦‚ç‡ä»…10%ï¼Œæ­¤æ¬¡æ¨¡æ‹Ÿæœªæˆäº¤ã€‚"
        elif action == "sell":
            lower = limits.get("lower")
            if lower is not None and price <= lower / threshold:
                if random.random() >= self.LIMIT_ORDER_SUCCESS_RATE:
                    return False, f"æ¥è¿‘è·Œåœä»· Â¥{lower:.2f}ï¼Œå–å•æˆäº¤æ¦‚ç‡ä»…10%ï¼Œæ­¤æ¬¡æ¨¡æ‹Ÿæœªæˆäº¤ã€‚"
        return True, None
    
    def _compute_portfolio_metrics(
        self,
        positions: Dict[str, Any],
        today_date: str,
        current_time: Optional[str]
    ) -> Dict[str, Any]:
        metrics = {
            "cash": float(positions.get("CASH", 0.0) or 0.0),
            "position_value": 0.0,
            "unrealized_total": 0.0,
            "total_equity": 0.0,
            "holdings": [],
        }
        holdings_symbols: List[str] = []
        processed: List[Dict[str, Any]] = []
        is_open_slot = bool(current_time and current_time.endswith("10:30:00"))
        opening_price_cache: Dict[str, Optional[float]] = {}
        opening_reference_time = f"{today_date} 10:30:00" if is_open_slot else None
        
        if not positions:
            metrics["total_equity"] = metrics["cash"]
            return metrics
        
        for symbol, data in positions.items():
            if symbol == "CASH":
                continue
            if not isinstance(data, dict):
                continue
            shares = int(data.get("shares", 0) or 0)
            if shares <= 0:
                continue
            holdings_symbols.append(symbol)
            processed.append({
                "symbol": symbol,
                "shares": shares,
                "purchase_date": data.get("purchase_date"),
                "avg_price": data.get("avg_price")
            })
        
        price_lookup: Dict[str, Optional[float]] = {}
        target_time = current_time or f"{today_date} 15:00:00"
        if self.dm and holdings_symbols:
            try:
                price_lookup = self.dm.get_prices_at(holdings_symbols, target_time) or {}
            except Exception as e:
                print(f"âš ï¸ Failed to fetch current prices: {e}")
                price_lookup = {}
        
        for item in processed:
            symbol = item["symbol"]
            shares = item["shares"]
            purchase_date = item.get("purchase_date")
            avg_price = item.get("avg_price")
            current_price = None
            if price_lookup:
                current_price = price_lookup.get(symbol.upper()) or price_lookup.get(symbol)
            if current_price is None and self.dm:
                try:
                    current_price = self.dm.get_price_at(symbol, target_time)
                except Exception:
                    current_price = None

            if current_price is None and is_open_slot and self.dm and opening_reference_time:
                plain_symbol = strip_exchange_prefix(symbol) or symbol
                if plain_symbol not in opening_price_cache:
                    try:
                        df_open = self.dm.get_hourly_stock_data(
                            symbol=plain_symbol,
                            end_date=opening_reference_time,
                            lookback_hours=1
                        )
                        if df_open is not None and not df_open.empty:
                            latest_row = df_open.iloc[-1]
                            price_candidate = latest_row.get("open")
                            if price_candidate is None or pd.isna(price_candidate):
                                price_candidate = latest_row.get("close")
                            opening_price_cache[plain_symbol] = float(price_candidate) if price_candidate is not None else None
                        else:
                            opening_price_cache[plain_symbol] = None
                    except Exception as e:
                        print(f"âš ï¸ Failed to fetch opening price for {plain_symbol}: {e}")
                        opening_price_cache[plain_symbol] = None
                current_price = opening_price_cache.get(plain_symbol)
            
            if avg_price is None and self.dm and purchase_date:
                try:
                    approx_time = f"{purchase_date} 15:00:00"
                    avg_price = self.dm.get_price_at(symbol, approx_time)
                except Exception:
                    avg_price = None
            
            market_value = float(current_price or 0.0) * shares if current_price is not None else 0.0
            cost_basis = None
            if avg_price is not None:
                cost_basis = float(avg_price) * shares
            unrealized = None
            if cost_basis is not None and current_price is not None:
                unrealized = market_value - cost_basis
                metrics["unrealized_total"] += unrealized
            
            metrics["position_value"] += market_value
            metrics["holdings"].append({
                "symbol": symbol,
                "shares": shares,
                "avg_price": avg_price,
                "current_price": current_price,
                "market_value": market_value,
                "unrealized": unrealized
            })
        
        metrics["total_equity"] = metrics["cash"] + metrics["position_value"]
        return metrics

    def _parse_timestamp(self, value: Any) -> Optional[datetime]:
        if value is None:
            return None
        try:
            parsed = pd.to_datetime(value)
            if hasattr(parsed, "to_pydatetime"):
                parsed = parsed.to_pydatetime()
            if isinstance(parsed, datetime):
                return parsed.replace(tzinfo=None)
        except Exception:
            return None
        return None
    
    async def _ainvoke_with_retry(
        self,
        message: List[Dict[str, str]],
        recursion_limit: Optional[int] = None
    ) -> Any:
        """Agent invocation with retry"""
        limit = recursion_limit or self.max_steps or 20
        for attempt in range(1, self.max_retries + 1):
            try:
                return await self.agent.ainvoke(
                    {"messages": message}, 
                    {"recursion_limit": limit}
                )
            except Exception as e:
                # Handle Google API specific errors
                error_type = type(e).__name__
                if "google" in str(type(e).__module__).lower():
                    print(f"âš ï¸ Google API error ({error_type}): {e}")
                else:
                    print(f"âš ï¸ Error ({error_type}): {e}")
                if attempt == self.max_retries:
                    print(f"âŒ All {self.max_retries} attempts failed")
                    raise e
                
                wait_time = self.base_delay * attempt
                print(f"âš ï¸ Attempt {attempt} failed, retrying after {wait_time} seconds...")
                await asyncio.sleep(wait_time)
    
    async def run_trading_session(
        self,
        today_date: str,
        current_time: str,
        decision_count: int = 1
    ) -> None:
        """
        Run single trading session
        
        Args:
            today_date: Trading date
            current_time: Current simulation time
            decision_count: Which decision this is (1-3)
        """
        print(f"ğŸ“ˆ Starting trading session: {current_time} (Decision {decision_count}/3)")
        
        # Ensure config values are set (é˜²æ­¢å¤–éƒ¨æµç¨‹é—æ¼å¯¼è‡´å·¥å…·æŠ¥é”™)
        write_config_value("TODAY_DATE", today_date)
        write_config_value("CURRENT_TIME", current_time)
        write_config_value("DECISION_COUNT", decision_count)
        self.runtime_context["TODAY_DATE"] = today_date
        self.runtime_context["CURRENT_TIME"] = current_time
        self.runtime_context["DECISION_COUNT"] = decision_count

        # Set up logging
        log_file = self._setup_logging(today_date, current_time)
        
        # Update system prompt with decision count
        self.agent = create_react_agent(
            self.model,
            tools=self.tools,
            prompt=get_agent_system_prompt(
                today_date,
                self.signature,
                dm=self.dm,
                current_time=current_time,
                decision_count=decision_count
            ),
        )
        
        snapshot_result = None
        snapshot_bundle: Optional[Dict[str, Any]] = None
        observation_summary = ""
        try:
            if not self.prefetch_coordinator:
                raise RuntimeError("Shared prefetch coordinator is not available")

            def _build_snapshot() -> Dict[str, Any]:
                return self._collect_prefetch_bundle(today_date, current_time, decision_count)

            snapshot_result = self.prefetch_coordinator.ensure_snapshot(
                today_date=today_date,
                current_time=current_time,
                symbols_signature=self._symbols_signature(),
                builder=_build_snapshot,
            )
            snapshot_bundle = snapshot_result.data
            observation_summary = self._apply_prefetch_bundle(snapshot_bundle)
        except Exception as e:
            print(f"âš ï¸ Shared prefetch unavailable ({e}), falling back to per-agent prefetch.")
            snapshot_bundle = self._collect_prefetch_bundle(today_date, current_time, decision_count)
            observation_summary = snapshot_bundle.get("observation_summary") or self._build_observation_summary()
            snapshot_result = None

        snapshot_id = (snapshot_bundle or {}).get("snapshot_id")
        snapshot_path = snapshot_result.path if snapshot_result else None
        snapshot_created = snapshot_result.created if snapshot_result else False
        self._current_snapshot_info = {
            "snapshot_id": snapshot_id,
            "snapshot_path": snapshot_path,
            "snapshot_created": snapshot_created,
            "mode": "shared" if snapshot_result else "local",
        }
        self._log_snapshot_reference(log_file, current_time, decision_count)

        # Build dynamic context message with full positions snapshot
        try:
            latest_positions, _, latest_record = get_latest_position(today_date, self.signature)
        except Exception as e:
            print(f"âš ï¸ Failed to load latest positions: {e}")
            latest_positions = {}
            latest_record = None

        positions_json = json.dumps(latest_positions, ensure_ascii=False, indent=2)
        last_action = None
        if latest_record:
            last_action = latest_record.get("this_action")

        if decision_count == 1:
            stage = "opening (observe & prepare)"
        elif decision_count == 2:
            stage = "midday (deploy capital)"
        else:
            stage = "afternoon (adjust/lock profits)"

        metrics = self._compute_portfolio_metrics(latest_positions, today_date, current_time)
        if metrics["cash"] <= 0 and not metrics["holdings"]:
            print("âš ï¸ Cash balance is zero with no holdings. Consider enabling FORCE_REPLAY to reset positions.")
        holdings_lines: List[str] = []
        for holding in metrics.get("holdings", []):
            sym = holding["symbol"]
            shares = holding["shares"]
            avg_price = holding.get("avg_price")
            current_price = holding.get("current_price")
            market_value = holding.get("market_value", 0.0)
            unrealized = holding.get("unrealized")
            line = f"  â€¢ {sym}: {shares} shares"
            if current_price is not None:
                line += f", Px Â¥{current_price:,.2f}"
            if avg_price is not None:
                line += f", Avg Â¥{avg_price:,.2f}"
            line += f", MV Â¥{market_value:,.2f}"
            if unrealized is not None:
                line += f", PnL Â¥{unrealized:,.2f}"
            holdings_lines.append(line)
        if not holdings_lines:
            holdings_lines.append("  â€¢ (no equity positions)")
        
        context_message = (
            f"Trading context for {self.signature}:\n"
            f"- Date: {today_date}\n"
            f"- Time: {current_time}\n"
            f"- Decision index: {decision_count}/3 â€” stage: {stage}\n"
            f"- Latest recorded action: {json.dumps(last_action, ensure_ascii=False) if last_action else 'N/A'}\n"
            f"- Cash: Â¥{metrics['cash']:,.2f}\n"
            f"- Position value: Â¥{metrics['position_value']:,.2f}\n"
            f"- Total equity: Â¥{metrics['total_equity']:,.2f}\n"
            f"- Unrealized PnL: Â¥{metrics['unrealized_total']:,.2f}\n"
            f"- Holdings detail:\n" + "\n".join(holdings_lines) + "\n"
            f"- Observation snapshot (last 3 days news + latest hourly data):\n{observation_summary}\n"
            f"- Full positions JSON (from position file, do not trim):\n{positions_json}\n\n"
            "- æ— è®ºæ˜¯å¦äº¤æ˜“ï¼Œå¿…é¡»åœ¨ç»“å°¾ä½¿ç”¨ <FINISH_SIGNAL> å¹¶ç»™å‡ºæ¸…æ™°ç»“è®ºï¼›è‹¥é€‰æ‹©ä¸äº¤æ˜“ï¼Œè¯·è¯´æ˜å…·ä½“åŸå› ä¸ä¸‹ä¸€æ­¥è®¡åˆ’ã€‚\n"
            "Please analyze current signals, adjust positions if needed, obey all rules, "
            "and provide a concise plan before calling tools or trades. Conclude with the required stop signal."
        )

        user_query = [{"role": "user", "content": context_message}]
        message = user_query.copy()
        
        # Log initial message
        self._log_message(log_file, user_query, decision_time=current_time, decision_count=decision_count)
        
        final_agent_summary: Optional[str] = None
        collected_tool_errors: List[str] = []
        
        try:
            response = await self._ainvoke_with_retry(message, recursion_limit=self.max_steps)
            
            # Extract agent response
            agent_response = extract_conversation(response, "final")
            if agent_response and agent_response.strip():
                final_agent_summary = agent_response
                if STOP_SIGNAL in agent_response:
                    print("âœ… Received stop signal, trading session ended")
                else:
                    print("â„¹ï¸ Agent completed without explicit stop signal")
                    collected_tool_errors.append("Missing stop signal in agent response")
                print(agent_response)
                self._log_message(
                    log_file,
                    [{"role": "assistant", "content": agent_response}],
                    decision_time=current_time,
                    decision_count=decision_count,
                )
            else:
                print("âš ï¸ Agent produced no final response")
                final_agent_summary = "NO_TRADE: æ¨¡å‹æœªæä¾›æœ‰æ•ˆè¾“å‡ºã€‚"
                collected_tool_errors.append("Agent produced no final response")
                self._log_message(
                    log_file,
                    [{"role": "system", "content": "Agent produced no final response."}],
                    decision_time=current_time,
                    decision_count=decision_count,
                )

            # Extract and summarize tool outputs for logging/error tracking
            tool_msgs = extract_tool_messages(response)
            tool_response = self._combine_tool_outputs(tool_msgs)
            if tool_response:
                collected_tool_errors.extend(self._extract_tool_errors(tool_response))
                tool_summary = self._summarize_content(tool_response) or self._truncate_text(tool_response, 600)
            self._log_message(
                log_file,
                [{"role": "system", "content": f"Tool summary: {tool_summary}"}],
                decision_time=current_time,
                decision_count=decision_count,
            )
        except Exception as e:
            print(f"âŒ Trading session error: {str(e)}")
            print(f"Error details: {e}")
            import traceback

            traceback.print_exc()
            # å³ä½¿å‡ºé”™ä¹Ÿè¦è®°å½•é”™è¯¯ä¿¡æ¯åˆ°æ—¥å¿—
            try:
                error_msg = f"Trading session failed: {str(e)}"
                self._log_message(
                    log_file,
                    [{"role": "assistant", "content": error_msg}],
                    decision_time=current_time,
                    decision_count=decision_count,
                )
            except Exception:
                pass  # å¦‚æœæ—¥å¿—è®°å½•ä¹Ÿå¤±è´¥ï¼Œè‡³å°‘ä¸å½±å“ä¸»æµç¨‹
            raise
        finally:
            # æ— è®ºæˆåŠŸä¸å¦éƒ½å°è¯•å¤„ç†äº¤æ˜“ç»“æœå¹¶è®°å½•çŠ¶æ€
            try:
                await self._handle_trading_result(
                    today_date,
                    current_time,
                    decision_count,
                    log_file,
                    final_agent_summary,
                    collected_tool_errors,
                )
            except Exception as e:
                print(f"âš ï¸ Error handling trading result: {e}")
                try:
                    error_msg = f"Error handling trading result: {str(e)}"
                    self._log_message(
                        log_file,
                        [{"role": "system", "content": error_msg}],
                        decision_time=current_time,
                        decision_count=decision_count,
                    )
                except Exception:
                    pass
            try:
                from tools.backup_utils import run_backup_snapshot, save_pnl_snapshot
                reason = f"decision_{decision_count}_{today_date}_{current_time}"
                ok = run_backup_snapshot(reason=reason)
                if ok:
                    print(f"âœ… Backup completed for decision {decision_count}")
                save_pnl_snapshot(reason=reason)  # é¢å¤–ä¿å­˜æ”¶ç›Šæ›²çº¿
            except Exception as e:
                print(f"âš ï¸ Error during backup: {e}")
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œå…³é—­ TinySoft å®¢æˆ·ç«¯ï¼Œä»¥ä¿æŒä¼šè¯å¤ç”¨
            # ä¼šè¯ä¼šåœ¨ run_date_range ç»“æŸæ—¶ç»Ÿä¸€å…³é—­ï¼Œé¿å…é¢‘ç¹ç™»å½•
            pass
    async def _handle_trading_result(
        self,
        today_date: str,
        decision_time: str,
        decision_count: int,
        log_file: str,
        final_agent_summary: Optional[str],
        collected_tool_errors: Optional[List[str]]
    ) -> None:
        """Handle trading results"""
        if_trade = get_config_value("IF_TRADE")
        if if_trade:
            write_config_value("IF_TRADE", False)
            print("âœ… Trading completed")
            self._log_message(
                log_file,
                [{"role": "system", "content": "Trade executed during this session."}],
                decision_time=decision_time,
                decision_count=decision_count
            )
        else:
            print("ğŸ“Š No trading, maintaining positions")
            try:
                add_no_trade_record(today_date, decision_time, decision_count, self.signature)
            except NameError as e:
                print(f"âŒ NameError: {e}")
                raise
            write_config_value("IF_TRADE", False)
            needs_followup = not final_agent_summary or not str(final_agent_summary).strip()
            if needs_followup:
                reason_text = None
                if collected_tool_errors:
                    filtered = [msg for msg in collected_tool_errors if msg]
                    if filtered:
                        reason_text = "; ".join(dict.fromkeys(filtered))  # å»é‡ä¿æŒé¡ºåº
                if reason_text is None:
                    reason_text = "å·¥å…·æ‰§è¡ŒæœªæˆåŠŸå®Œæˆ"
                clarification = (
                    "æœ¬è½®äº¤æ˜“æœ€ç»ˆæœªæˆäº¤ï¼Œå·²è®°å½•æ— äº¤æ˜“ã€‚"
                    f"åŸå› ï¼š{reason_text}ã€‚"
                )
                self._log_message(
                    log_file,
                    [{"role": "assistant", "content": clarification}],
                    decision_time=decision_time,
                    decision_count=decision_count
                )
    
    def _get_trading_hours(self, today_date: str) -> List[str]:
        return [
            f"{today_date} 10:30:00",  # é¦–æ¬¡å°æ—¶çº¿å¯ç”¨
            f"{today_date} 11:30:00",  # åˆé—´å‰ï¼ˆä¸Šåˆæœ€åä¸€æ ¹ï¼‰
            f"{today_date} 14:00:00",  # åˆåæ ¸å¿ƒæ—¶æ®µ
        ]

    async def run_intraday_trading(self, today_date: str, start_index: int = 0) -> None:
        """
        æ¯å¤©è¿›è¡Œ3æ¬¡äº¤æ˜“å†³ç­–ï¼š10:30ã€11:30ã€14:00ã€‚
        """
        trading_hours = self._get_trading_hours(today_date)

        for idx, current_time in enumerate(trading_hours[start_index:], start_index + 1):
            print(f"ğŸ•’ ç¬¬ {idx}/3 æ¬¡å†³ç­– - æ—¶é—´: {current_time}")
            write_config_value("CURRENT_TIME", current_time)
            write_config_value("DECISION_COUNT", idx)  # è®°å½•ç¬¬å‡ æ¬¡å†³ç­–
            await self.run_session_with_retry(today_date, current_time, decision_count=idx)

    def register_agent(self) -> None:
        """Register new agent, create initial positions"""
        position_exists = os.path.exists(self.position_file)
        if position_exists and not self.force_replay:
            print(f"âš ï¸ Position file {self.position_file} already exists, skipping registration")
            return
        if position_exists and self.force_replay:
            print(f"ğŸ—‘ï¸ force_replay=True, clearing existing data for {self.signature}")
            self._reset_agent_storage()
        
        # Ensure directory structure exists
        os.makedirs(self.data_path, exist_ok=True)
        position_dir = os.path.join(self.data_path, "position")
        log_dir = os.path.join(self.data_path, "log")
        os.makedirs(position_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        
        # Determine initial record date & decision time
        seed_date = self.init_date
        seed_decision_time = f"{self.init_date} 00:00:00"
        if not seed_date:
            seed_date = datetime.now().strftime("%Y-%m-%d")
            seed_decision_time = f"{seed_date} 00:00:00"
        
        # Create initial positions
        init_position = {symbol: {"shares": 0, "purchase_date": None} for symbol in self.stock_symbols}
        init_position['CASH'] = self.initial_cash
        
        with open(self.position_file, "w") as f:  # Use "w" mode to ensure creating new file
            f.write(json.dumps({
                "date": seed_date,
                "decision_time": seed_decision_time,
                "decision_count": 0,
                "id": 0, 
                "seed": True,
                "this_action": {"action": "seed", "symbol": "", "amount": 0},
                "positions": init_position
            }) + "\n")
        
        print(f"âœ… Agent {self.signature} registration completed")
        print(f"ğŸ“ Position file: {self.position_file}")
        print(f"ğŸ’° Initial cash: ${self.initial_cash}")
        print(f"ğŸ“Š Number of stocks: {len(self.stock_symbols)}")
    
    def get_trading_dates(self, init_date: str, end_date: str) -> List[str]:
        """
        Get trading date list
        
        Args:
            init_date: Start date
            end_date: End date
            
        Returns:
            List of trading dates
        """
        if not os.path.exists(self.position_file):
            self.register_agent()
        
        start_dt = datetime.strptime(init_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        last_completed: Optional[datetime] = None
        
        if os.path.exists(self.position_file):
            with open(self.position_file, "r") as f:
                for line in f:
                    try:
                        doc = json.loads(line)
                    except Exception:
                        continue
                    if doc.get("seed"):
                        continue
                    date_str = doc.get("date")
                    if not date_str:
                        continue
                    try:
                        decision_dt = datetime.strptime(date_str, "%Y-%m-%d")
                    except Exception:
                        continue
                    if decision_dt < start_dt:
                        continue
                    if (last_completed is None) or (decision_dt > last_completed):
                        last_completed = decision_dt
        
        current_dt = start_dt
        if last_completed and last_completed >= start_dt:
            next_day = last_completed + timedelta(days=1)
            if next_day > current_dt:
                current_dt = next_day
        
        trading_dates: List[str] = []
        while current_dt <= end_dt:
            if current_dt.weekday() < 5:
                trading_dates.append(current_dt.strftime("%Y-%m-%d"))
            current_dt += timedelta(days=1)
        
        return trading_dates
    
    def _determine_resume_point(self, init_date: str, end_date: str) -> Optional[tuple[str, int]]:
        """
        æ ¹æ®æŒä»“è®°å½•åˆ¤æ–­æ˜¯å¦éœ€è¦åœ¨æŸä¸ªæ—¥æœŸ/æ—¶é—´ç‚¹é‡æ–°å¼€å§‹ã€‚
        è¿”å› (date, start_index)ï¼›start_index å¯¹åº” trading_hours ä¸­çš„ç´¢å¼•ã€‚
        """
        if not os.path.exists(self.position_file):
            return None

        try:
            init_dt = datetime.strptime(init_date, "%Y-%m-%d").date()
            end_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
        except Exception:
            return None

        candidate_records: List[Dict[str, Any]] = []
        with open(self.position_file, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    doc = json.loads(line)
                except Exception:
                    continue

                decision_time = doc.get("decision_time")
                decision_date = doc.get("date")
                if not decision_time or not decision_date:
                    continue

                try:
                    decision_date_obj = datetime.strptime(decision_date, "%Y-%m-%d").date()
                except Exception:
                    continue

                if decision_date_obj < init_dt or decision_date_obj > end_dt:
                    continue

                candidate_records.append(doc)

        if not candidate_records:
            return None

        candidate_records.sort(
            key=lambda item: (
                item.get("date", ""),
                item.get("decision_time", ""),
                item.get("id", 0),
            )
        )

        last_date = candidate_records[-1].get("date")
        if not last_date:
            return None

        trading_hours = self._get_trading_hours(last_date)
        recorded_times = sorted({
            item.get("decision_time")
            for item in candidate_records
            if item.get("date") == last_date and item.get("decision_time")
        })

        if not recorded_times:
            return None

        if set(recorded_times) >= set(trading_hours):
            # æœ€åä¸€ä¸ªäº¤æ˜“æ—¥å·²å…¨éƒ¨å®Œæˆ
            return None

        last_time = recorded_times[-1]
        if last_time not in trading_hours:
            return None

        return last_date, trading_hours.index(last_time)
    
    async def run_session_with_retry(self, today_date: str, current_time: str, decision_count: int = 1) -> None:
        """Run method with retry"""
        for attempt in range(1, self.max_retries + 1):
            try:
                print(f"ğŸ”„ Attempting to run {self.signature} - {current_time} (Decision {decision_count}/3, Attempt {attempt})")
                await self.run_trading_session(today_date, current_time, decision_count)
                print(f"âœ… {self.signature} - {current_time} run successful")
                return
            except Exception as e:
                print(f"âŒ Attempt {attempt} failed: {str(e)}")
                if attempt == self.max_retries:
                    print(f"ğŸ’¥ {self.signature} - {current_time} all retries failed")
                    raise
                else:
                    wait_time = self.base_delay * attempt
                    print(f"â³ Waiting {wait_time} seconds before retry...")
                    await asyncio.sleep(wait_time)
    
    async def run_date_range(self, init_date: str, end_date: str) -> None:
        """
        Run all trading days in date range
        
        Args:
            init_date: Start date
            end_date: End date
        """
        print(f"ğŸ“… Running date range: {init_date} to {end_date}")
        
        trading_dates = self.get_trading_dates(init_date, end_date)
        resume_point = self._determine_resume_point(init_date, end_date)

        start_indices: Dict[str, int] = {}
        dates_to_process: List[str] = []

        if resume_point:
            resume_date, resume_index = resume_point
            dates_to_process.append(resume_date)
            start_indices[resume_date] = resume_index

        for date in trading_dates:
            dates_to_process.append(date)

        # ä¿æŒé¡ºåºå¹¶å»é‡
        ordered_dates: List[str] = []
        seen_dates = set()
        for date in dates_to_process:
            if date not in seen_dates:
                ordered_dates.append(date)
                seen_dates.add(date)

        if not ordered_dates:
            print(f"â„¹ï¸ No trading days to process")
            if os.path.exists(self.position_file):
                print("ğŸ’¡ Hint: set FORCE_REPLAY=true (or agent_config.force_replay) to reset state for replays.")
            return
        
        print(f"ğŸ“Š Trading days to process: {ordered_dates}")
        
        # Process each trading day
        for date in ordered_dates:
            print(f"ğŸ”„ Processing {self.signature} - Date: {date}")
            
            # Set configuration
            write_config_value("TODAY_DATE", date)
            write_config_value("SIGNATURE", self.signature)
            
            try:
                start_index = start_indices.get(date, 0)
                await self.run_intraday_trading(date, start_index=start_index)
            except Exception as e:
                print(f"âŒ Error processing {self.signature} - Date: {date}")
                print(e)
                raise
        if self.dm:
            try:
                self.dm.close_ts_client(force=True)
            except Exception as close_err:
                print(f"âš ï¸ Failed to close TinySoft session: {close_err}")
        print(f"âœ… {self.signature} processing completed")
    
    def get_position_summary(self) -> Dict[str, Any]:
        """Get position summary"""
        if not os.path.exists(self.position_file):
            return {"error": "Position file does not exist"}
        
        positions = []
        with open(self.position_file, "r") as f:
            for line in f:
                positions.append(json.loads(line))
        
        if not positions:
            return {"error": "No position records"}
        
        latest_position = positions[-1]
        return {
            "signature": self.signature,
            "latest_date": latest_position.get("date"),
            "positions": latest_position.get("positions", {}),
            "total_records": len(positions)
        }
    
    def __str__(self) -> str:
        return f"BaseAgent(signature='{self.signature}', basemodel='{self.basemodel}', stocks={len(self.stock_symbols)})"
    
    def __repr__(self) -> str:
        return self.__str__()
